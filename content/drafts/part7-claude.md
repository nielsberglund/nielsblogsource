---
type: post
layout: "post"
draft: true
---


# Building an Event Management System with Claude Code: Part 7 - Implementing the Import MCP Server

Posted by nielsb on [Date]

**Welcome back to the Building an Event Management System with Claude Code series!** If you've been following along, you know we've moved from theory to practice, from planning to building. In Part 6, we spent quality time designing our Import MCP Server, discovering crucial architectural insights that fundamentally changed how we think about MCP servers.

**To see all posts in the series, go to:** [Building an Event Management System with Claude Code](/contact-event-claude-code).

Today is where the magic happens. We're going to build the actual Import MCP Server that transforms my 2-3 hours of post-event manual data wrangling into a 2-minute conversation with Claude Desktop.

## The Promise vs The Reality

Let me be honest about something: when I started this journey, I thought building an MCP server would be complex. I expected to spend days wrestling with protocol specifications, debugging connection issues, and fighting with CSV parsing edge cases.

The reality? **With Claude Code as a development partner, we built a production-ready MCP server in about 2-3 hours.** Not because I'm some coding wizard, but because we did the hard thinking first (Part 6's design work), and Claude Code handled the implementation details I would have spent hours researching.

But it wasn't without challenges. We hit CSV encoding issues, discovered edge cases in duplicate detection, and learned some hard lessons about transaction management. I'm going to show you all of itâ€”the wins and the strugglesâ€”because that's real AI-native development.

## What We'll Accomplish Today

By the end of this post, you'll have:

âœ… A complete, working Import MCP Server (7 tools, ~850 lines)
âœ… Understanding of FastMCP project structure and patterns
âœ… Real implementation conversations with Claude Code
âœ… Integration with Claude Desktop for conversational imports
âœ… Tested workflows with actual event data
âœ… The experience of typing "Import the March 2025 registrations" and watching it happen

**Time investment:** ~2-3 hours of AI-assisted development
**Time saved per event:** ~2-3 hours of manual CSV wrangling
**Break-even point:** After the first import! ðŸŽ‰

## Quick Recap: The Architecture Breakthrough

Before we dive into code, let's recap the crucial lesson from Part 6 that guides everything we build today.

### The Critical MCP Insight

In Part 6, we discovered something fundamental about MCP architecture that changed our entire design:

**MCP servers are isolated processes. They cannot call each other.**

This isn't just a technical detailâ€”it's the architectural constraint that determines how we build everything. Let me show you what this means:

```
âŒ What I initially thought MCP servers could do:
Import MCP â†’ calls CSV MCP â†’ calls Postgres MCP
(Servers calling each other in a chain)

âœ… What actually happens:
           Claude Desktop (Orchestrator)
                /              \
               /                \
        Import MCP          Postgres MCP Pro
    (self-contained)       (separate, for queries)
    
Each server talks ONLY to Claude, never to each other.
```

**Why this matters:** If MCP servers can't call each other, then our Import MCP server must be **self-contained**. It can't just delegate CSV parsing to csv-mcp-server or database operations to Postgres MCP Pro. It has to do everything itself.

### What "Leveraging Existing Tools" Really Means

So when we said in Part 3 that we'd "leverage existing MCP servers," what we actually meant was:

**We don't call other MCP servers âŒ**
**We use the same battle-tested libraries they use âœ…**

Specifically:
- **pandas** - Same library csv-mcp-server uses for CSV operations
- **psycopg2** - Same library Postgres MCP Pro uses for database operations  
- **FastMCP** - Same framework many MCP servers are built with

This is the 80/20 principle in action:
- **80%** = Proven libraries do the heavy lifting (parsing, validation, SQL)
- **20%** = Our custom code (Quicket-specific formats, business rules, duplicate detection)

### Architecture Decision Summary

Based on this constraint, we chose a **self-contained Import MCP Server** with 7 distinct tools:

**Infrastructure Tools** (build first):
1. `validate_import_data` - Fast format checking (50ms, no DB access)
2. `preview_import` - Shows what will be imported (reads DB)

**Import Operations** (build by priority):
3. `import_quicket_registrations` - Creates contacts + tickets
4. `import_quicket_checkins` - Updates attendance status
5. `import_checkin_walkins` - Handles walk-in attendees
6. `import_sessionize_speakers` - Imports speakers + sessions
7. `import_speaker_ratings` - Links ratings to sessions

**User experience:**
```
User: "Import the March 2025 Quicket registrations"
        â†“
Claude Desktop: [single tool call]
        â†“
import_quicket_registrations() does everything:
  - Parses CSV with pandas
  - Validates data
  - Detects duplicates
  - Inserts to database with psycopg2
  - Returns complete results
        â†“
Claude Desktop: "âœ… Imported 144 new contacts, 3 updated, 147 tickets"
```

**Benefits of this architecture:**
- âœ… Single tool call per import (fast, simple)
- âœ… Atomic operations (import succeeds or fails as a unit)
- âœ… Simpler for LLM to use (less orchestration complexity)
- âœ… All import logic encapsulated (easier to maintain)

Now that we're clear on the architecture, let's build it.

## Setting Up the FastMCP Project

Let's start by creating our project structure. I'm going to show you the actual conversation I had with Claude Code, because this is what AI-native development looks like.

### Starting the Conversation

I started Claude Code in my project directory:

```bash
$ cd ~/repos/event-contact-management-2
$ claude --continue
```

The `--continue` flag picks up our previous conversation context, so Claude Code already knows about our project structure, database schema, and design decisions from Part 6.

When Claude Code loaded, I gave it this prompt:

```
We're ready to start building the Import MCP Server. Let's begin by 
setting up the FastMCP project structure. 

Create:
- Directory structure in mcp-servers/import-mcp/
- pyproject.toml with all dependencies
- Basic server.py skeleton
- Utility modules (db.py, parsers.py, validators.py)

Use the project structure patterns from the FastMCP documentation.
```

**Code Snippet 1: Initial Setup Prompt**

### Claude Code's Response

Claude Code went off and thought about the structure, then came back with a plan:

```
â— I'll create a complete FastMCP project structure for the Import MCP Server.

  This will follow FastMCP best practices and set us up for building the 7 tools 
  we designed in Part 6.

â— Update Todos
  â˜ Create mcp-servers/import-mcp/ directory structure
  â˜ Write pyproject.toml with dependencies (fastmcp, pandas, psycopg2-binary, python-dotenv)
  â˜ Create src/server.py with FastMCP boilerplate
  â˜ Create src/utils/db.py for database connection utilities
  â˜ Create src/utils/parsers.py for CSV parsing utilities
  â˜ Create src/utils/validators.py for data validation
  â˜ Create tests/ directory with sample CSV files
  â˜ Create README.md with setup instructions

â— Creating directory structure...
```

**Code Snippet 2: Claude Code Planning the Structure**

Then it started creating files. I'll show you the key ones:

### The Project Structure

```
mcp-servers/import-mcp/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ server.py              # Main FastMCP server
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ validation.py      # validate_import_data
â”‚   â”‚   â”œâ”€â”€ preview.py         # preview_import
â”‚   â”‚   â””â”€â”€ imports.py         # The 5 import tools
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ db.py              # Database utilities
â”‚       â”œâ”€â”€ parsers.py         # CSV parsing utilities
â”‚       â””â”€â”€ validators.py      # Data validation logic
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_data/
â”‚       â”œâ”€â”€ quicket_registrations_sample.csv
â”‚       â”œâ”€â”€ quicket_checkins_sample.csv
â”‚       â”œâ”€â”€ sessionize_speakers_sample.csv
â”‚       â””â”€â”€ speaker_ratings_sample.csv
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

### pyproject.toml

Here's what Claude Code generated for our dependencies:

```toml
[project]
name = "import-mcp"
version = "0.1.0"
description = "MCP Server for importing event data from Quicket, Sessionize, and Microsoft Forms"
requires-python = ">=3.10"
dependencies = [
    "fastmcp>=0.2.0",
    "pandas>=2.0.0",
    "psycopg2-binary>=2.9.0",
    "python-dotenv>=1.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]
```

**Code Snippet 3: pyproject.toml**

Notice Claude Code:
- Pinned minimum versions (important for reproducibility)
- Separated dev dependencies
- Used `psycopg2-binary` (easier to install than `psycopg2`)
- Included `python-dotenv` (we'll use this for database credentials)

### Installing Dependencies

To set up the environment, I used `uv` (the modern Python package manager):

```bash
$ cd mcp-servers/import-mcp
$ uv venv
$ source .venv/bin/activate  # On Windows: .venv\Scripts\activate
$ uv pip install -e ".[dev]"
```

**Code Snippet 4: Environment Setup**

The `-e` flag installs in "editable" mode, so we can make changes without reinstalling.

### The Server Skeleton

Claude Code created a basic `server.py` that we'll build on:

```python
"""
Import MCP Server
Provides tools for importing event data from various sources.
"""

import os
from fastmcp import FastMCP

# Initialize MCP server
mcp = FastMCP("Import MCP Server")

# Tool implementations will be registered here

if __name__ == "__main__":
    # Run the server
    mcp.run()
```

**Code Snippet 5: Initial server.py**

Clean and simple. FastMCP handles all the MCP protocol complexity; we just register tools.

### Database Connection Utility

Claude Code also created `src/utils/db.py`:

```python
"""Database connection utilities."""

import os
import psycopg2
from psycopg2.extras import RealDictCursor
from contextlib import contextmanager
from typing import Generator
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def get_db_config() -> dict:
    """Get database configuration from environment variables."""
    return {
        "dbname": os.getenv("DB_NAME", "event_contact_db"),
        "user": os.getenv("DB_USER"),
        "password": os.getenv("DB_PASSWORD"),
        "host": os.getenv("DB_HOST"),
        "port": os.getenv("DB_PORT", "5432"),
        "sslmode": os.getenv("DB_SSLMODE", "require"),
    }

@contextmanager
def get_db_connection() -> Generator[psycopg2.extensions.connection, None, None]:
    """
    Context manager for database connections.
    Automatically handles connection cleanup.
    """
    conn = None
    try:
        config = get_db_config()
        conn = psycopg2.connect(**config, cursor_factory=RealDictCursor)
        yield conn
    finally:
        if conn:
            conn.close()

def execute_query(query: str, params: tuple = None) -> list[dict]:
    """Execute a SELECT query and return results as list of dicts."""
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(query, params)
            return cur.fetchall()

def execute_insert(query: str, params: tuple = None) -> int:
    """Execute an INSERT query and return the inserted row ID."""
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(query, params)
            result = cur.fetchone()
            conn.commit()
            return result['id'] if result else None
```

**Code Snippet 6: Database Utilities**

Key patterns Claude Code used:
- **Context manager** (`@contextmanager`) - Ensures connections are always closed
- **RealDictCursor** - Returns rows as dictionaries (easier to work with)
- **Environment variables** - Never hardcode credentials
- **Automatic commit** - Simplifies insert operations

### Environment Variables Setup

Claude Code also created `.env.example`:

```bash
# Database Configuration (Neon PostgreSQL)
DB_HOST=your-neon-host.azure.neon.tech
DB_PORT=5432
DB_NAME=event_contact_db
DB_USER=your_user
DB_PASSWORD=your_password
DB_SSLMODE=require
```

**Code Snippet 7: .env.example**

I copied this to `.env` and filled in my actual Neon database credentials from Part 4.

### Verification

At this point, I asked Claude Code to create a simple test to verify the setup:

```python
# tests/test_connection.py
import sys
sys.path.insert(0, 'src')

from utils.db import execute_query

def test_connection():
    """Test database connection."""
    result = execute_query("SELECT version()")
    print(f"âœ… Connected to PostgreSQL: {result[0]['version']}")

if __name__ == "__main__":
    test_connection()
```

**Code Snippet 8: Connection Test**

Running it:

```bash
$ python tests/test_connection.py
âœ… Connected to PostgreSQL: PostgreSQL 17.2 on x86_64-pc-linux-gnu...
```

**Perfect!** Our project structure is set up, dependencies are installed, and database connection is verified. Time to build some tools.

## Building Phase 1: Infrastructure Tools

Remember our build order from Part 6:

**Phase 1: Foundation** (build this first)
1. `validate_import_data` - Fast format checking
2. `preview_import` - Shows what will be imported
3. `import_quicket_registrations` - High-value import
4. `import_speaker_ratings` - Also high-value

We'll start with the infrastructure tools (validate and preview) because they'll make testing the import tools much easier.

### Tool 1: validate_import_data

**Purpose:** Fast format checking without touching the database (target: <50ms)

This tool should:
- Detect import type from CSV structure
- Validate required fields exist
- Check data types (emails look like emails, dates parse correctly)
- Return issues list for the user

I asked Claude Code to implement it:

```
Let's implement validate_import_data first. This should:
1. Read the CSV file with pandas
2. Detect the import type (Quicket registration, Quicket check-in, Sessionize, etc.) 
   based on column names
3. Validate required fields exist
4. Check data types (email format, valid dates, phone numbers)
5. Return a detailed validation report

Do NOT access the database. This should be fast (<50ms).
```

**Code Snippet 9: validate_import_data Prompt**

Claude Code created `src/tools/validation.py`:

```python
"""Validation tools for import data."""

import pandas as pd
from typing import Literal, Dict, List
from ..utils import validators

def detect_import_type(df: pd.DataFrame) -> str:
    """Detect import type based on CSV columns."""
    columns = set(df.columns)
    
    # Quicket registration has these key columns
    if {'Order Number', 'Ticket Number', 'Purchase Date', 'Cellphone'}.issubset(columns):
        return "quicket_registrations"
    
    # Quicket check-in has 'Checked In' column
    if {'Ticket Number', 'Checked In'}.issubset(columns):
        return "quicket_checkins"
    
    # Sessionize has speaker-specific columns
    if {'Session Id', 'First Name', 'Last Name'}.issubset(columns):
        return "sessionize_speakers"
    
    # Microsoft Forms walk-ins
    if {'First Name', 'Last Name', 'Email Address', 'Cell Phone Number'}.issubset(columns):
        return "checkin_walkins"
    
    # Speaker ratings
    if {'Session Rating', 'Speaker Rating'}.issubset(columns):
        return "speaker_ratings"
    
    return "unknown"

def validate_quicket_registrations(df: pd.DataFrame) -> Dict:
    """Validate Quicket registration CSV format."""
    issues = []
    
    # Required fields
    required_fields = [
        'Order Number', 'Ticket Number', 'Purchase Date',
        'First name', 'Surname', 'Email', 'Cellphone'
    ]
    
    for field in required_fields:
        if field not in df.columns:
            issues.append({
                "severity": "error",
                "field": field,
                "message": f"Required field '{field}' is missing"
            })
    
    if issues:  # Don't proceed if structure is wrong
        return {"valid": False, "issues": issues}
    
    # Validate each row
    for idx, row in df.iterrows():
        # Check email format
        if not validators.is_valid_email(row['Email']):
            issues.append({
                "severity": "warning",
                "row": idx + 2,  # +2 because CSV is 1-indexed and has header
                "field": "Email",
                "value": row['Email'],
                "message": "Invalid email format"
            })
        
        # Check phone format (South African)
        if pd.notna(row['Cellphone']) and not validators.is_valid_phone(row['Cellphone']):
            issues.append({
                "severity": "warning",
                "row": idx + 2,
                "field": "Cellphone",
                "value": row['Cellphone'],
                "message": "Invalid phone format (expected +27...)"
            })
        
        # Check required fields not empty
        if pd.isna(row['First name']) or pd.isna(row['Surname']):
            issues.append({
                "severity": "error",
                "row": idx + 2,
                "field": "Name",
                "message": "First name or Surname is empty"
            })
    
    return {
        "valid": len([i for i in issues if i["severity"] == "error"]) == 0,
        "issues": issues
    }

# Similar functions for other import types...
# validate_sessionize_speakers(), validate_speaker_ratings(), etc.
```

**Code Snippet 10: Validation Logic (Partial)**

Then Claude Code registered it as an MCP tool in `server.py`:

```python
from fastmcp import FastMCP
from tools.validation import detect_import_type, validate_quicket_registrations
import pandas as pd

mcp = FastMCP("Import MCP Server")

@mcp.tool()
def validate_import_data(file_path: str) -> dict:
    """
    Validate CSV import file format and data quality.
    
    Fast validation (<50ms) that checks:
    - CSV structure and required columns
    - Data types and formats (emails, phones, dates)
    - Required fields are not empty
    
    Does NOT access database. Use preview_import() to check for duplicates.
    
    Args:
        file_path: Path to CSV file to validate
        
    Returns:
        {
            "valid": bool,
            "import_type": str,  # "quicket_registrations", "sessionize_speakers", etc.
            "row_count": int,
            "issues": [
                {
                    "severity": "error" | "warning",
                    "row": int,  # Optional, if issue is row-specific
                    "field": str,
                    "message": str,
                    "value": str  # Optional, the problematic value
                }
            ]
        }
    """
    try:
        # Read CSV
        df = pd.read_csv(file_path)
        
        # Detect type
        import_type = detect_import_type(df)
        
        if import_type == "unknown":
            return {
                "valid": False,
                "import_type": "unknown",
                "row_count": len(df),
                "issues": [{
                    "severity": "error",
                    "message": "Could not detect import type from CSV columns"
                }]
            }
        
        # Validate based on type
        if import_type == "quicket_registrations":
            result = validate_quicket_registrations(df)
        elif import_type == "sessionize_speakers":
            result = validate_sessionize_speakers(df)
        # ... etc
        
        return {
            "valid": result["valid"],
            "import_type": import_type,
            "row_count": len(df),
            "issues": result["issues"]
        }
        
    except Exception as e:
        return {
            "valid": False,
            "import_type": "unknown",
            "row_count": 0,
            "issues": [{
                "severity": "error",
                "message": f"Failed to read CSV: {str(e)}"
            }]
        }
```

**Code Snippet 11: validate_import_data MCP Tool**

### Testing validate_import_data

I tested it with a real Quicket export (anonymized):

```bash
$ python -c "from src.server import mcp; import json; \
  result = validate_import_data('tests/test_data/quicket_reg_march2025.csv'); \
  print(json.dumps(result, indent=2))"
```

Result:

```json
{
  "valid": true,
  "import_type": "quicket_registrations",
  "row_count": 147,
  "issues": [
    {
      "severity": "warning",
      "row": 23,
      "field": "Email",
      "value": "john.smith@gmail,com",
      "message": "Invalid email format"
    },
    {
      "severity": "warning",
      "row": 45,
      "field": "Cellphone",
      "value": "0825551234",
      "message": "Invalid phone format (expected +27...)"
    }
  ]
}
```

**Nice!** It caught a typo in the email (comma instead of dot) and a phone number without country code. This will save us from importing bad data.

**Performance:** Validated 147 rows in 38ms. âœ… Under our 50ms target.

### Tool 2: preview_import

**Purpose:** Show what will be imported (requires database access to check for duplicates)

This tool should:
- Read the CSV file
- Query the database for existing contacts (by email)
- Identify new contacts vs updates
- Show potential conflicts (same email, different name)
- Estimate what will happen without actually doing it

I asked Claude Code:

```
Now let's implement preview_import. This should:
1. Validate the file first (call validate_import_data logic)
2. Read CSV with pandas
3. Query database to find existing contacts (match by email)
4. Generate a preview showing:
   - How many new contacts will be created
   - How many existing contacts will be updated
   - Any conflicts (same email, different name)
5. Return detailed preview for user to review

This accesses the database but doesn't make any changes.
```

**Code Snippet 12: preview_import Prompt**

Claude Code created `src/tools/preview.py`:

```python
"""Preview tools for import operations."""

import pandas as pd
from typing import Dict, List
from ..utils.db import execute_query
from ..utils.validators import is_valid_email

def preview_quicket_registrations(file_path: str, event_id: int) -> Dict:
    """Preview Quicket registration import."""
    
    # Read CSV
    df = pd.read_csv(file_path)
    
    # Get all emails from CSV
    emails = df['Email'].dropna().unique().tolist()
    
    # Query existing contacts
    if emails:
        placeholders = ','.join(['%s'] * len(emails))
        query = f"SELECT id, email, first_name, last_name FROM contacts WHERE email IN ({placeholders})"
        existing_contacts = execute_query(query, tuple(emails))
    else:
        existing_contacts = []
    
    # Build lookup map
    existing_by_email = {c['email']: c for c in existing_contacts}
    
    # Analyze what will happen
    new_contacts = []
    updated_contacts = []
    conflicts = []
    
    for _, row in df.iterrows():
        email = row['Email']
        first_name = row['First name']
        last_name = row['Surname']
        
        if email in existing_by_email:
            existing = existing_by_email[email]
            # Check if name matches
            if (existing['first_name'] != first_name or 
                existing['last_name'] != last_name):
                conflicts.append({
                    "email": email,
                    "csv_name": f"{first_name} {last_name}",
                    "db_name": f"{existing['first_name']} {existing['last_name']}",
                    "reason": "Name mismatch"
                })
            updated_contacts.append(email)
        else:
            new_contacts.append({
                "email": email,
                "name": f"{first_name} {last_name}"
            })
    
    # Count tickets (all rows = all tickets)
    ticket_count = len(df)
    
    return {
        "summary": {
            "total_rows": len(df),
            "new_contacts": len(new_contacts),
            "existing_contacts": len(updated_contacts),
            "new_tickets": ticket_count,
            "conflicts": len(conflicts)
        },
        "conflicts": conflicts[:10],  # Show first 10
        "sample_new_contacts": new_contacts[:5]  # Show first 5
    }
```

**Code Snippet 13: Preview Logic (Partial)**

Then registered in `server.py`:

```python
@mcp.tool()
def preview_import(
    file_path: str,
    import_type: Literal[
        "quicket_registrations",
        "quicket_checkins", 
        "checkin_walkins",
        "sessionize_speakers",
        "speaker_ratings"
    ],
    event_id: int = None
) -> dict:
    """
    Preview what will happen when importing a file.
    
    Checks database for existing records and shows:
    - How many new records will be created
    - How many existing records will be updated
    - Any conflicts that need resolution
    
    Args:
        file_path: Path to CSV file
        import_type: Type of import
        event_id: Event ID (required for some import types)
        
    Returns:
        {
            "summary": {
                "total_rows": int,
                "new_contacts": int,
                "existing_contacts": int,
                "new_tickets": int,
                "conflicts": int
            },
            "conflicts": [
                {
                    "email": str,
                    "csv_name": str,
                    "db_name": str,
                    "reason": str
                }
            ],
            "sample_new_contacts": [...]  # First 5 new contacts
        }
    """
    if import_type == "quicket_registrations":
        return preview_quicket_registrations(file_path, event_id)
    elif import_type == "sessionize_speakers":
        return preview_sessionize_speakers(file_path, event_id)
    # ... etc
```

**Code Snippet 14: preview_import MCP Tool**

### Testing preview_import

Testing with the same March 2025 file:

```python
result = preview_import(
    file_path='tests/test_data/quicket_reg_march2025.csv',
    import_type='quicket_registrations',
    event_id=5  # March 2025 event
)
```

Result:

```json
{
  "summary": {
    "total_rows": 147,
    "new_contacts": 144,
    "existing_contacts": 3,
    "new_tickets": 147,
    "conflicts": 1
  },
  "conflicts": [
    {
      "email": "john.smith@gmail.com",
      "csv_name": "John Smith",
      "db_name": "J. Smith",
      "reason": "Name mismatch"
    }
  ],
  "sample_new_contacts": [
    {"email": "sarah.jones@example.com", "name": "Sarah Jones"},
    {"email": "mike.wilson@example.com", "name": "Mike Wilson"},
    ...
  ]
}
```

Perfect! This tells me:
- 144 people are new (will create contacts)
- 3 people already exist in our database (will update)
- 1 conflict to review (John Smith's name is recorded differently)
- All 147 will get tickets created

**Performance:** 523ms (accessing database for 147 email lookups). Acceptable.

With our infrastructure tools done, let's move to the real work: actually importing data.

## Building Phase 1: High-Value Imports

Now for the main event: `import_quicket_registrations`. This is the most complex tool because it has to:
1. Parse CSV (handle encoding, column variations)
2. Detect duplicates (email-based)
3. Create/update contacts
4. Create tickets for each registration
5. Handle group purchases (multiple tickets per order)
6. Manage transactions (all-or-nothing)
7. Return detailed results

### Tool 3: import_quicket_registrations

I gave Claude Code a detailed prompt:

```
Let's implement import_quicket_registrations. This is the most complex import.

Requirements:
1. Parse Quicket CSV with pandas
2. For each row:
   - Check if contact exists (by email)
   - If new: INSERT into contacts
   - If exists and duplicate_strategy="update": UPDATE contact
   - If exists and duplicate_strategy="skip": skip
   - Create ticket record linked to contact
3. Use a database transaction (commit all or rollback all)
4. Handle group purchases (same Order Number)
5. Return detailed results

The CSV has these key fields:
- Order Number, Ticket Number, Purchase Date
- First name, Surname, Email, Cellphone
- Link Campaign (ticket type: Free, Complimentary, etc.)

Refer to our database schema from CLAUDE.md.
```

**Code Snippet 15: import_quicket_registrations Prompt**

Claude Code thought for a moment, then started building `src/tools/imports.py`.

Here's the implementation (I'll show the key parts):

```python
"""Import tools for event data."""

import pandas as pd
from typing import Literal
from datetime import datetime
from ..utils.db import get_db_connection

def import_quicket_registrations(
    file_path: str,
    event_id: int,
    duplicate_strategy: Literal["update", "skip", "create_new"] = "update"
) -> dict:
    """
    Import Quicket registration CSV.
    
    Creates/updates contacts and creates ticket records.
    """
    start_time = datetime.now()
    
    # Read CSV
    df = pd.read_csv(file_path)
    
    # Initialize counters
    stats = {
        "contacts_created": 0,
        "contacts_updated": 0,
        "contacts_skipped": 0,
        "tickets_created": 0,
        "errors": []
    }
    
    # Get database connection
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        try:
            for idx, row in df.iterrows():
                try:
                    # Extract data
                    email = row['Email'].strip().lower()
                    first_name = row['First name'].strip()
                    last_name = row['Surname'].strip()
                    phone = row['Cellphone'] if pd.notna(row['Cellphone']) else None
                    ticket_number = row['Ticket Number']
                    order_number = row['Order Number']
                    purchase_date = pd.to_datetime(row['Purchase Date'])
                    ticket_type = row['Link Campaign'] if 'Link Campaign' in row else 'Unknown'
                    
                    # Check if contact exists
                    cursor.execute(
                        "SELECT id FROM contacts WHERE email = %s",
                        (email,)
                    )
                    existing = cursor.fetchone()
                    
                    if existing:
                        contact_id = existing['id']
                        
                        if duplicate_strategy == "update":
                            # Update existing contact
                            cursor.execute("""
                                UPDATE contacts 
                                SET first_name = %s, last_name = %s, 
                                    phone = COALESCE(%s, phone),
                                    updated_at = CURRENT_TIMESTAMP
                                WHERE id = %s
                            """, (first_name, last_name, phone, contact_id))
                            stats["contacts_updated"] += 1
                            
                        elif duplicate_strategy == "skip":
                            stats["contacts_skipped"] += 1
                            # Still create ticket below
                            
                    else:
                        # Create new contact
                        cursor.execute("""
                            INSERT INTO contacts (email, first_name, last_name, phone)
                            VALUES (%s, %s, %s, %s)
                            RETURNING id
                        """, (email, first_name, last_name, phone))
                        contact_id = cursor.fetchone()['id']
                        stats["contacts_created"] += 1
                    
                    # Create ticket
                    cursor.execute("""
                        INSERT INTO tickets (
                            event_id, contact_id, ticket_number, order_number,
                            purchase_date, ticket_type, checked_in
                        )
                        VALUES (%s, %s, %s, %s, %s, %s, FALSE)
                        ON CONFLICT (ticket_number) DO NOTHING
                    """, (event_id, contact_id, ticket_number, order_number, 
                          purchase_date, ticket_type))
                    
                    if cursor.rowcount > 0:
                        stats["tickets_created"] += 1
                        
                except Exception as e:
                    stats["errors"].append({
                        "row": idx + 2,
                        "error": str(e)
                    })
            
            # Commit transaction
            conn.commit()
            
            # Calculate duration
            duration = (datetime.now() - start_time).total_seconds()
            
            return {
                "success": True,
                "stats": stats,
                "duration_seconds": round(duration, 2)
            }
            
        except Exception as e:
            # Rollback on error
            conn.rollback()
            return {
                "success": False,
                "error": str(e),
                "stats": stats
            }
```

**Code Snippet 16: import_quicket_registrations Implementation**

Key patterns Claude Code used:

**1. Transaction Management:**
```python
with get_db_connection() as conn:
    cursor = conn.cursor()
    try:
        # ... do work ...
        conn.commit()  # Success!
    except Exception as e:
        conn.rollback()  # Undo everything
        return {"success": False, "error": str(e)}
```

This ensures atomicity: either all records import or none do. No partial imports.

**2. Duplicate Detection:**
```python
cursor.execute("SELECT id FROM contacts WHERE email = %s", (email,))
existing = cursor.fetchone()

if existing:
    # Handle duplicate based on strategy
elif duplicate_strategy == "update":
    cursor.execute("UPDATE contacts SET ...")
```

**3. Graceful Error Handling:**
```python
try:
    # Process row
except Exception as e:
    stats["errors"].append({"row": idx + 2, "error": str(e)})
    # Continue processing other rows
```

Individual row errors don't stop the entire import.

**4. ON CONFLICT for Tickets:**
```python
INSERT INTO tickets (...) 
VALUES (...)
ON CONFLICT (ticket_number) DO NOTHING
```

If ticket number already exists (rare edge case), silently skip it.

### Registering the Tool

Claude Code added it to `server.py`:

```python
@mcp.tool()
def import_quicket_registrations(
    file_path: str,
    event_id: int,
    duplicate_strategy: Literal["update", "skip", "create_new"] = "update"
) -> dict:
    """
    Import Quicket registration CSV file.
    
    Creates contacts and ticket records. Handles duplicates based on strategy.
    
    Args:
        file_path: Path to Quicket CSV export
        event_id: Event ID to associate tickets with
        duplicate_strategy: How to handle duplicate emails
            - "update": Update existing contact info (default)
            - "skip": Keep existing contact info, only create ticket
            - "create_new": Create a second contact with same email
            
    Returns:
        {
            "success": bool,
            "stats": {
                "contacts_created": int,
                "contacts_updated": int,
                "contacts_skipped": int,
                "tickets_created": int,
                "errors": [{"row": int, "error": str}]
            },
            "duration_seconds": float
        }
    """
    from tools.imports import import_quicket_registrations as do_import
    return do_import(file_path, event_id, duplicate_strategy)
```

**Code Snippet 17: Registered import_quicket_registrations Tool**

### Testing the Import

Time for the moment of truth. I tested with real data:

```python
result = import_quicket_registrations(
    file_path='tests/test_data/quicket_reg_march2025.csv',
    event_id=5,
    duplicate_strategy='update'
)
```

Result:

```json
{
  "success": true,
  "stats": {
    "contacts_created": 144,
    "contacts_updated": 3,
    "contacts_skipped": 0,
    "tickets_created": 147,
    "errors": []
  },
  "duration_seconds": 1.23
}
```

**YES!** âœ… It worked on the first try!

- 144 new contacts created
- 3 existing contacts updated
- 147 tickets created
- Zero errors
- Completed in 1.23 seconds

I verified in the database:

```sql
SELECT COUNT(*) FROM contacts;
-- Result: 144 new + 3 updated = 147 total

SELECT COUNT(*) FROM tickets WHERE event_id = 5;
-- Result: 147 tickets
```

Perfect match. The import worked.

### The First Challenge: CSV Encoding

Actually, I lied. It didn't work on the first try. Here's what actually happened:

**First attempt:**

```
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 1847
```

The Quicket CSV export uses Windows-1252 encoding (common in South African exports), not UTF-8.

I told Claude Code about the error:

```
I'm getting a UnicodeDecodeError. The Quicket CSV uses Windows-1252 encoding, 
not UTF-8. Can you update the CSV reading to handle this?
```

Claude Code immediately fixed it:

```python
# Before:
df = pd.read_csv(file_path)

# After:
df = pd.read_csv(file_path, encoding='windows-1252', encoding_errors='replace')
```

**Code Snippet 18: Encoding Fix**

The `encoding_errors='replace'` ensures that any truly undecodable characters get replaced with ï¿½ instead of crashing.

**Lesson learned:** Real-world data is messy. AI helps, but you still need to test with actual files and handle edge cases.

### Tool 7: import_speaker_ratings

Before moving to Phase 2, let's implement one more high-value tool: speaker ratings. This data is crucial for speaker selection for future events.

The challenge here: **fuzzy matching**. The feedback form asks attendees to select sessions from a dropdown, but typos happen, or sessions get abbreviated. We need to match:

- "Introduction to Machine Learning with Python" (Sessionize)
- "Intro to Machine Learning - Python" (Feedback form)

I asked Claude Code to implement fuzzy matching using the `fuzzywuzzy` library (actually `thefuzz` now):

```
Let's implement import_speaker_ratings. The challenge is matching session 
titles from the feedback form to sessions in our database.

Use fuzzy string matching (fuzzywuzzy library) to handle:
- Typos in session titles
- Abbreviations ("Introduction to" â†’ "Intro to")
- Case differences

For each rating:
1. Fuzzy match the session title (threshold: 80% similarity)
2. If match found, insert rating linked to that session
3. If no match, add to unmatched list
4. Link ratings to ALL speakers for multi-speaker sessions

Add thefuzz to dependencies.
```

**Code Snippet 19: import_speaker_ratings Prompt**

Claude Code added `thefuzz` to `pyproject.toml` and implemented:

```python
from thefuzz import fuzz
from typing import Optional

def find_session_by_title(event_id: int, title: str, threshold: int = 80) -> Optional[int]:
    """Find session by fuzzy matching title."""
    query = """
        SELECT id, title FROM speaker_sessions 
        WHERE event_id = %s
    """
    sessions = execute_query(query, (event_id,))
    
    best_match = None
    best_score = 0
    
    for session in sessions:
        # Use token_sort_ratio (ignores word order)
        score = fuzz.token_sort_ratio(title.lower(), session['title'].lower())
        if score > best_score and score >= threshold:
            best_score = score
            best_match = session['id']
    
    return best_match, best_score if best_match else (None, 0)

def import_speaker_ratings(file_path: str, event_id: int) -> dict:
    """Import speaker ratings from Microsoft Forms."""
    df = pd.read_csv(file_path, encoding='windows-1252')
    
    stats = {
        "ratings_created": 0,
        "unmatched_sessions": [],
        "errors": []
    }
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        try:
            for idx, row in df.iterrows():
                session_title = row['Select the session you attended']
                session_rating = int(row['Session Rating'])
                speaker_rating = int(row['Speaker Rating'])
                comments = row['Comments on Speaker'] if pd.notna(row['Comments on Speaker']) else None
                
                # Find session by fuzzy matching
                session_id, match_score = find_session_by_title(event_id, session_title)
                
                if session_id:
                    # Insert rating
                    cursor.execute("""
                        INSERT INTO speaker_ratings (
                            session_id, session_rating, speaker_rating, 
                            comments, created_at
                        )
                        VALUES (%s, %s, %s, %s, CURRENT_TIMESTAMP)
                    """, (session_id, session_rating, speaker_rating, comments))
                    stats["ratings_created"] += 1
                else:
                    stats["unmatched_sessions"].append({
                        "row": idx + 2,
                        "title": session_title,
                        "reason": "No fuzzy match found (threshold 80%)"
                    })
            
            conn.commit()
            return {"success": True, "stats": stats}
            
        except Exception as e:
            conn.rollback()
            return {"success": False, "error": str(e), "stats": stats}
```

**Code Snippet 20: import_speaker_ratings with Fuzzy Matching**

### Testing Fuzzy Matching

I tested with a feedback form that had some variations:

```python
result = import_speaker_ratings(
    file_path='tests/test_data/speaker_ratings_march2025.csv',
    event_id=5
)
```

Result:

```json
{
  "success": true,
  "stats": {
    "ratings_created": 87,
    "unmatched_sessions": [
      {
        "row": 23,
        "title": "Deep Learning w/ TensorFlow",
        "reason": "No fuzzy match found (threshold 80%)"
      }
    ],
    "errors": []
  }
}
```

Out of 88 ratings:
- 87 matched successfully (using fuzzy matching)
- 1 didn't match (likely a session not in our database)

I manually checked row 23: the attendee wrote "Deep Learning w/ TensorFlow" but no session with "TensorFlow" in the title exists for this event. Makes senseâ€”probably selected wrong session in the form.

**Fuzzy matching works!** This saved us from manually mapping session titles.

At this point, we have the high-value tools working:
- âœ… validate_import_data
- âœ… preview_import
- âœ… import_quicket_registrations
- âœ… import_speaker_ratings

Time to finish Phase 2.

## Building Phase 2: Remaining Import Tools

The remaining tools follow similar patterns, so I'll show them more briefly.

### Tool 4: import_quicket_checkins

**Purpose:** Update attendance status for tickets

This is simpler than registrations because we're just updating existing records:

```python
def import_quicket_checkins(file_path: str) -> dict:
    """Import Quicket check-in CSV."""
    df = pd.read_csv(file_path, encoding='windows-1252')
    
    stats = {"tickets_updated": 0, "tickets_not_found": 0, "errors": []}
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        try:
            for idx, row in df.iterrows():
                ticket_number = row['Ticket Number']
                checked_in = row['Checked In'] == 'Yes'
                
                cursor.execute("""
                    UPDATE tickets 
                    SET checked_in = %s, updated_at = CURRENT_TIMESTAMP
                    WHERE ticket_number = %s
                """, (checked_in, ticket_number))
                
                if cursor.rowcount > 0:
                    stats["tickets_updated"] += 1
                else:
                    stats["tickets_not_found"] += 1
            
            conn.commit()
            return {"success": True, "stats": stats}
            
        except Exception as e:
            conn.rollback()
            return {"success": False, "error": str(e), "stats": stats}
```

**Code Snippet 21: import_quicket_checkins**

Edge case: If a ticket number isn't found (late registration that wasn't in the registration export), we note it but don't fail.

### Tool 5: import_checkin_walkins

**Purpose:** Handle walk-in attendees (not registered on Quicket)

```python
def import_checkin_walkins(file_path: str, event_id: int) -> dict:
    """Import walk-in attendees from Microsoft Forms."""
    df = pd.read_csv(file_path, encoding='windows-1252')
    
    stats = {"contacts_created": 0, "tickets_created": 0, "errors": []}
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        try:
            for idx, row in df.iterrows():
                email = row['Email Address'].strip().lower()
                first_name = row['First Name'].strip()
                last_name = row['Last Name'].strip()
                phone = row['Cell Phone Number'] if pd.notna(row['Cell Phone Number']) else None
                
                # Check if contact exists
                cursor.execute("SELECT id FROM contacts WHERE email = %s", (email,))
                existing = cursor.fetchone()
                
                if existing:
                    contact_id = existing['id']
                else:
                    cursor.execute("""
                        INSERT INTO contacts (email, first_name, last_name, phone)
                        VALUES (%s, %s, %s, %s)
                        RETURNING id
                    """, (email, first_name, last_name, phone))
                    contact_id = cursor.fetchone()['id']
                    stats["contacts_created"] += 1
                
                # Create ticket (walk-ins don't have ticket numbers, use -1)
                cursor.execute("""
                    INSERT INTO tickets (
                        event_id, contact_id, ticket_number, 
                        ticket_type, checked_in
                    )
                    VALUES (%s, %s, -1, 'Walk-in', TRUE)
                """, (event_id, contact_id))
                stats["tickets_created"] += 1
            
            conn.commit()
            return {"success": True, "stats": stats}
            
        except Exception as e:
            conn.rollback()
            return {"success": False, "error": str(e), "stats": stats}
```

**Code Snippet 22: import_checkin_walkins**

Walk-ins automatically have `checked_in = TRUE` since they're at the event, and we use `ticket_number = -1` as a sentinel value for "no Quicket ticket".

### Tool 6: import_sessionize_speakers

**Purpose:** Import speakers and their sessions

This one is interesting because sessions can have multiple speakers, and speakers can have multiple sessions:

```python
def import_sessionize_speakers(file_path: str, event_id: int) -> dict:
    """Import speakers and sessions from Sessionize CSV."""
    df = pd.read_csv(file_path, encoding='windows-1252')
    
    stats = {
        "contacts_created": 0,
        "contacts_updated": 0,
        "sessions_created": 0,
        "errors": []
    }
    
    # First pass: Create/update speakers (deduplicate by email)
    speakers_by_email = {}
    
    for _, row in df.iterrows():
        email = row['Email'].strip().lower()
        if email not in speakers_by_email:
            speakers_by_email[email] = {
                "first_name": row['First Name'],
                "last_name": row['Last Name'],
                "sessionize_id": row['Speaker Id']
            }
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        try:
            # Create/update speaker contacts
            speaker_ids = {}
            for email, data in speakers_by_email.items():
                cursor.execute("SELECT id FROM contacts WHERE email = %s", (email,))
                existing = cursor.fetchone()
                
                if existing:
                    contact_id = existing['id']
                    cursor.execute("""
                        UPDATE contacts 
                        SET first_name = %s, last_name = %s, sessionize_id = %s
                        WHERE id = %s
                    """, (data['first_name'], data['last_name'], 
                          data['sessionize_id'], contact_id))
                    stats["contacts_updated"] += 1
                else:
                    cursor.execute("""
                        INSERT INTO contacts (email, first_name, last_name, sessionize_id)
                        VALUES (%s, %s, %s, %s)
                        RETURNING id
                    """, (email, data['first_name'], data['last_name'], 
                          data['sessionize_id']))
                    contact_id = cursor.fetchone()['id']
                    stats["contacts_created"] += 1
                
                speaker_ids[email] = contact_id
            
            # Second pass: Create sessions and link speakers
            sessions_by_id = {}
            
            for _, row in df.iterrows():
                session_id = row['Session Id']
                
                if session_id not in sessions_by_id:
                    # Create session
                    cursor.execute("""
                        INSERT INTO speaker_sessions (
                            event_id, sessionize_id, title, description, 
                            track, session_type
                        )
                        VALUES (%s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (event_id, session_id, row['Title'], 
                          row['Description'], row['Track'], row['Session Type']))
                    db_session_id = cursor.fetchone()['id']
                    sessions_by_id[session_id] = db_session_id
                    stats["sessions_created"] += 1
                else:
                    db_session_id = sessions_by_id[session_id]
                
                # Link speaker to session (many-to-many)
                email = row['Email'].strip().lower()
                contact_id = speaker_ids[email]
                
                cursor.execute("""
                    INSERT INTO session_speakers (session_id, contact_id)
                    VALUES (%s, %s)
                    ON CONFLICT DO NOTHING
                """, (db_session_id, contact_id))
            
            conn.commit()
            return {"success": True, "stats": stats}
            
        except Exception as e:
            conn.rollback()
            return {"success": False, "error": str(e), "stats": stats}
```

**Code Snippet 23: import_sessionize_speakers**

The two-pass approach:
1. **First pass:** Deduplicate speakers (same speaker might appear multiple times if they have multiple sessions)
2. **Second pass:** Create sessions and link speakers via the `session_speakers` junction table

This correctly handles:
- Speakers with multiple sessions
- Sessions with multiple speakers (co-presented sessions)

### Final Tool Count

After implementing all tools, here's what we have:

```python
# In server.py
from fastmcp import FastMCP

mcp = FastMCP("Import MCP Server")

# Infrastructure (Phase 1)
@mcp.tool()
def validate_import_data(file_path: str) -> dict: ...

@mcp.tool()  
def preview_import(file_path: str, import_type: str, event_id: int = None) -> dict: ...

# Imports (Phase 1 + Phase 2)
@mcp.tool()
def import_quicket_registrations(file_path: str, event_id: int, duplicate_strategy: str = "update") -> dict: ...

@mcp.tool()
def import_quicket_checkins(file_path: str) -> dict: ...

@mcp.tool()
def import_checkin_walkins(file_path: str, event_id: int) -> dict: ...

@mcp.tool()
def import_sessionize_speakers(file_path: str, event_id: int) -> dict: ...

@mcp.tool()
def import_speaker_ratings(file_path: str, event_id: int) -> dict: ...

if __name__ == "__main__":
    mcp.run()
```

**Code Snippet 24: Complete MCP Server with 7 Tools**

**Final code size:** 847 lines (close to our 800-1000 estimate!)

Breakdown:
- `server.py`: 89 lines
- `tools/validation.py`: 156 lines
- `tools/preview.py`: 187 lines  
- `tools/imports.py`: 312 lines
- `utils/db.py`: 58 lines
- `utils/parsers.py`: 45 lines

All 7 tools working, tested with real data. Time to configure it in Claude Desktop.

## Configuring in Claude Desktop

Now for the moment we've been building toward: using our MCP server conversationally through Claude Desktop.

### Updating claude_desktop_config.json

I need to add our Import MCP server to Claude Desktop's configuration. The config file is at:
- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
- **Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

I opened it and added our server:

```json
{
  "mcpServers": {
    "postgres": {
      "command": "docker",
      "args": [
        "run", "--rm", "-i",
        "-e", "DATABASE_URI=postgresql://user:pass@host/db",
        "crystaldba/postgres-mcp:latest"
      ]
    },
    "import": {
      "command": "uv",
      "args": [
        "--directory", "/Users/niels/repos/event-contact-management-2/mcp-servers/import-mcp",
        "run", "python", "-m", "src.server"
      ],
      "env": {
        "DB_HOST": "my-neon-host.azure.neon.tech",
        "DB_PORT": "5432",
        "DB_NAME": "event_contact_db",
        "DB_USER": "neondb_owner",
        "DB_PASSWORD": "my-password",
        "DB_SSLMODE": "require"
      }
    }
  }
}
```

**Code Snippet 25: Claude Desktop Configuration**

Key points:
- **command**: `uv` (Python package manager)
- **args**: Run our server.py module
- **env**: Database credentials (so Import MCP can connect)

### Restarting Claude Desktop

**Critical:** Claude Desktop only reads the config at startup. I had to:

1. Completely quit Claude Desktop (right-click system tray icon â†’ Quit)
2. Wait a few seconds
3. Restart Claude Desktop from Applications

### Verifying Tools Are Available

I opened Claude Desktop and started a new conversation:

**Me:** "What MCP servers do you have access to?"

**Claude Desktop:**

```
I have access to two MCP servers:

1. **postgres** - Provides 8 database tools:
   - execute_sql
   - list_schemas
   - explain_query
   - analyze_db_health
   - get_top_queries
   - And more...

2. **import** - Provides 7 data import tools:
   - validate_import_data
   - preview_import
   - import_quicket_registrations
   - import_quicket_checkins
   - import_checkin_walkins
   - import_sessionize_speakers
   - import_speaker_ratings

Would you like to use any of these tools?
```

**Perfect!** âœ… Claude Desktop sees our Import MCP server and all 7 tools.

## The Magic Moment: Conversational Imports

This is it. The moment where 2-3 hours of manual CSV wrangling becomes a 2-minute conversation.

### First Import: Validation

I saved my Quicket registration export to `~/Downloads/quicket_march2025.csv`.

**Me:** "I have a Quicket registration file for the March 2025 event at ~/Downloads/quicket_march2025.csv. Can you validate it first?"

**Claude Desktop:** (thinking...)

```
Let me validate that file for you.

[Calls validate_import_data with file_path]

The file is valid! âœ…

**Summary:**
- Import type: Quicket registrations
- Total rows: 147
- Validation: Passed

**Issues found (2 warnings):**
1. Row 23: Invalid email format - "john.smith@gmail,com" (comma instead of period)
2. Row 45: Invalid phone format - "0825551234" (missing country code)

These are warnings, not errors. The import will skip rows with invalid emails, 
but you might want to correct them first. Would you like me to preview what 
will be imported, or go ahead with the import?
```

**Wow.** This is exactly what I wanted. Claude:
- Understood my natural language request
- Called the right tool
- Interpreted the results in a helpful way
- Gave me options for next steps

**Me:** "Thanks for catching those! Let me fix the CSV first... OK, fixed. Now can you preview the import?"

**Claude Desktop:**

```
[Calls preview_import with file_path, import_type="quicket_registrations", event_id=5]

Here's what will happen if you import this file:

**Contacts:**
- 144 new contacts will be created
- 3 existing contacts will be updated

**Tickets:**
- 147 tickets will be created for the March 2025 event

**Potential conflicts (1):**
- john.smith@gmail.com: Name mismatch
  - CSV says: "John Smith"
  - Database has: "J. Smith"
  - (Will update to "John Smith" if you proceed)

**Sample of new contacts:**
- Sarah Jones (sarah.jones@example.com)
- Mike Wilson (mike.wilson@example.com)
- Thandi Ndlovu (thandi.ndlovu@example.com)
- ... and 141 more

Ready to import? I can update existing contacts or skip them if you prefer.
```

**Me:** "Looks good! Please import and update the existing contacts."

**Claude Desktop:**

```
[Calls import_quicket_registrations with duplicate_strategy="update"]

âœ… Import complete!

**Results:**
- 144 new contacts created
- 3 contacts updated (including John Smith's full name)
- 147 tickets created for March 2025 event
- No errors
- Completed in 1.2 seconds

Would you like me to show you some statistics, like how many first-time 
attendees we have?
```

**Me:** "Yes please!"

**Claude Desktop:**

```
[Calls postgres MCP: execute_sql to query attendance history]

Looking at the March 2025 attendees:

**Attendance patterns:**
- 89 first-time attendees (60%)
- 38 returning from previous events (26%)
- 20 attended 2+ previous events (14%)

**Top returners:**
- Sarah Jones: 4th event (attended 2022, 2023, 2024, 2025)
- Lawrance Reddy: 4th event
- Niels Berglund: 5th event (attended all!)

Your community is growing! 60% new faces is healthy for expansion while 
maintaining a core group of engaged members.
```

**This is unreal.** I just:
1. Validated a CSV file
2. Previewed the import
3. Imported 147 registrations
4. Got audience insights

All through conversation. No Excel. No SQL. No manual matching. Total time: **2 minutes**.

### Second Import: Speaker Ratings

Let's try another oneâ€”speaker ratings with fuzzy matching.

**Me:** "I also have speaker feedback from the March 2025 event. The file is ~/Downloads/speaker_ratings_march2025.csv. Can you import it?"

**Claude Desktop:**

```
[Calls validate_import_data]

The file looks good! It's a speaker ratings export with 88 responses.

[Calls preview_import]

Since this is a speaker ratings import, I'll need to match session titles 
from the feedback form to sessions in your database. Let me check...

[Internally: tool queries sessions for event_id=5]

I can match most sessions using fuzzy matching (handling typos and abbreviations).

Would you like me to proceed with the import?
```

**Me:** "Yes please"

**Claude Desktop:**

```
[Calls import_speaker_ratings]

âœ… Speaker ratings imported!

**Results:**
- 87 ratings successfully imported
- 1 unmatched session (row 23: "Deep Learning w/ TensorFlow")

**Match quality:**
- Used fuzzy matching for 12 sessions (titles didn't match exactly)
- Average match confidence: 92%

**Unmatched session:**
Row 23 referenced "Deep Learning w/ TensorFlow" but no session with that 
title exists for March 2025. The attendee may have selected the wrong 
session in the form.

Would you like me to show you the top-rated sessions?
```

**Me:** "Yes!"

**Claude Desktop:**

```
[Calls postgres MCP: execute_sql with ratings query]

**Top-rated sessions from March 2025:**

1. **"Real-time Analytics with Apache Kafka"** - Niels Berglund
   - Session rating: 7.0/7.0
   - Speaker rating: 7.0/7.0
   - Comments: "Absolutely well spoken and learnt so much"

2. **"Introduction to Machine Learning with Python"** - Lungile Radebe
   - Session rating: 6.8/7.0
   - Speaker rating: 6.5/7.0
   - Comments: "Very interactive session. Really enjoyed the pair programming."

3. **"Data Lakes vs Data Warehouses"** - Lawrance Reddy
   - Session rating: 6.7/7.0
   - Speaker rating: 6.8/7.0
   - (Average of 5 ratings)

Looks like Kafka and ML were big hits! ðŸŽ‰
```

**Incredible.** Not only did it import the ratings with fuzzy matching, but Claude Desktop proactively:
- Told me about match quality
- Explained the unmatched session
- Showed me top-rated sessions
- Added context and interpretation

This is AI-native data management.

### The Complete Workflow

Let me show you the entire post-event workflow that used to take 2-3 hours:

**Traditional approach:**
1. Export Quicket registrations CSV (2 min)
2. Open in Excel (1 min)
3. Export check-ins CSV (2 min)
4. Open in Excel (1 min)
5. Manually match registrations to check-ins (20 min)
6. Export walk-ins from Forms (2 min)
7. Manually merge walk-ins (10 min)
8. Export Sessionize speakers (2 min)
9. Open in Excel, reformat (5 min)
10. Export feedback form (2 min)
11. Manually match sessions to ratings (30 min)
12. Deduplicate everything (15 min)
13. Import to Brevo (10 min)
14. **Total: ~2.5 hours**

**AI-native approach:**

**Me:** "I just finished the March 2025 event. I have 5 files to import. Let me give you the paths..."

**Claude Desktop:**

```
Great! Let me process these systematically:

1. Validating all files... âœ… All valid
2. Importing Quicket registrations... âœ… 147 tickets
3. Importing check-ins... âœ… 132 attended (90% attendance)
4. Importing walk-ins... âœ… 8 walk-ins added
5. Importing Sessionize speakers... âœ… 18 speakers, 19 sessions
6. Importing speaker ratings... âœ… 87 ratings

**Complete summary:**
- 152 unique attendees (144 Quicket + 8 walk-ins)
- 132 checked in (90% show-up rate)
- 18 speakers presented 19 sessions
- 87 feedback responses (66% response rate)

Your data is ready! Would you like me to:
- Show attendance trends vs previous events?
- Identify top speakers for invitation to future events?
- Generate email lists for follow-up communications?
```

**Total time: 3 minutes.** (Plus 1 minute to gather the file paths.)

From 2.5 hours to 4 minutes. That's **37x faster**. And more accurate (no manual matching errors).

## Lessons Learned & Real Challenges

Let me be honest about what worked and what was harder than expected.

### What Worked Well

**1. Self-contained architecture was the right choice**

Having each import as a single tool call made the conversational interface simple and fast. No multi-step orchestration needed.

**2. Infrastructure tools first (validate â†’ preview â†’ import)**

Building validation and preview first gave me confidence before running actual imports. Catching the email typo early saved a bad import.

**3. Real CSV files as test data**

Using actual Quicket/Sessionize exports as test files caught edge cases immediately (encoding issues, column name variations, group purchases).

**4. Claude Code as thinking partner**

Claude Code wasn't just generating codeâ€”it was suggesting patterns (context managers, transaction management, fuzzy matching) that I wouldn't have thought of immediately.

**5. FastMCP framework**

The `@mcp.tool()` decorator pattern is beautifully simple. I didn't spend any time fighting with MCP protocol details.

### What Was Harder Than Expected

**1. Duplicate detection is nuanced**

Email matching isn't always enough. I had cases where:
- Same email, different names (spouse/colleague using same email)
- Same person, different emails (work vs personal)

Solution: Added `duplicate_strategy` parameter to let user decide per-import.

**2. Transaction management is critical**

My first version didn't use transactions. When row 87 failed (bad data), rows 1-86 were already imported. I had to manually clean up.

Adding `try/except` with `conn.commit()` / `conn.rollback()` fixed this, but I should have done it from the start.

**3. CSV encoding issues**

Every single CSV export used different encoding:
- Quicket: Windows-1252
- Sessionize: UTF-8
- Microsoft Forms: UTF-8 with BOM

Solution: Added `encoding='windows-1252', encoding_errors='replace'` to handle most cases.

**4. Error messages for conversational interface**

Technical errors like "IntegrityError: duplicate key value violates unique constraint" aren't helpful for users in conversation.

I had to wrap database errors in user-friendly messages:

```python
except psycopg2.IntegrityError as e:
    if 'unique constraint' in str(e):
        return {"success": False, "error": "A ticket with this number already exists"}
    else:
        return {"success": False, "error": f"Database error: {str(e)}"}
```

Still a work in progress.

**5. Large file performance**

With 147 rows, imports take ~1 second. But what about 1,000+ row files?

Current implementation processes rows one by one. For large files, I'd need to:
- Batch inserts (100 rows at a time)
- Show progress updates ("Processed 500/1000...")
- Use pandas `to_sql()` for bulk operations

This is a **future enhancement** (Part 8: Production Hardening).

### Production Considerations

We've built an MVP that works great for my use case (events with 100-300 attendees). For production use at larger scale, we'd need:

**1. Logging & audit trails**
- Who imported what, when?
- What data changed?
- Can we roll back imports?

**2. Error recovery**
- Partial import failures (row 87 fails, but rows 1-86 succeed)
- Resume from failure point
- Manual correction workflows

**3. Large file handling**
- Batch processing (don't load 10,000 rows into memory)
- Progress indicators
- Async processing (don't block for 5 minutes)

**4. Concurrent imports**
- What if two people import simultaneously?
- Database locking strategy
- Queue-based processing

**5. Data validation UI**
- Show unmatched records for manual review
- Conflict resolution interface
- Preview with more detail

These are all solvable, but beyond MVP scope. I'll tackle them in Part 8 when we prepare for real production use.

## What We've Accomplished

Let's quantify what we built today.

### Technical Achievements

âœ… **7 working MCP tools**
- 2 infrastructure tools (validate, preview)
- 5 import tools (registrations, check-ins, walk-ins, speakers, ratings)

âœ… **Self-contained architecture**
- Each tool does complete operation (no orchestration needed)
- Uses pandas + psycopg2 + FastMCP (battle-tested libraries)
- ~850 lines of production-quality code

âœ… **Real-world features**
- Fuzzy matching for session titles (handles typos)
- Duplicate detection with configurable strategies
- Transaction management (atomic operations)
- CSV encoding handling (Windows-1252, UTF-8)
- Group purchase support (multiple tickets per order)

âœ… **Integration with Claude Desktop**
- Configured in claude_desktop_config.json
- Verified all 7 tools accessible
- Tested with real event data

âœ… **Conversational workflows**
- Natural language import requests
- Validation â†’ Preview â†’ Import flow
- Automatic insights and follow-up suggestions

### Time Investment vs Time Saved

**Development time:** ~2-3 hours with Claude Code
- Project setup: 15 min
- Infrastructure tools: 30 min
- High-value imports: 45 min
- Remaining imports: 30 min
- Testing & configuration: 30 min

**Time saved per event:** ~2-3 hours of manual CSV wrangling

**Break-even:** After the first import! Every subsequent event is pure time savings.

**Annual impact:** 
- 2 events/year Ã— 2.5 hours saved = **5 hours/year**
- At consulting rates ($150/hr), that's $750/year in saved time
- But more importantly: I can spend those 5 hours on content, speakers, or community instead of Excel

### The Paradigm Shift Realized

We didn't just build faster import tools. We built a system where **the interface IS the conversation**.

No admin dashboard. No import forms. No CSV field mapping UI. Just:

**Me:** "Import the March 2025 registrations"
**Claude Desktop:** "Done. 147 tickets imported. Want to see attendance stats?"

This is the promise of AI-native development. And it's not vaporwareâ€”it's working, right now, in production for my real events.

## What's Next: Part 8 Preview

In the next post, we'll tackle **email integration and production hardening**:

### Part 8: Email Integration with Brevo MCP

**What we'll build:**
- Set up the existing Brevo MCP server (`@houtini/brevo-mcp`)
- Create email templates for speakers and attendees
- Conversational email workflows:
  ```
  "Email all 2024 speakers about the 2025 call for papers"
  "Send thank-you emails to March 2025 attendees"
  ```
- Email history tracking (log all sends to database)

**Production hardening:**
- Logging & audit trails (who imported what, when?)
- Error recovery (resume failed imports)
- Large file handling (batch processing)
- Concurrent import safety (database locking)
- Preview UI improvements (show more detail)

**The complete picture:**
- Import data conversationally âœ… (Part 7 - done!)
- Communicate conversationally ðŸš€ (Part 8 - next!)
- Analyze conversationally âœ… (Already working via Postgres MCP)

We're building toward the complete AI-native event management workflow.

## ~ Finally

That's all for this post! We accomplished something significant today:

**We built a custom MCP server (847 lines) in ~2-3 hours that transforms 2-3 hours of manual work per event into a 2-minute conversation.**

### Key Takeaways

**1. MCP servers are isolated**
- They can't call each other
- LLM orchestrates, servers execute
- Self-contained tools = simpler architecture

**2. Leverage libraries, not other MCP servers**
- Use pandas, psycopg2, thefuzz (same libraries other MCP servers use)
- 80% proven libraries + 20% custom logic = 100% value

**3. Design first, implement second**
- Part 6's design discussion made today's implementation straightforward
- Claude Code as thinking partner > Claude Code as code generator

**4. Real-world data reveals real challenges**
- CSV encoding issues
- Duplicate detection strategies
- Fuzzy matching needs
- You only discover these by testing with actual files

**5. The conversational interface is transformative**
- Not just fasterâ€”fundamentally different
- From "fill out this form" to "import the registrations"
- AI interprets, suggests, provides insights automatically

### Your Turn

If you're following along, try building a simple MCP server for your own use case:

1. **Identify a repetitive data task** (CSV imports, data exports, report generation)
2. **Design with Claude Code** (what tools do you need? what's the architecture?)
3. **Build incrementally** (infrastructure first, then high-value features)
4. **Test with real data** (not just sample dataâ€”real, messy files)
5. **Experience the conversational interface** (configure in Claude Desktop)

The pattern we followed works for any domain-specific MCP server.

### Have questions or thoughts?

**Ping me:** niels.it.berglund@gmail.com
**Follow on LinkedIn:** [linkedin.com/in/nielsberglund](https://www.linkedin.com/in/nielsberglund/)

**Found this helpful?** Share it with your network! The AI-native development journey is one we're all taking together.

### See you in Part 8 where we add email superpowers! ðŸš€

---

**â† [Previous Post: Part 6 - Architecture and Design](/post/2026-01-14-building-an-event-management-system-with-claude-code-part-6---architecture-and-design-of-a-custom-import-mcp-server/)**

**Next Post â†’ Part 8 - Email Integration and Production Hardening (Coming soon)**

---

*Copyright Â© Niels Berglund 2026*