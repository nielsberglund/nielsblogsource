Above, where we discussed the `@parallel` parameter, we said that we could not use `@parallel` for certain things, model creation for example, as the execution of the parallel "streams", are independent of each other. An example of this is if we were to take the code in *Code Snippet 3* were we created a model from the `rxLinMod` function and altered the code somewhat:

```sql title: "Model in Parallel"
EXEC sp_execute_external_script
      @language = N'R'
    , @script = N'
        pid <- Sys.getpid()
        myModel <- rxLinMod(y ~ rand1 + rand2 + rand3 + rand4 + rand5, 
                      data=InputDataSet)
        model <- serialize(myModel, NULL);
        modelbinstr = paste(model, collapse = "")
        OutputDataSet <- data.frame(ProcessId = pid, 
                                    nRows=myModel$nValidObs, 
                                    theModel = modelbinstr)'
   , @input_data_1 = N'SELECT TOP(2400000) y, rand1, rand2, rand3, 
                                           rand4, rand5 
                       FROM dbo.tb_Rand_3M 
                       WHERE  rand5 >= 10           
                       OPTION(querytraceon 8649, MAXDOP 4)'
   , @parallel = 1
WITH RESULT SETS ((ProcessId int, Rows int NOT NULL, 
                   ModelAsVarchar nvarchar(max) NULL));
```
**Code Snippet 9:** *Model Creation in Parallel*

The code in *Code Snippet 9* creates a model using parallel execution (`@parallel = 1`), based on 2,400,000 rows and outputs a dataset with process id, the number of rows processed and the model created converted to a string. When we execute the code in *Code Snippet 9* the output is:

{% img center /images/posts/sql_r_services_ext_scriptIII_ssms4.png 796 130 %}
**Figure 13:** *Parallel Model Creation*

Oh, that was probably not what we wanted - four different models from four different process id's. We see that the models are different based on the outlined, in red, part of *Figure 13*. So what we have seen right now should tell us that executing in parallel is not always a good thing.

Hmm, by now some of you, my dear readers, probably voice your objections and say something like: "Hey, what do you mean? I have read about all these RevoScaleR functions, optimised for Big Data and parallel processing. Is that just a marketing ploy?" No, it is not, it is correct that the  **RevoScaleR** functions are optimised, and have parallel capabilities. However, for parallel processing, they need the help of the **SQL Server Compute Context**.

As part of functionality in RevoScaleR is the ability to define where a workload executes. By default, it executes on your local machine, but you can also set it to execute somewhere else: Hadoop, Spark, etc., and also SQL Server. When you set up a compute-context, you can define how many tasks your workload should run under, and this is what we use now.

> **NOTE:** I am not going into details of compute-contexts and how they work in this post. Future posts may cover that.

To use a compute context we take the code in *Code Snippet 9* and alter it somewhat:

```sql title: "Compute Context"
DECLARE @inputData nvarchar(max) = N'SELECT y, rand1, rand2, rand3, 
                                     rand4, rand5 
                                    FROM dbo.tb_Rand_30M 
                                    TABLESAMPLE(75 PERCENT) 
                                    REPEATABLE(98074)'

EXEC sp_execute_external_script
      @language = N'R'
    , @script = N'
        pid <- Sys.getpid()

        sqlConnString <- "Driver=SQL Server;Server=.; 
                          Database=TestParallel;uid=sa;pwd=sapwd"

        sqlCtx <- RxInSqlServer(connectionString = sqlConnString, 
                                        numTasks = 2)  
        rxSetComputeContext(sqlCtx)
        ds <- RxSqlServerData(connectionString=sqlConnString, sqlQuery=inData)

        myModel <- rxLinMod(y ~ rand1 + rand2 + rand3 + rand4 + rand5, 
                      data=ds)

        model <- serialize(myModel, NULL);
        modelbinstr = paste(model, collapse = "")
        OutputDataSet <- data.frame(ProcessId = pid, 
                              nRows=myModel$nValidObs, 
                              theModel = modelbinstr)'
    , @input_data_1 = N''
    , @params = N'@inData nvarchar(max)'
    , @inData = @inputData
WITH RESULT SETS ((ProcessId int, Rows int NOT NULL, 
                   ModelAsVarchar nvarchar(max) NULL));
```
**Code Snippet 10:** *Model Creation with Compute Context*

A couple of things to notice in *Code Snippet 10*:

* We do not send in the dataset to use from SQL Server, but we let the execution engine retrieve it, and we define the query to use in the `@inputData` parameter, used by the `@inData` parameter which we defined in the `@params` collection.
* We use the table with 30 million rows to make it easier to see what happens.
* In the script we set up a connection string to the server on which we want to execute on.
* We use the [`RxInSqlServer`][1] function to create a compute-context, and in the function, we say we want to use 2 tasks (processes). We use the `numTasks` parameter for that. You should however be aware that SQL Server reserves the right to start fewer processes if there is not enough data, or if SQL Server uses too many resources already, or if numTasks exceeds the `MAXDOP` configuration option. 
* We set the compute-context for this execution using [`rxSetComputeContext`][2].
* We retrieve the data we want using [`RxSqlServerData`][3].

To see how this works:

* Run *Process Explorer* as admin.
* Restart the launchpad service (this is to clean-up any RTerm processes).
* Navigate to the `Launchpad.exe` process in *Process Explorer*.
* Execute the code in *Code Snippet 10*.

While the code is executing, do as we did before and look under the `Launchpad.exe` process in *Process Explorer*. You should see something like so:

{% img center /images/posts/sql_r_services_ext_scriptIII_procmon6.png 455 640 %}
**Figure 14:** *Output from Using Compute Context*

In *Figure 14* we see how two RTerm processes are active, (outlined in red), so it looks like we executed the code in parallel. The question now is what the result was - will we see two rows coming back, similar to what we saw in *Figure 13*, or? Look at the *Results* tab in *SSMS*, and you see:

{% img center /images/posts/sql_r_services_ext_scriptIII_ssms5.png 615 70 %}
**Figure 15:** *Model Creation Compute Context*

Aha, only one row comes back, so it looks like each process builds a part of the model, the parts are merged, and you get the result back. I have done some ["spelunking"][4] with *Process Monitor* (I won't cover that here) and from what I can see I believe the flow looks something like so:

{% img center /images/posts/sql_r_services_ext_scriptIII_compute_context.png 800 530 %}
**Figure 16:** *Model Creation Compute Context*

We see in *Figure 16* how:

1. We call `sp_execute_external_script` and SQL Server calls into the launchpad service.
1. The launchpad service creates RTerm processes which in turn creates BxlServer processes. One process becomes the "main" process
1. TCP connection from the SqlSatellite in the "main" process get established.
1. A compute-context is created.
1. The compute-context and the SqlSatellites establish connections based on `numTasks`.
1. The compute-context pushes data to the SqlSatellites
1. The `BxlServer.exe` processes the data.
1. Processed data merge back to the "main" process.
1. SQL Server receives data back from the "main" process.