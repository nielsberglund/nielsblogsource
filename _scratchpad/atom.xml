<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="http://nielsberglund.com/atom.xml" rel="self"/>
  <link href="http://nielsberglund.com/"/>
  <updated>2018-09-09T21:11:32+02:00</updated>
  <id>http://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36]]></title>
    <link href="http://nielsberglund.com/2018/09/09/interesting-stuff---week-36/" rel="alternate" type="text/html"/>
    <updated>2018-09-09T21:11:32+02:00</updated>
    <id>http://nielsberglund.com/2018/09/09/interesting-stuff---week-36/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/articles/Async-Streams">Async Streams in C# 8</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about, as the title says, C# support for async streams. This, proposed, new functionality in C# is to combine the async/awaiting feature with a yielding operator. Very interesting!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/microservices-post-kubernetes">Microservices in a Post-Kubernetes Era</a>. Another article from <a href="https://www.infoq.com/">InfoQ</a>. This article questions some of the original microservices ideas and acknowledges the fact that they are not standing as strong in the post-Kubernetes era as they were before. Well worth a read!</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/09/05/introduction-to-azure-durable-functions/">Introduction to Azure Durable Functions</a>. This post, from one of the .NET engineering teams is about Azure Durable Functions. Azure Durable functions is a new programming model based on Microsoft serverless’ platform Azure Functions. It allows you to write a workflow as code and have the execution run with the scalability and the reliability of serverless with high throughput.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/category/devops/">DevOps for Data Science </a>. A series of posts by <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> where he discusses various aspects of DevOps in a Data Science world. This is a must read!</li>
<li><a href="https://www.confluent.io/blog/putting-power-apache-kafka-hands-data-scientists/">Putting the Power of Apache Kafka into the Hands of Data Scientists</a>. A blogpost which combines two of my favorite topics: Kafka and Data Science, how good is that?! The post discusses how they at <a href="https://www.stitchfix.com/">Stitch Fix</a> exposes a multitude of data sources to their Data Scientists, and allow the Data Scientists to create new topics etc., on the fly. This is a <strong>MUST</strong> read post!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/ebook/i-heart-logs-event-data-stream-processing-and-data-integration/">I Heart Logs: Event Data, Stream Processing, and Data Integration</a>. Registration page for downloading <a href="https://twitter.com/jaykreps">Jay Kreps</a> e-book &ldquo;I Heart Logs&rdquo;. If you are interested in streaming in general, you <strong>SHOULD</strong> really get this book!</li>
<li><a href="https://data-artisans.com/blog/serializable-acid-transactions-on-streaming-data">Serializable ACID Transactions on Streaming Data</a>. Guys, (and girls), this is <strong>BIG</strong>. This blogpost introduces <em>data Artisans Streaming Ledger</em>, a new technology that brings serializable ACID transactions to applications built on a streaming architecture!</li>
<li><a href="https://www.confluent.io/blog/data-wrangling-apache-kafka-ksql">Data Wrangling with Apache Kafka and KSQL</a>. A post by <a href="https://twitter.com/rmoff">Robin Moffat</a> about how we can use Kafka and KSQL to manipulate data.</li>
<li><a href="https://www.infoq.com/articles/democratizing-stream-processing-kafka-ksql-part2">Democratizing Stream Processing with Apache Kafka® and KSQL - Part 2</a>. Part 2 in a series of posts by <a href="https://twitter.com/rmoff">Robin Moffat</a> about Kafka and KSQL. In this post Robin covers how Apache Kafka® and KSQL can be used to build powerful data integration and processing applications. You find Part 1 in the series <a href="https://www.infoq.com/articles/democratizing-stream-processing-apache-kafka-ksql">here</a>.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>This weekend I did two talks at SQL Saturday in Cape Town:</p>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>I also did a full day workshop on the Friday (September 7) before the event: <strong><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a></strong>. I had 16 people attending, and I believe it went quite well!</p>

<p>Now there is only one SQL Saturday left, in Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<p>In Durban I also do a full day workshop on the Friday, (September 14), before the event: <strong><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">A Day of SQL Server Machine Learning Services with Niels Berglund</a></strong>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35]]></title>
    <link href="http://nielsberglund.com/2018/09/02/interesting-stuff---week-35/" rel="alternate" type="text/html"/>
    <updated>2018-09-02T08:38:38+02:00</updated>
    <id>http://nielsberglund.com/2018/09/02/interesting-stuff---week-35/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>The content this week is a bit meagre, due to me not having had time to browse around that much, as I have been &ldquo;prepping&rdquo; for the upcoming <strong>SQL Saturdays</strong>.</p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://www.microsoft.com/en-us/research/blog/optimizing-imperative-functions-in-relational-databases-with-froid/">Optimizing imperative functions in relational databases with Froid</a>. A post from Microsoft Research, where they discuss <strong>Froid</strong>. Froid is an extensible framework for optimising imperative programs in relational databases, and the purpose is to enable developers to use the abstraction of UDFs without compromising on performance.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/hadoop-for-beginners">Hadoop for Beginners- Part 1</a>. The first post in a series about, as the title says, Hadoop. This series is really worthwhile reading if you are interested in what Hadoop is and what it can do for you.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/apache-kafka-talk-series/">Apache Kafka: Online Talk Series</a>. This is a registration page for an on-demand video series about Kafka. I bring the popcorn, and you bring the coke!</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>In a couple of previous roundups I have mentioned that the SQL Saturday &ldquo;season&rdquo; is here, and yesterday, (September, 1), I flew out to Johannesburg and did a presentation about SQL Server Machine Learning Services, <a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a> to around 40 people.</p>

<p>The event was a smashing success thanks to the awesome arrangements by <a href="https://twitter.com/MikeJohnsonZA/">Michael Johnson</a> and his fellow volunteers!</p>

<p>Having done Johannesburg, next in turn is Cape Town:</p>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>and finally Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 34]]></title>
    <link href="http://nielsberglund.com/2018/08/26/interesting-stuff---week-34/" rel="alternate" type="text/html"/>
    <updated>2018-08-26T10:22:37+02:00</updated>
    <id>http://nielsberglund.com/2018/08/26/interesting-stuff---week-34/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="http://mattwarren.org/2018/08/21/Monitoring-and-Observability-in-the-.NET-Runtime/">Monitoring and Observability in the .NET Runtime</a>. Yet another awesome blog-post by <a href="https://twitter.com/matthewwarren">Matthew</a>. This post covers how we can monitor .NET through <em>Diagnostics</em>, <em>Profiling</em> and <em>Debugging</em>.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/chaos-engineering-introduction">Chaos Engineering: Building Immunity in Production Systems</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation discussing Chaos Engineering, its purpose, how to go about it, metrics to collect, the purpose of monitoring and logging, etc.</li>
</ul>

<h2 id="databases-storage">Databases / Storage</h2>

<ul>
<li><a href="https://queue.acm.org/detail.cfm?id=3236388">Mind Your State for Your State of Mind</a>. A paper by <a href="https://twitter.com/pathelland">Pat Helland</a>, where Pat explores the evolution of computation from a single process to microservices, the evolution of storage from files to key-value, and how they interact. Just as a side-note, you should read anything by Pat. He certainly knows what he is talking about!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/pakk-your-alpakka-reactive-streams-integrations-for-aws-azure-google-cloud">Pakk Your Alpakka: Reactive Streams Integrations For AWS, Azure, &amp; Google Cloud</a>. The link of this post had Cloud in the title, but I believe it fits better into <em>Streaming</em>. Anyway, this post links to a webinar about <a href="https://akka.io/blog/2016/08/23/intro-alpakka">Alpakka</a>. Alpakka is an integration framework for Akka Streams and the webinar looks at how Alpakka can be used for integrations with other systems.</li>
<li><a href="https://www.youtube.com/watch?v=p9LBi11KR2c">Pat Helland | Kafka Summit 2017 Keynote (Standing on the Distributed Shoulders of Giants)</a>. Speaking about <a href="https://twitter.com/pathelland">Pat</a>. Here is a YouTube video from his Kafka Summit keynote in 2017. It is based on a paper he published in 2016: <a href="https://queue.acm.org/detail.cfm?id=2953944">Standing on Distributed Shoulders of Giants</a>.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/logical-index-organization-in-cosmos-db.html">Logical index organization in Cosmos DB</a>. Another Cosmos DB post by <a href="https://twitter.com/muratdemirbas">Murat</a>. In this post, he looks at Cosmos DB&rsquo;s logical indexing subsystem.</li>
<li><a href="https://towardsdatascience.com/from-big-data-to-micro-services-how-to-serve-spark-trained-models-through-aws-lambdas-ebe129f4849c">From Big Data to micro-services: how to serve Spark-trained models through AWS lambdas</a>. This blog-post looks at how you take a Spark trained model, deploy it to AWS and expose it as an AWS Lambda endpoint. Very cool!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2018/08/22/snorkel-rapid-training-data-creation-with-weak-supervision/">Snorkel: rapid training data creation with weak supervision</a>. In this post <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper which tackles one of the central questions in supervised machine learning: how do you get a large enough set of training data to power modern deep models?</li>
<li><a href="http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/">Serverless Machine Learning with ML.NET and Azure Functions</a>. Earlier in this weeks roundup, I linked to a post about Spark models and AWS Lambdas. This post talks about training a classification model using <a href="https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet">ML.NET</a> and deploy it with <a href="https://azure.microsoft.com/en-us/services/functions/">Azure Functions</a>.</li>
<li><a href="https://blogs.msdn.microsoft.com/data_insights_global_practice/2018/08/22/measuring-model-goodness-part-1/">Measuring Model Goodness – Part 1</a>. This post, which is part one of two-part series, is focused on measuring model goodness, specifically looking at quantifying business value and converting typical machine learning performance metrics (like precision, recall, RMSE, etc.) to business metrics.</li>
<li><a href="http://101.datascience.community/2018/08/24/microsoft-weekly-data-science-news-for-august-24-2018">MICROSOFT WEEKLY DATA SCIENCE NEWS FOR AUGUST 24, 2018</a>. I found the <a href="https://blogs.msdn.microsoft.com/data_insights_global_practice/2018/08/22/measuring-model-goodness-part-1/">Measuring Model Goodness</a> post above thanks to <a href="https://twitter.com/ryanswanstrom">Ryan&rsquo;s</a> blog and this post. Ryan&rsquo;s blog is awesome if you are interested in what Microsoft does in data science and AI!</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>The third post in the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series, which I promised a couple of weeks ago would soon be finished has to wait a bit. Reason for this is my prep for the upcoming SQL Saturdays.</p>

<p>As usual I present in Johannesburg, Cape Town and Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/785/EventHome.aspx">Johannesburg</a>, September 1:

<ul>
<li><a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a>.</li>
</ul></li>
</ul>

<p><img src="/images/posts/sqlsat793_speaking_300x225.png" alt="" /></p>

<ul>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 33]]></title>
    <link href="http://nielsberglund.com/2018/08/19/interesting-stuff---week-33/" rel="alternate" type="text/html"/>
    <updated>2018-08-19T09:20:03+02:00</updated>
    <id>http://nielsberglund.com/2018/08/19/interesting-stuff---week-33/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/canopy-scalable-tracing-analytics-facebook">Canopy: Scalable Distributed Tracing &amp; Analysis @ Facebook</a>. This post links to an <a href="https://www.infoq.com/">InfoQ</a> presentation about <strong>Canopy</strong>, which is Facebook&rsquo;s performance and efficiency tracing infrastructure. The presentation covers lessons learned applying Canopy and present case studies of its use in solving various performance and efficiency challenges. Very interesting!</li>
<li><a href="https://azure.microsoft.com/en-us/resources/designing-distributed-systems/">Designing Distributed Systems</a>. A download link to an e-book: <strong>Designing Distributed Systems</strong>. The e-book provides repeatable, generic patterns, and reusable components to make developing reliable systems easier and more efficient. It is written by <a href="https://twitter.com/brendandburns">Brendan Burns</a> who is a Distinguished Engineer at Microsoft and works on Azure.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/schema-agnostic-indexing-with-azure.html">Schema-Agnostic Indexing with Azure Cosmos DB</a>. In this blog post, <a href="https://twitter.com/muratdemirbas">Murat</a> dissects a white paper about the schema-agnostic indexing subsystem of Cosmos DB. The post (and the paper) is very interesting, go ahead and read it, please!</li>
<li><a href="https://azure.microsoft.com/en-us/blog/azure-hdinsight-interactive-query-simplifying-big-data-analytics-architecture-and-operations/">Azure #HDInsight Interactive Query: simplifying big data analytics architecture</a>. This post discusses a new feature of Hive 2, Low Latency Analytics Processing (LLAP). LLAP produces significantly faster queries on raw data stored in commodity storage systems such as Azure Blob store or Azure Data Lake Store. This is quite exciting, and I need to check it out!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/build-udf-udaf-ksql-5-0">How to Build a UDF and/or UDAF in KSQL 5.0</a>. Not one week without at least one Kafka related post - that is the &ldquo;law&rdquo;. This post discusses a new feature in <strong>KSQL</strong> 5, the ability for the users o write their own functions for KSQL to use. Think about the possibilities that open up!</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/neural-networks-from-a-bayesian-perspective">Neural Networks from a Bayesian Perspective</a>. This post covers different ways to obtain uncertainty in Deep Neural Networks from a Bayesian perspective. The post is quite theoretical but very interesting!</li>
<li><a href="https://databricks.com/blog/2018/08/15/100x-faster-bridge-between-spark-and-r-with-user-defined-functions-on-databricks.html">100x Faster Bridge between Apache Spark and R with User-Defined Functions on Databricks</a>. Spark exposes an API, SparkR User Defined Function API, which acts as a bridge between Spark and R. Unfortunately the bridge is far from efficient. Databricks has made the bridge more efficient when you run Spark on Databricks, and this post talks about how it is done.</li>
<li><a href="https://towardsdatascience.com/the-most-important-part-of-a-data-science-project-is-writing-a-blog-post-50715f37833a">The most important part of a data science project is writing a blog post</a>. A somewhat provocative title of this blog post, but it makes a good point. Always document your data science projects so other data scientists can see what you have achieved!</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>It is that time of the year again: SQL Saturday season! As usual I present in Johannesburg, Cape Town and Durban:</p>

<ul>
<li><a href="http://www.sqlsaturday.com/785/EventHome.aspx">Johannesburg</a>, September 1:

<ul>
<li><a href="http://www.sqlsaturday.com/785/Sessions/Details.aspx?sid=84967">Overview SQL Server Machine Learning Services</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/793/EventHome.aspx">Cape Town</a>, September 8:

<ul>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84975">Azure Machine Learning</a>.</li>
<li><a href="http://www.sqlsaturday.com/793/Sessions/Details.aspx?sid=84978">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
<li><a href="http://www.sqlsaturday.com/803/EventHome.aspx">Durban</a>, September 15:

<ul>
<li><a href="http://www.sqlsaturday.com/803/Sessions/Details.aspx?sid=85097">The Ins and Outs of sp_execute_external_script</a>.</li>
</ul></li>
</ul>

<p>Even if you are not interested in the topics I present, please register and come and listen to a lot of interesting talks by some of the industry&rsquo;s brightest people.</p>

<h3 id="precon">PreCon</h3>

<p>This year I also do precons in Cape Town and Durban on the Friday before the SQL Saturday event. My precons is a day where we talk about <strong>SQL Server Machine Learning Services</strong>, what it is and what we can do with it. It is in a format so if you want you can bring your laptop and code along as the day progresses.</p>

<p>The precon is not free, but hey &hellip;</p>

<ul>
<li><a href="https://www.quicket.co.za/events/47683-sqlsaturday-cape-town-2018-precon-a-drill-down-into-sql-server-machine-learning/#/">Cape Town, September 7 - A Drill Down Into SQL Server Machine Learning Services with Niels Berglund</a>.</li>
<li><a href="https://www.quicket.co.za/events/55545-sqlsaturday-durban-precon-2018-a-day-of-sql-server-machine-learning-services/#/">Durban, September 14 - A Day of SQL Server Machine Learning Services with Niels Berglund</a>.</li>
</ul>

<p>Even though the titles of the precons are different, I cover the same material.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Test Post and Feed Apologies]]></title>
    <link href="http://nielsberglund.com/2018/08/18/test-post-and-feed-apologies/" rel="alternate" type="text/html"/>
    <updated>2018-08-18T09:38:50+02:00</updated>
    <id>http://nielsberglund.com/2018/08/18/test-post-and-feed-apologies/</id>
    <content type="html"><![CDATA[<p>In my <a href="/2018/08/18/goodbye-jekyll-welcome-hugo/">Goodbye Jekyll, Welcome Hugo!</a> post I wrote how I have changed blog engine from Jekyll to Hugo, and how everything seemed to work. Saying that obviously jinxed it, as after I deployed to the hosting site my Atom feed was completely hosed.</p>

<p>I have now spent 4 hours trying to fix everything, but I am not sure I have succeeded. So this is a test post to see if things look OK.</p>

<p></p>

<p>I also want to apologise if I have messed up your feeds, as at one stage I saw in my feeds a lot of duplicates posts from me. Sorry!</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Goodbye Jekyll, Welcome Hugo!]]></title>
    <link href="http://nielsberglund.com/2018/08/18/goodbye-jekyll-welcome-hugo/" rel="alternate" type="text/html"/>
    <updated>2018-08-18T05:03:30+02:00</updated>
    <id>http://nielsberglund.com/2018/08/18/goodbye-jekyll-welcome-hugo/</id>
    <content type="html"><![CDATA[<p>Back in 2013 I wrote a <a href="/2013/10/02/moving-to-a-new-blog-engine/">post</a> about how I moved from a self hosted <strong>WordPress</strong> blog to a static blog; <a href="http://octopress.org"><strong>OctoPress</strong></a>. OctoPress spoke to my &ldquo;geeky&rdquo; side, as the foundation of it is the static blog-generator <a href="https://jekyllrb.com/"><strong>Jekyll</strong></a>, which in turn is Ruby based. In fact, OctoPress is more or less just some extra Ruby plugins on top of Jekyll, and to generate sites with OctoPress / Ruby you need Ruby installed on you machine.</p>

<p>I thought that by changing the blog platform, I might write more posts just due to the &ldquo;geekiness&rdquo; of the blog engine. Fast forward to the end of 2016, and the number of posts I had written since the switch came to a grand total of three. Those three includes the post where I announced the switch. Yeah, switching increased indeed my productivity - NOT!</p>

<p>So what does this have to do with anything?</p>

<p></p>

<p>Before I get into the reason for this post, let us look at what a static site generator is.</p>

<h2 id="static-site">Static Site</h2>

<p>A static site generator is a framework that takes source files and generates an entirely static website. We deploy the files that make up the site to the web server, and when a user requests a page, the web server returns that page to the user. This opposed to something like WordPress, where WordPress builds the page from a number of templates, gets the content and other site data from the database and sends the complete HTML page back to the user.</p>

<p>The advantages of a static site are that it is usually less complicated than a dynamic site; no templates, no database and so forth. Quite often serving a page to the user is better performing as the page is not dynamically created.</p>

<p>The downside of a static site is that you build it for each time you do a change to a page, adding a post, and so on, and depending on the size of your site (number of pages etc.), the build can take a while. So that is where my problem lies and the reason for this post.</p>

<h2 id="problem">Problem</h2>

<p>I mentioned above how I didn&rsquo;t manage to write any blog posts up to the end of 2016. Since then, however, I have been reasonably productive in my writing - and managed at least one post per week. It helps when you have cool stuff like <strong>SQL Server Machine Learning Services</strong> to write about. While it is cool that I produce posts, what I noticed was that the build time of the site took longer and longer. I did not think much about it until a couple of weeks ago when I had just finished the <a href="/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/">sp_execute_external_script and SQL Compute Context - III</a> post and tried to generate the site. It did not work; it just hung, what to do?</p>

<p>I ended up removing all OctoPress plugins and edited all files that referenced the plugins, to be able to run a bare-bones Jekyll generated site. I eventually managed to get it to work again, but this made me look around for other site generators.</p>

<h2 id="hugo">Hugo</h2>

<p>In my looking around for static site generators I came across <a href="https://gohugo.io/">Hugo</a>. Like Jekyll it is an open source static site generator, but it is built on <a href="https://golang.org/"><strong>Go</strong></a> instead of Ruby. One of the differences between Hugo and Jekyll is that when you generate a site with Jekyll you execute Ruby commands, and, as I mentioned above, you need Ruby installed. Hugo, on the other hand, comes as an executable <code>Hugo.exe</code> and you do not need Go installed at all. That Hugo is a self contained executable is a big plus in my book, since I have had versioning issues with Ruby a couple of times.</p>

<p>The most significant difference between Jekyll and Hugo though is speed when generating a site, or at least that is what the Hugo website says: &ldquo;<em>The world’s fastest framework for building websites</em>&rdquo;.</p>

<p>With the above points in mind, I decided to try and convert this blog from Jekyll to Hugo.</p>

<h2 id="blog-conversion">Blog Conversion</h2>

<p>So, I spent a couple of hours a day for around a week converting my posts and pages to Hugo, and it was not that difficult. The biggest issue was how Jekyll refers to posts: <code>{% post_url 2018-08-04-sp-execute-external-script-and-sql-compute-context---iii %}</code> versus <code>/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/</code>, and I had a lot of those. I eventually wrote a small <code>C#</code> console application that trawled through the posts and did the conversion. Apart from that everything is pretty straightforward. Obviously there are differences, but nothing earthshattering, as far as I can tell.</p>

<p>What about the speed then? Well, to build the Jekyll site took around 20 seconds, to build the site using Hugo takes around 2 seconds! Based on this, I decided to switch to Hugo, and this is the first post that I publish using Hugo as blog site generator. I hope I have not missed anything and that all is Ok. If not, well then, hopefully, you, my readers, <a href="mailto:niels.it.berglund@gmail.com">tell me</a> if you notice something amiss.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 32]]></title>
    <link href="http://nielsberglund.com/2018/08/12/interesting-stuff---week-32/" rel="alternate" type="text/html"/>
    <updated>2018-08-12T12:02:02+02:00</updated>
    <id>http://nielsberglund.com/2018/08/12/interesting-stuff---week-32/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/the-many-faces-of-consistency.html">The many faces of consistency</a>. A blog post by <a href="https://twitter.com/muratdemirbas">Murat</a> where he dissects a white paper about consistency. The paper talks about two types of consistency: <em>state</em> and <em>operation</em>. Seeing that Murat now does sabbatical work at Microsoft (see below), he compares the two consistency types with what Cosmos DB provides. The post is a must read if you are the least interested in distributed computing and consistency.</li>
</ul>

<h2 id="cloud-big-data">Cloud / Big Data</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/globally-replicated-data-lakes-with-livedata-using-wandisco-on-azure/">Globally replicated data lakes with LiveData using WANdisco on Azure</a>. The post discusses how you can achieve globally replicated Azure Data Lakes. The replication can be both hybrid: on-prem to Azure as well as Azure to Azure.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2018/08/09/cloud-data-and-ai-services-training-roundup-august-2018/">Cloud data and AI services training roundup August 2018</a>. This post lists some free data and AI training sessions.</li>
<li><a href="http://muratbuffalo.blogspot.com/2018/08/azure-cosmos-db.html">Azure Cosmos DB</a>. This post by <a href="https://twitter.com/muratdemirbas">Murat</a> is about his first impressions of Azure Cosmos DB. Murat has taken a sabbatical and spends a year at Microsoft in the Cosmos DB team. I look forward to more posts by Murat about Cosmos DB, and other Azure related topics.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://charlla.com/howto-docker-on-windows/">HowTo - Docker on Windows</a>. My mate and colleague, <a href="https://twitter.com/charllamprecht">Charl</a> continues his blogging journey. This post is how to run Docker on Windows.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/streaming-data-dominates-over-2000-developers-say-only-batch-is-almost-extinct">Streaming Data Dominates: Over 2000 Developers Say “Only Batch” Is Almost Extinct</a>. A survey by <a href="https://www.lightbend.com/">Lightbend</a>, (formerly known as <a href="https://en.wikipedia.org/wiki/Lightbend">Typesafe</a>), makes it clear that developers now moves more and more towards real-time processing as opposed to batch. That, my friends, is music to my ears!</li>
<li><a href="https://data-artisans.com/blog/apache-flink-1-6-0-whats-new-in-the-latest-apache-flink-release">Apache Flink 1.6.0: What’s new in the latest Apache Flink release</a>. What the title says; this is a post detailing some of the new features in the Flink 1.6.0 release.</li>
<li><a href="https://www.confluent.io/blog/getting-started-apache-kafka-kubernetes/">Getting Started with Apache Kafka and Kubernetes</a>. A blog-post about the work done to enable Kafka to run on Kubernetes. The post points to a white paper: <a href="https://www.confluent.io/resources/recommendations-for-deploying-apache-kafka-on-kubernetes">Run Confluent Platform on Kubernetes Using Best Practices</a> which is really good!</li>
<li><a href="https://www.confluent.io/blog/kafka-streams-action">Kafka Streams in Action</a>. A post about the upcoming book: <a href="https://www.manning.com/books/kafka-streams-in-action">Kafka Streams in Action</a>. Apart from announcing the book, the post also contains the foreword to the book. This book is a must if you are interested in Kafka Streams!</li>
<li><a href="https://databricks.com/blog/2018/08/09/building-a-real-time-attribution-pipeline-with-databricks-delta.html">Building a Real-Time Attribution Pipeline with Databricks Delta</a>. this blog post looks at how to use the Databricks DataFrame API to build Structured Streaming applications and use Databricks Delta to query the streams in near-real-time.</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="https://dzone.com/articles/model-serving-stream-processing-vs-rpc-rest-with-j">Model Serving: Stream Processing vs. RPC/REST With Java, gRPC, Apache Kafka, TensorFlow</a>. A short and sweet blog-post comparing stream processing applications with a model serving infrastructure, like <strong>TensorFlow Serving</strong>, for serving machine learning models.</li>
<li><a href="http://blog.revolutionanalytics.com/2018/08/ieee-language-rankings-2018.html">IEEE Language Rankings 2018</a>. A post by <a href="https://twitter.com/revodavid">David</a> about the latest IEEE Spectrum language rankings.</li>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/scalable-iot-ml-platform-with-apache-kafka-deep-learning-mqtt">Scalable IoT ML Platform with Apache Kafka + Deep Learning + MQTT</a>. This post describes a hybrid machine learning infrastructure leveraging Apache Kafka as a scalable central nervous system. Very interesting!</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I have started on the third post in the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I hope to be able to publish it in a week or so.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 31]]></title>
    <link href="http://nielsberglund.com/2018/08/05/interesting-stuff---week-31/" rel="alternate" type="text/html"/>
    <updated>2018-08-05T08:01:44+02:00</updated>
    <id>http://nielsberglund.com/2018/08/05/interesting-stuff---week-31/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/08/02/tiered-compilation-preview-in-net-core-2-1/">Tiered Compilation Preview in .NET Core 2.1</a>. A blog-post about a new feature in .NET Core 2.1: <strong>Tiered Compilation</strong>. Tiered Compilation allows .NET to have multiple compilations for the same method that can be hot-swapped at runtime. This should improve compile times drastically!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://charlla.com/whatsthat-kafka/">Apache Kafka - Whats That</a>. This post about Kafka is by a colleague and a good friend of mine, <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a>. In the post, he takes us through a very succinct overview of Kafka. Charl is &ldquo;Mr Kafka&rdquo; at <a href="/Derivco">Derivco</a>, and he knows his &ldquo;stuff&rdquo;. Please be sure to follow his <a href="https://charlla.com/">blog</a> for more about Kafka (Charl, no pressure, hey?!).</li>
<li><a href="https://www.confluent.io/blog/decoupling-systems-with-apache-kafka-schema-registry-and-avro/">Decoupling Systems with Apache Kafka, Schema Registry and Avro</a>. An excellent post on how to decouple the systems you integrate via Kafka by using the <a href="https://www.confluent.io/confluent-schema-registry/">Confluent Schema Registry</a>. An added bonus in this post is that the code is .NET code!</li>
<li><a href="https://data-artisans.com/blog/a-practical-guide-to-broadcast-state-in-apache-flink">A Practical Guide to Broadcast State in Apache Flink</a>. This article discusses <strong>Broadcast State</strong>, a new feature in Apache Flink 1.5. With Broadcast State you can evaluate dynamic patterns on event streams by combining and jointly process two streams of events in a specific way.</li>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-5-0/">Introducing Confluent Platform 5.0</a>. As the title says, this post introduces the latest version of Confluent Platform: 5.0. Lots and lots of new interesting features. Go and have a look!</li>
<li><a href="https://www.confluent.io/landing-page/microservices-online-talk-series/">Apache Kafka for Microservices: A Confluent Online Talk Series</a>. This post is a link to a three-part online talk series which introduces fundamental concepts, use cases and best practices for getting started with microservices and Kafka.</li>
</ul>

<h2 id="big-data-cloud">Big Data / Cloud</h2>

<ul>
<li><a href="https://databricks.com/blog/2018/07/31/processing-petabytes-of-data-in-seconds-with-databricks-delta.html">Processing Petabytes of Data in Seconds with Databricks Delta</a>. In my roundups lately, I have covered Databricks Delta quite a bit and discussed how efficient it is processing lots and lots of data. This blog post takes a look under the hood and examines what makes Databricks Delta capable of sifting through petabytes of data within seconds. If you, like me, are interested in knowing how &ldquo;stuff&rdquo; works under the covers, then this post is a must-read!</li>
<li><a href="https://eng.uber.com/databook/">Databook: Turning Big Data into Knowledge with Metadata at Uber</a>. This post is about <strong>Databook</strong>, Uber’s in-house platform that surfaces and manages metadata about the internal locations and owners of specific datasets, and allows Uber to turn data into knowledge.</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="http://blog.revolutionanalytics.com/2018/08/r-python-in-sql-server.html">Video: How to run R and Python in SQL Server from a Jupyter notebook</a>. A short post by <a href="https://twitter.com/revodavid">David</a> linking to a video showing how to run Python and R from inside SQL Server.</li>
<li><a href="https://www.infoq.com/minibooks/emag-real-world-machine-learning">The InfoQ eMag: Real-World Machine Learning: Case Studies, Techniques and Risks</a>. An <a href="https://www.infoq.com/">InfoQ</a> link to an eMag focusing on the current landscape of machine-learning technologies and real-world case studies of applied machine learning.</li>
<li><a href="https://blogs.technet.microsoft.com/machinelearning/2018/07/31/3-steps-to-build-your-first-intelligent-app-conference-buddy/">3 Steps to Build Your First Intelligent App – Conference Buddy</a>. A blog-post which takes us through how to build an application utilising AI.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<ul>
<li><a href="/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/">sp_execute_external_script and SQL Compute Context - III</a>. I finally managed to finish and publish the third post in the <a href="/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series. In this post we use WinDbg, Process Monitor and WireShark to look in detail what happens in SQL Server when we use RxSqlServerData to pull data.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[sp_execute_external_script and SQL Compute Context - III]]></title>
    <link href="http://nielsberglund.com/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/" rel="alternate" type="text/html"/>
    <updated>2018-08-04T16:05:46+02:00</updated>
    <id>http://nielsberglund.com/2018/08/04/sp_execute_external_script-and-sql-compute-context---iii/</id>
    <content type="html"><![CDATA[<p>In the <a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services - sp_execute_external_script - III</a> post I wrote about <code>sp_execute_external_script</code> (SPEES) and the <strong>SQL Server Compute Context</strong> (SQLCC). Afterwards I realised I had some things wrong, so I wrote a followup post: <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">sp_execute_external_script and SQL Compute Context - I</a> where I tried to correct my mistakes from the <a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">initial post</a>. That post led to <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a> and now we have a mini-series.</p>

<p>To see other posts (including this) in the series, go to <a href="/spees_and_sql_compute_context"><strong>sp_execute_external_script and SQL Server Compute Context</strong></a>.</p>

<p>In the previous post in this series, we looked at how data is sent to the SqlSatellite from SQL Server when we are in the SQLCC. This post was meant to look at what goes on inside SQL Server when we execute in SQLCC, but I realised that it would make more sense if, before we look at the internal working when in SQLCC, I covered what happens when pulling data in the local context. So that is what this post is all about.</p>

<p></p>

<p>Before we dive into todays topic let us recap.</p>

<h2 id="recap">Recap</h2>

<p>In <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> we discussed what the SQLCC is and we said that as part of RevoScaleR, you can define where a workload executes. By default, it executes on your local machine, but you can also set it to execute in the context of somewhere else: Hadoop, Spark and also SQL Server. So, in essence, you can run some code on your development machine and have it execute in the environments mentioned above. To use the SQLCC in SQL Server we use <code>RxInSqlServer</code> and <code>rxSetComputeContext</code>:</p>

<pre><code class="language-r"># set up the connection string
sqlServerConnString &lt;- &quot;Driver=SQL Server;
                        server=.; # localhost
                        database=testParallel;
                        uid=some_uid;pwd=some_pwd&quot;

# set up the context
sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                        numTasks = 1)
# set the compute context to be the sql context
rxSetComputeContext(sqlCtx)    
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Set up SQL Server Compute Context</em></p>

<p>The connection string we see in <em>Code Snippet 1</em> indicates the where we execute, not necessarily where the data we work with lives. The <code>numTasks</code> argument defines the maximum number of tasks SQL Server can use. Something interesting when setting <code>numTasks</code> to be greater than 1 is that when we run the code, we run it hosted in a <code>mpiexec.exe</code> process. If we run in SQLCC under <code>numTasks = 1</code> we do not see <code>mpiexec.exe</code>, but we see another <code>bxlserver.exe</code> process. We also said that when we run in SQLCC. <code>sp_execute_external_script</code> executes multiple times.</p>

<p>The most interesting thing that came out of <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> was the performance benefit of using SQLCC when loading large datasets. So in <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a>, we tried to see where that performance benefit came from.</p>

<p>In <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> we said we had three ways of getting data into an R script:</p>

<ul>
<li>Push using <code>@input_data_1</code>.</li>
<li>Pull using <code>RxSqlServerData</code> in the local context.</li>
<li>Pull using <code>RxSqlServerData</code> and the SQLCC.</li>
</ul>

<p>We saw that to use SQLCC we need to pull the data; we could not push it. We also saw that for large datasets there was a significant performance difference between pulling in the local context and pulling using SQLCC. The interesting point was that pushing data and pulling using SQLCC had the same performance characteristics.</p>

<p>When pulling the data (<code>RxSqlServerData</code>), we use ODBC, and the protocol is TDS. However, when we use the SQLCC, the BXL protocol is also used and that gives us very efficient processing of data which is the reason we see good performance.</p>

<h2 id="housekeeping">Housekeeping</h2>

<p>Before we go any further let us look at the code and the tools we use today. This section is here for those who want to follow along in what we are doing in the post.</p>

<h4 id="helper-tools">Helper Tools</h4>

<p>To help us figure out the things we want, we use:</p>

<ul>
<li><em>Process Monitor</em> - to filter out TCP traffic.</li>
<li><em>WinDbg</em> - to see what happens inside SQL Server. If you need help with setting it up, we covered that in <a href="/2017/03/18/microsoft-sql-server-r-services---internals-i/">Microsoft SQL Server R Services - Internals I</a>.</li>
<li><em>WireShark</em> - to &ldquo;sniff&rdquo; network packets. We covered setting up <em>WireShark</em> in <a href="/2017/08/29/microsoft-sql-server-r-services---internals-x/">Internals - X</a>. Please remember that if you run SSMS and SQL Server on the same machine, then you need the <a href="https://nmap.org/npcap/"><strong>Npcap</strong></a> packet sniffer library instead of the default <strong>WinPcap</strong>.</li>
</ul>

<h4 id="code">Code</h4>

<p>This is the database objects we use in this post:</p>

<pre><code class="language-sql">USE master;
GO

SET NOCOUNT ON;
GO

DROP DATABASE IF EXISTS TestParallel;
GO

CREATE DATABASE TestParallel;
GO

USE TestParallel;
GO

DROP TABLE IF EXISTS dbo.tb_Rand_50M
GO
CREATE TABLE dbo.tb_Rand_50M
(
  RowID bigint identity PRIMARY KEY, 
  y int NOT NULL, rand1 int NOT NULL, 
  rand2 int NOT NULL, rand3 int NOT NULL, 
  rand4 int NOT NULL, rand5 int NOT NULL,
);
GO

INSERT INTO dbo.tb_Rand_50M(y, rand1, rand2, rand3, rand4, rand5)
SELECT TOP(50000000) CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 25 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 50 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 100 AS INT)
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO

</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Setup of Database, Table and Data</em></p>

<p>We use more or less the same database and database object as in the <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> post:</p>

<ul>
<li>A database: <code>TestParallel</code>.</li>
<li>A table: <code>dbo.tb_Rand_50M</code>. This table contains the data we want to analyse.</li>
</ul>

<p>In addition to creating the database and the table <em>Code Snippet 2</em> also loads 50 million records into the <code>dbo.tb_Rand_50M</code>. Be aware that when you run the code in <em>Code Snippet 2</em> it may take some time to finish due to the loading of the data. Yes, I know - the data is entirely useless, but it is a lot of it, and it helps to illustrate what we want to do.</p>

<p>Not only is the database and database objects similar to what we used in <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a>, the code we use is also almost the same:</p>

<pre><code class="language-sql">DECLARE @isCtx bit = 0;
DECLARE @numTasks int = 1;
EXEC sp_execute_external_script
      @language = N'R'
    , @script = N'
      # set up the connection string
      sqlServerConnString &lt;- &quot;Driver=SQL Server;server=.;
                              database=testParallel;
                              uid=&lt;username&gt;;pwd=&lt;userpwd&gt;&quot;
      
      if(useContext == 1) {
        sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                                numTasks = tasks)
        # set the compute context to be the sql context
        rxSetComputeContext(sqlCtx)
      }

      mydata &lt;- RxSqlServerData(sqlQuery = &quot;SELECT y, rand1, rand2, 
                                            rand3, rand4, rand5 
                                            FROM dbo.tb_Rand_50M&quot;,
                                connectionString = sqlServerConnString);
                        
      myModel &lt;- rxLinMod(y ~ rand1 + rand2 + rand3 + rand4 + rand5, 
                      data=mydata)'
    , @params = N'@tasks int, @useContext bit'
    , @tasks = @numTasks
    , @useContext = @isCtx
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Test Code</em></p>

<p>As we see in <em>Code Snippet 3</em> we parameterise the <code>sp_execute_external_script</code> call, and we have parameters for whether to use the SQLCC and also how many tasks to run when executing in the context. The default is to execute in the local context, and when we execute in SQLCC the default for the number of tasks is 1 (<code>numTasks = 1</code>).</p>

<h2 id="local-context-pull-vs-push">Local Context Pull vs Push</h2>

<p>Let us try and see what happens in SQL Server when we execute in the local context, and we pull data (<code>RxSqlServerData</code>) compared to when we push data (<code>@input_data_1</code>).</p>

<p>First, let us try and understand when and how the <code>SELECT</code> statement in <code>Code Snippet 3</code> executes. To check this we use the same technique as we did in <a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">Microsoft SQL Server R Services - Internals XIII</a>, we execute a <code>SELECT</code> statement which causes a division by zero exception. We capture that exception in <em>WinDbg</em> and we check the call stack.</p>

<p>So, run <em>WinDbg</em> as admin and enable an event-filter, so the debugger breaks at a <code>C++ EH</code> exception. In the <em>Event Filters</em> dialog you can set how that particular exception should be handled. We want it to be enabled, but not handled:</p>

<p><img src="/images/posts/sql_r_services_windbg_eventfilters_enable.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Enable C++ EH Exception</em></p>

<p>When the <em>Event Filter</em> is enabled as in <em>Figure 1</em> we:</p>

<ul>
<li>Attach <em>WinDbg</em> to the <code>sqlservr.exe</code> process (please do not do this on a production machine - <em>#justsaying</em>).</li>
<li>Set a breakpoint at <code>bp sqllang!SpExecuteExternalScript</code>.</li>
<li>Change the <code>SELECT</code> statement in <em>Code Snippet 3</em> slightly, so it generates the division by zero exception mentioned above: <code>SELECT TOP(1) (1 / y - y), y, rand1, ...</code>.</li>
</ul>

<p>After changing the <code>SELECT</code> statement, we execute the code. Continue through the breakpoint at <code>SpExecuteExternalScript</code>, and when the execution breaks at the exception we check the call stack: <code>k</code>:</p>

<pre><code class="language-cpp">KERNELBASE!RaiseException+0x68
...
sqllang!CXStmtQuery::ErsqExecuteQuery+0x5a2
sqllang!CXStmtSelect::XretExecute+0x2f2
sqllang!CMsqlExecContext::ExecuteStmts&lt;1,1&gt;+0x4c5
sqllang!CMsqlExecContext::FExecute+0xaae
sqllang!CSQLSource::Execute+0xa2c
sqllang!process_request+0xe52
sqllang!process_commands_internal+0x289
sqllang!process_messages+0x213
sqldk!SOS_Task::Param::Execute+0x231
...
ntdll!RtlUserThreadStart+0x21
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Call Stack Local Context Pull</em></p>

<p>The call stack we see in <em>Code Snippet 4</em> is somewhat edited in that it shows only the important parts. Let us compare it with the call-stack we see when we push the data, (<code>@input_data_1</code>), and receive a division by zero exception as we did in <a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">Microsoft SQL Server R Services - Internals XIII</a>:</p>

<pre><code class="language-cpp">KERNELBASE!RaiseException+0x68
...
sqllang!CXStmtQuery::ErsqExecuteQuery+0x49f
sqllang!CXStmtSelect::XretExecute+0x2f2
sqllang!CMsqlExecContext::ExecuteStmts&lt;1,1&gt;+0x4c5
sqllang!CMsqlExecContext::FExecute+0xaae
sqllang!CSQLSource::Execute+0xa2c
sqllang!SpExecuteExternalScript+0x154b
sqllang!CSpecProc::ExecuteSpecial+0x31e
sqllang!CXProc::Execute+0x139
sqllang!CSQLSource::Execute+0xb5b
sqllang!CStmtExecProc::XretLocalExec+0x2d3
sqllang!CStmtExecProc::XretExecExecute+0x4a1
sqllang!CXStmtExecProc::XretExecute+0x38
sqllang!CMsqlExecContext::ExecuteStmts&lt;1,1&gt;+0x4c5
sqllang!CMsqlExecContext::FExecute+0xaae
sqllang!CSQLSource::Execute+0xa2c
sqllang!process_request+0xe52
sqllang!process_commands_internal+0x289
sqllang!process_messages+0x213
sqldk!SOS_Task::Param::Execute+0x231
...
ntdll!RtlUserThreadStart+0x21
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Call Stack Local Context Push</em></p>

<p>The call-stack in <em>Code Snippet 5</em> is also edited and when we compare the two call-stacks, they look somewhat similar.  We see <code>sqllang!CXStmtQuery::ErsqExecuteQuery</code> in both code snippets, and we know from <a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">Internals - XIII</a> that <code>ErsqExecuteQuery</code> handles execution of SQL statements and also sending data to the SQL Satellite. So that makes sense that we see it in <em>Code Snippet 4</em>. However, what does not make sense is that we do not see <code>SpExecuteExternalScript</code> in the <em>Code Snippet 4</em> call stack (and no, I did not edit it out). What goes on here?</p>

<p>Ok, let us refer back to what we saw in <a href="/2017/10/31/microsoft-sql-server-r-services---internals-xii/">Internals - XII</a> and  <a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">Internals - XIII</a>, how SQL Server sends various data packages to the SqlSatellite and then finally calls <code>sqllang!CSatelliteCargoContext::SendChunkEndMessage</code>. So, add a breakpoint, in addition to the <code>SpExecuteExternalScript</code> breakpoint, at <code>sqllang!CSatelliteCargoContext::SendChunkEndMessage</code> and execute the code again. Oh, we see how we hit the <code>SpExecuteExternalScript</code> breakpoint first, followed by <code>SendChunkEndMessage</code>, and finally, we break at the division by zero exception. That is interesting, SQL Server sends the chunk end message and only after that, the query executes.</p>

<p>So what with <code>ErsqExecuteQuery</code>, what does that do? Let us see if we can find out:</p>

<ul>
<li>Change the code not to cause a divide by zero exception: <code>SELECT TOP(1) y, rand1, ...</code>.</li>
<li>Set a breakpoint at <code>ErsqExecuteQuery</code>.</li>
</ul>

<p>When you execute the code you see something like so:</p>

<pre><code class="language-cpp">0:099&gt; g
Breakpoint 0 hit
sqllang!SpExecuteExternalScript:
0:091&gt; g
Breakpoint 2 hit
sqllang!CXStmtQuery::ErsqExecuteQuery:
0:091&gt; g
Breakpoint 1 hit
sqllang!CSatelliteCargoContext::SendChunkEndMessage:
0:091&gt; g
Breakpoint 2 hit
sqllang!CXStmtQuery::ErsqExecuteQuery:
0:078&gt; g
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Breakpoints Hit</em></p>

<p>In <em>Code Snippet 6</em> we see how we first hit the <code>SpExecuteExternalScript</code> breakpoint, followed by <code>ErsqExecuteQuery</code>, (which is expected), then we hit the <code>SendChunkEndMessage</code> followed by a second <code>ErsqExecuteQuery</code>. That is interesting; two <code>ErsqExecuteQuery</code>, and we know from above that the query executes after the second <code>ErsqExecuteQuery</code>. So the question is now how the data flows from SQL Server to the SqlSatellite. We know from [Internals - XIII<a href="/2017/11/11/microsoft-sql-server-r-services---internals-xiii/">si13</a>, as we mentioned above, that when we push data, SQL Server makes these calls (among many others):</p>

<ul>
<li><code>sqlmin!CQScanUdx::PushNextChildRow</code>.</li>
<li><code>sqllang!CUDXR_ExternalScript::PushRow</code>.</li>
<li><code>sqllang!CSatelliteCargoContext::SendPackets</code>.</li>
</ul>

<p>Let us set a breakpoint at <code>sqllang!CSatelliteCargoContext::SendPackets</code> and see what happens when we execute.</p>

<p>THAT was interesting! We never hit <code>SendPackets</code>. In the recap above we said that when we pull the data we use ODBC and the TDS protocol, and what we see here further ensures that is the case. So what does SQL Server call when it uses TDS to send data? To find out:</p>

<ul>
<li>Disable the breakpoints at <code>SendChunkEndMessage</code> and <code>SendPackets</code>.</li>
<li>Execute the query.</li>
</ul>

<p>When you reach <code>ErsqExecuteQuery</code> the second time, do a <code>wt  -l4</code> (watch and trace 4 levels deep) and continue. Look at the output from the <code>wt</code>:</p>

<pre><code class="language-cpp">sqllang!CXStmtQuery::ErsqExecuteQuery
  ...   
    sqllang!CXStmtQuery::SetupQueryScanAndExpression
      sqlmin!CQuery::CreateExecPlan
        sqldk!CMemProc::CpagesInUse
      ...
      sqlmin!CQueryScan::StartupQuery
  ...      
  sqlmin!CQueryScan::GetRow
    sqlmin!CQScanTopNew::GetRow
      sqlmin!CQScanTableScanNew::GetRow
        sqlmin!CQScanRowsetNew::GetRowWithPrefetch
      sqlmin!CQScanTableScanNew::GetRow
        sqllang!CValOdsRow::SetDataX
        sqllang!CShilohTds::SendRowImpl
      sqlmin!CQScanTableScanNew::GetRow
    sqlmin!CQScanTopNew::GetRow
  sqlmin!CQueryScan::GetRow
...
sqllang!CXStmtQuery::ErsqExecuteQuery
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Output from wt</em></p>

<p>The output shows a lot of data, so in <em>Code Snippet 7</em> I have edited out all but the interesting parts. We see some routines that have to do with setting up and starting the query and retrieving result data:</p>

<ul>
<li><code>sqllang!CXStmtQuery::SetupQueryScanAndExpression</code>.</li>
<li><code>sqlmin!CQueryScan::StartupQuery</code>.</li>
<li><code>sqlmin!CQueryScan::GetRow</code>.</li>
</ul>

<p>That is interesting but what is more interesting is what we see after the <code>GetRow</code> calls: <code>sqllang!CShilohTds::SendRowImpl</code>. SQL Server calls <code>SendRowImpl</code> when it pushes data to the network packet which it then sends to the caller. We can confirm this is the case by using <em>WinDbg</em> and <em>Process Monitor</em>. So, run <em>Process Monitor</em> as admin and set an event filter which captures TCP Receive events for the <code>BxlServer.exe</code> process.</p>

<blockquote>
<p><strong>NOTE:</strong> If you are not sure how to set an event filter in <em>Process Monitor</em>, you can read about it in the <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a> post.</p>
</blockquote>

<p>Now, disable all breakpoints in <em>WinDbg</em> and execute the code in <em>Code Snippet 3</em>, (with <code>SELECT TOP(1) ...</code>) and watch the output from <em>Process Monitor</em>:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_procmon_pull_1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Process Monitor Output</em></p>

<p>What we see in <em>Figure 2</em> is similar to what we saw in <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a>, when we looked at the TCP packets SQL Server sends to the <code>BxlServer.exe</code> process. The outlined row in <em>Figure 2</em> is the packet with the result of the <code>SELECT</code>. I know that because I tried with multiple <code>TOP</code> values and saw which row changed length between the <code>TOP</code> values. In <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> the corresponding row had a length of 1358 as we did a <code>TOP(50)</code>. You may wonder what the three highlighted parts if the figure is and we come back to that in a little bit.</p>

<p>Now let us confirm that the outlined row is the row with the data:</p>

<ul>
<li>Re-enable (set) the breakpoint at <code>sqllang!SpExecuteExternalScript</code>.</li>
<li>Set a breakpoint at <code>sqllang!CShilohTds::SendRowImpl</code>.</li>
</ul>

<p>Execute the code and continue through until you break at <code>SendRowImpl</code>. If you look at the <em>Process Monitor</em> output, the last row you see is a row with a length of 405. That is the fourth row from the end as we see in <em>Figure 2</em>. Now continue the execution, and you see how <em>Process Monitor</em> outputs the three last rows, with a row with a length of 133 as the first one. We have &ldquo;thus&rdquo; proven that <code>SendRowImpl</code> sends the data row to the data packet which then SQL Server sends to the caller. Oh, the reason I use <code>TOP(1)</code> is that there is one call to <code>SendRowImpl</code> for each row, and I do not want to have to press F5 millions of times, or even 50 times.</p>

<h2 id="packets">Packets</h2>

<p>Now, what about the highlighted areas in <em>Figure 2</em>, what are those? First of all, if you look at the length of the packets it seems like each area have packets of the same length: in the yellow area, we see packets with a length of 37, 1245, 67 and 405, which we also see in the green and blue area. In <em>Figure 2</em> we also see that the highlighted areas originate from the same <code>BxlServer.exe</code> process (process id 6572), but different ports: 6799, 6800 and 6801. OK, let us see if we can figure out what this is all about, and to try to get a clearer picture we also want to filter on what <code>BxlServer.exe</code> sends to SQL Server. To see that, we add two new conditions to the existing <em>Process Monitor</em> filter:</p>

<ul>
<li>First condition - <em>Operation</em> (first drop-down) <em>is</em> (second dropdown): &ldquo;TCP Send&rdquo;.</li>
<li>Second condition - <em>Path</em> <em>contains</em>: the definition for the SQL Server port (1443). In my examples it is: <code>win10-dev:ms-sql-s</code>. We set this filter condition, so we only see the ODBC communication.</li>
</ul>

<p>Disable all breakpoints in <em>WinDbg</em> and execute the code and watch the <em>Process Monitor</em> output:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_procmon_pull_2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Process Monitor Output - II</em></p>

<p>The output we see in <em>Figure 3</em> is quite similar to what we see in <em>Figure 2</em>, apart from that we also see the packets the <code>BxlServer.exe</code> process sends. We see the same structure of three separate sections of packets, and we have packets of the same length in the three sections. Once again we see the data packet being in the last section, once again with a length of 133 (the outlined row). What are these packets and why do we have three sections? Let us see if we can figure out what the packets do with the help of <em>WireShark</em>:</p>

<ul>
<li>Start <em>WireShark</em> as admin.</li>
<li>On the opening screen double click the adapter for <em>Npcap</em> (most likely named &ldquo;Npcap Loopback Adapter&rdquo;).</li>
</ul>

<p>This starts capturing events immediately and to stop the capture you click <strong>Ctrl + E</strong>. What we want to do now is to create a <em>WireShark</em> display filter, so we only see network packets we are interested in. What we are interested in are packets for port 1433, and we set the filter in the text box just underneath the toolbox: <code>tcp.port==1433</code>.  Finally, we click on the right arrow to the right the filter box to apply the filter (outlined in red below):</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_filter.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>WireShark Display Filter</em></p>

<p>TThe assumption at this stage is that the <em>WireShark</em> capture is off. Start the capture (&ldquo;Ctrl+E&rdquo;) and then immediately execute the code. We stop the capture as soon as the code has finished executing:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_output_I.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>WireShark Output</em></p>

<p>In <em>Figure 5</em> we see part of the output from <em>WireShark</em>, and we may wonder why we see more packets than when we looked at the output from <em>Process Monitor</em> and why the packet sixes are not the same. There are a couple of answers to that:</p>

<ul>
<li><em>Process Monitor</em> only show packets that has a body (data part), but more messages are going over the wire (<code>ACK</code> etc.), and <em>WireShark</em> shows all of them.</li>
<li>The packet size question is related to the point above. <em>WireShark</em> shows the bytes going over the wire and that includes headers and so forth, whereas <em>Process Monitor</em> only shows the data size.</li>
<li>The <em>WireShark</em> display filter is set on port 1433 (anything going in and out of SQL Server), whereas in <em>Process Monitor</em> we filtered on traffic to and from the <code>BxlServer.exe</code> process. The section in <em>Figure 5</em> highlighted in blue is an example of this; this is the communication between <em>SQL Server Management Studio</em> and SQL Server.</li>
</ul>

<h4 id="yellow-section">Yellow Section</h4>

<p>Ok, so what does <em>WireShark</em> tell us? In <em>Figure 5</em> I have highlighted in yellow the part that corresponds to the highlighted yellow part in <em>Figure 3</em>. For this exercise, we can ignore the packets whose <em>Protocol</em> is TCP and concentrate on the ones which have TDS as a protocol. Just by looking at the info we see that the packets have to do with login and authentication: <code>TDS7 pre-login message</code> followed by <code>Response</code> followed by more pre-login messages, followed by <code>TLS Exchange</code> and a <code>Response</code>. I am not going into any details about these packets, but if you want all the &ldquo;gory&rdquo; details about TDS you find documentation here: <a href="https://msdn.microsoft.com/en-us/library/dd304523.aspx">[MS-TDS]: Tabular Data Stream Protocol</a>.</p>

<p>We now realise that the packets in the first section in <em>Figure 3</em> has to do with authentication. Seeing that we see packets with the same length in the green and blue sections in <em>Figure 3</em> we can safely assume those packets are also authentication packets, and a quick look at the packets in <em>WireShark</em> confirms that. What about the packets that are different?</p>

<h4 id="green-section">Green Section</h4>

<p>In the green section there are packets with lengths of 386, 138, 23 and 22, and in the blue section, we see packet lengths of 330 and 133. Well, we do know what the 133 packet is - that is the packet with the data SQL Server sends to the <code>BxlServer.exe</code> process. So, let us go back to the <em>WireShark</em> output and see what we can find out:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_output_II.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>WireShark Output II</em></p>

<p>We see in <em>Figure 6</em> the last packets of the green section packets in <em>Figure 3</em>. In <em>Figure 6</em> the first packet corresponds to the 405 packet in the green section in <em>Figure 3</em> which we know is a response package. To see what type of response packet it is, we click on the row in the &ldquo;Packet List&rdquo; pane in <em>WireShark</em> and the &ldquo;Packet Details&rdquo; pane shows the details about the packet:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_env_change.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>WireShark Packet Details</em></p>

<p>Aha, in <em>Figure 7</em> we see that this packet is an <code>EnvChange</code> packet (yellow highlight) which SQL Server use to notify the caller of an environment change (for example, database, language, and so on). In this case, it is a database context change from <code>master</code> (grey highlight) to <code>testParallel</code> (blue highlight). So this packet appears in all three section sin <em>Figure 3</em>. Now let us look at the 386, 138, 23, and 22 packets, which in <em>WireShark</em> have lengths of 856, 360, 130 and 128 and are highlighted in <em>Figure 6</em> with yellow, green, blue and purple respectively.</p>

<p><strong>WireShark Packet 856</strong>:</p>

<p>When we click on the packet in the &ldquo;Packet List&rdquo; pane, the &ldquo;Packet Details&rdquo; looks like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_prepare.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>WireShark Packet Details II</em></p>

<p>The packet details we see in <em>Figure 8</em> tells us that this is a &ldquo;Remote Procedure Call&rdquo; (which we also can see in <em>Figure 6</em>), and the call is to <code>sp_prepare</code> (highlighted in blue). The procedure <code>sp_prepare</code> is generally used for performance reasons and prepares a parameterized Transact-SQL statement and returns a statement handle for execution. The &ldquo;Packet Bytes&rdquo; pane shows us a hex dump the statement:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_packet_bytes.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>WireShark Packet Bytes</em></p>

<p><strong>WireShark Packet 360</strong>:</p>

<p>The 360 packet (green) is a response packet, and when we look at the packet we see:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_col_metadata.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>WireShark Packet Details and Bytes</em></p>

<p>We see in <em>Figure 10</em> that the response to <code>sp_prepare</code> is column metadata for the query.</p>

<p><strong>WireShark Packet 130 and 128</strong>:</p>

<p>Finally, the last two packets in the green section is a call to <code>sp_unprepare</code> (packet 130) and an &ldquo;End of message&rdquo; response (packet 128) to that.</p>

<h4 id="blue-section">Blue Section</h4>

<p>On to the blue section in <em>Figure 3</em>. We already said that there are only two packets that are different to the ones in the yellow and green sections:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_output_III.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>WireShark Output III</em></p>

<p>The packet highlighted in yellow in <em>Figure 11</em> is the packet with a length of 330 in <em>Figure 3</em>, and the 350 packet (highlighted in green) is the <em>Figure 3</em> 133 packet.</p>

<p><strong>WireShark Packet 744</strong>:</p>

<p>The 744 packet looks like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_sql_batch.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>WireShark Output III</em></p>

<p>In <em>Figure 12</em> we see how the packet is a SQL batch (yellow highlight), and it contains the actual query (green and blue highlights). So this is the query that SQL Server executes.</p>

<p><strong>WireShark Packet 350</strong>:</p>

<p>As we have mentioned a few times, the 350 packet contains the result of the query executed from the 744 packet, and the packet details and bytes look like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_III_wireshark_query_result.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>WireShark Output Query Result</em></p>

<p>With a cursory glance at <em>Figure 13</em> we may think there is no difference between what we see in <em>Figure 10</em> - which shows the response to the <code>sp_prepare</code> call - and what is in <em>Figure 13</em>. However, looking a bit closer we see that this packet contains the result of the query where we see highlighted in green and blue the two first column values, 2 and 6 respectively. The column values are not present in <em>Figure 10</em>.</p>

<h2 id="sections">Sections</h2>

<p>By now we sort of understand what goes on when we pull the data in the local context. However, why do we have these different sections, and why do we see multiple authentications? I have no answer why we see multiple authentications; maybe I can get an answer from someone &ldquo;in the know&rdquo;. As for the multiple sections; I am not 100% clear about that - but it seems to have something to do with what RevoScaleR function that accesses the data, remember from <a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">Context - II</a> how loading of the data happens not in the <code>RxSqlServerData</code> call, but in the call which uses the data. So why I say that the sections are related to this is that if we, instead of doing a <code>rxLinMod</code> call, did something like <code>ds &lt;-</code>rxImport(mydata)` then we only see the green and blue sections. Hopefully, I can get some more information about the sections as well which I can update this post with.</p>

<h2 id="summary">Summary</h2>

<p>So this post discusses the internal workings in SQL Server when we pull data in the local context. We talked about how SQL Server first sends the script data to the SQL Satellite through the usual <code>sqllang!CXStmtQuery::ErsqExecuteQuery</code> and <code>sqllang!CSatelliteCargoContext::SendChunkEndMessage</code>. After <code>SendChunkEndMessage</code> <code>ErsqExecuteQuery</code> is called again and at that stage the statement query executes.</p>

<p>The data transfer between the SqlSatellite (hosted by <code>BxlServer.exe</code>) and SQL Server happens through TDS packets, and we saw how authentication packages are sent multiple times, followed by <code>sp_prepare</code>. Finally, SQL Server executes the query and returns the result to the SqlSatellite.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 30]]></title>
    <link href="http://nielsberglund.com/2018/07/29/interesting-stuff---week-30/" rel="alternate" type="text/html"/>
    <updated>2018-07-29T19:11:58+02:00</updated>
    <id>http://nielsberglund.com/2018/07/29/interesting-stuff---week-30/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/chaos-engineering-autonous-vehicles">Properties of Chaos</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation about how and why chaos engineering is being applied to autonomous vehicle safety, and how to advance chaos engineering practices to explore beyond basic properties like system availability and extend into verifying system correctness.</li>
</ul>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="https://blog.pulumi.com/program-the-cloud-with-12-pulumi-pearls">Program the Cloud with 12 Pulumi Pearls</a>. In the <a href="/2018/06/24/interesting-stuff---week-25/">week 25</a> roundup I wrote about <a href="https://twitter.com/funcOfJoe">Joe Duffy</a> and his <a href="http://pulumi.com/">Pulumi</a>. This blog post by <a href="https://twitter.com/funcOfJoe">Joe</a> demonstrates some fun ways you can program the cloud using Pulumi. Very interesting!</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/sqlserverstorageengine/2018/07/23/pearl-abyss-massive-scale-using-azure-sql-database/">Pearl Abyss: Massive Scale using Azure SQL Database</a>. A blog post, showing off how you can achieve a massive scale of SQL Server by using Azure SQL Database. 7,500 cores, 46 instances 500,000 queries per second! Can I have two of those, please!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=ExEWJVjj-RA">Demo: Build a Streaming Application with KSQL</a>. A YouTube video, illustrating how to build a streaming application using KSQL.</li>
<li><a href="https://www.confluent.io/blog/ansible-playbooks-for-confluent-platform/">Ansible Playbooks for Confluent Platform</a>. This blog post covers how <a href="https://www.confluent.io/">Confluent</a> wants to make it super easy to set up <a href="https://www.confluent.io/product/confluent-platform/">Confluent Platform</a>. In this post, they introduce their first set of Ansible playbooks. It certainly seems easy!</li>
</ul>

<h2 id="data-science-ai">Data Science / AI</h2>

<ul>
<li><a href="http://blog.revolutionanalytics.com/2018/07/a-quick-tour-of-ai-services-in-azure.html">A quick tour of AI services in Azure</a>. A post by <a href="https://twitter.com/revodavid">David</a> at <a href="http://blog.revolutionanalytics.com">Revolution Analytics</a> talking about a video giving a quick overview of some of the services available in Azure to build AI-enabled applications. Watch the video if you are interested in AI and Azure.</li>
<li><a href="https://blogs.msdn.microsoft.com/mlserver/2018/07/26/dockerizing-r-and-python-web-services/">Dockerizing R and Python Web Services</a>.  A post looking into how to build a Docker image containing <strong>Machine Learning Server 9.3</strong>using Dockerfiles and how-to-perform Both R and Python operations.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am close to finish the third post in the <a href="/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series. I hope to have it out by the end of this upcoming week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 29]]></title>
    <link href="http://nielsberglund.com/2018/07/22/interesting-stuff---week-29/" rel="alternate" type="text/html"/>
    <updated>2018-07-22T16:06:58+02:00</updated>
    <id>http://nielsberglund.com/2018/07/22/interesting-stuff---week-29/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending. Unfortunately there was not much that caught my eye this week, so this is a short post.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2018/07/zuul-push-messaging">Scaling Push Messaging for Millions of Devices @Netflix - Susheel Aroskar at QCon NY</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about Zuul Push, which is a high performance asynchronous service based on Apache Netty, a non-blocking I/O (NIO)-based network application framework. The technology handles more than 5.5 million connected clients at peak.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-vs-enterprise-service-bus-esb-friends-enemies-or-frenemies/">Apache Kafka vs. Enterprise Service Bus (ESB)—Friends, Enemies, or Frenemies?</a>. A blog post showing why so many enterprises leverage the open source ecosystem of Apache Kafka for successful integration of different legacy and modern applications, and how this differs but also complements existing integration solutions like ESB or ETL tools.</li>
<li><a href="https://databricks.com/blog/2018/07/19/simplify-streaming-stock-data-analysis-using-databricks-delta.html">Simplify Streaming Stock Data Analysis Using Databricks Delta</a>. A post about how Databricks Delta helps solving many of the pain points of building a streaming system to analyze stock data in real-time.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.usejournal.com/python-vs-and-r-for-data-science-833b48ccc91d">Python vs (and) R for Data Science</a>. This post acts as a guide for those wishing to choose between Python and R Programming languages for Data Science.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>I am <del>busy</del> struggling with writing a couple of posts in the <a href="/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series, as well as the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. I do hope to have at least one out in a weeks time (or so).</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 28]]></title>
    <link href="http://nielsberglund.com/2018/07/15/interesting-stuff---week-28/" rel="alternate" type="text/html"/>
    <updated>2018-07-15T06:15:28+02:00</updated>
    <id>http://nielsberglund.com/2018/07/15/interesting-stuff---week-28/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/07/09/system-io-pipelines-high-performance-io-in-net/">System.IO.Pipelines: High performance IO in .NET</a>. A blog post which announces <code>System.IO.Pipelines</code> which is a new library that is designed to make it easier to do high performance IO in .NET. I wish it had been around when we wrote socket code, way back when.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/netflix-techblog/evolution-of-application-data-caching-from-ram-to-ssd-a33d6fa7a690">Evolution of Application Data Caching : From RAM to SSD</a>. A blog post about Netflix and how they move from purely RAM based caches to a hybrid between RAM and SSD. Very, very interesting!</li>
<li><a href="https://medium.com/netflix-techblog/auto-scaling-production-services-on-titus-1f3cd49f5cd7">Auto Scaling Production Services on Titus</a>. This blog post, also from Netflix, discusses auto-scaling on their container management system <a href="https://medium.com/netflix-techblog/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436">Titus</a>. It is interesting to read how the interaction happens between the AWS Auto Scaling engine and Titus.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/kafka-1-0-on-hdinsight-lights-up-real-time-analytics-scenarios/">Kafka 1.0 on HDInsight lights up real-time analytics scenarios</a>. This Microsoft blog post discusses the advantages that Kafka 1.0 on Azure HDInsight provides for data scientists and data analysts.</li>
<li><a href="/2018/07/10/install-confluent-platform-kafka-on-windows/">Install Confluent Platform (Kafka) on Windows</a>. This post from yours truly discusses we can install Kafka, in the guise of Confluent Platform, on Windows Subsystem for Linux (WSL). Useful if you, like me, is a Windows dude (or dudette) and you want to run Kafka locally on your development box.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="http://blog.revolutionanalytics.com/2018/07/r-351-update-now-available-.html">R 3.5.1 update now available</a>. This post by <a href="https://twitter.com/revodavid">David</a> talks about the new version of R: 3.5.1.</li>
<li><a href="https://blogs.msdn.microsoft.com/dotnet/2018/07/09/announcing-ml-net-0-3/">Announcing ML.NET 0.3</a>. At Build 2018 Microsoft released ML.NET 0.1, a cross-platform, open source machine learning framework for .NET developers, and I posted about it in the <a href="/2018/05/13/interesting-stuff---week-19/">week 19 roundup</a>. A month or two later they released ML.NET 0.2 which I covered in the roundup for <a href="/2018/06/10/interesting-stuff---week-23/">week 23</a>. It is now time for ML.NET 0.3 with quite a few new enhancements. What interests me is to see what &ldquo;cool&rdquo; new features application developers dreams up with this.</li>
<li><a href="https://www.infoq.com/presentations/r-framework-ai-apps">R for AI developers</a>. So, <a href="https://twitter.com/revodavid">David</a> is at it again. This time he did a presentation at QCon.ai where he makes a case for R as a platform for developing models for intelligent applications. The presentation is a must-see!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->

<!-- [findstr]: <> findstr /I <word_to_find> * -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Confluent Platform (Kafka) on Windows]]></title>
    <link href="http://nielsberglund.com/2018/07/10/install-confluent-platform-kafka-on-windows/" rel="alternate" type="text/html"/>
    <updated>2018-07-10T18:43:48+02:00</updated>
    <id>http://nielsberglund.com/2018/07/10/install-confluent-platform-kafka-on-windows/</id>
    <content type="html"><![CDATA[<p>You who follows my blog and have read my weekly roundups you know that I am quite (that is an understatement) interested in Apache Kafka and I am curious to find out what &ldquo;cool&rdquo; things one can do with it. For that, I want to be able to test &ldquo;stuff&rdquo; quickly. When I test and try out new things, I usually do it on my development box which contains everything I need: <strong>SQL Server</strong>, <strong>RabbitMQ</strong>, <strong>RStudio</strong>, <strong>Microsoft Machine Learning Server</strong>, <strong>Visual Studio</strong> and the list goes on.</p>

<p>So seeing that I have most of my &ldquo;tools of the trade&rdquo; on my machine I obviously also would like to have Kafka on the box. Herein lies a problem, I am a Windows dude and Kafka, and Windows do not gel. Yes, some people are running Kafka on Windows, but it is a chore. OK, so what to do? Sure, I could potentially run Kafka on a virtual machine, or in a Docker image, but it is not as transparent as I would like it to be (yeah, I am lazy).</p>

<p>Hmm, Microsoft did introduce the ability to run Linux binary executables (in ELF format) natively on Windows 10 in Windows 10 version 1607. The feature is called <strong>Windows Subsystem for Linux</strong> (WSL), and since I am now running version 1803, maybe I should try and install Kafka in <em>WSL</em>.</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>To install and run the <strong>Confluent Platform</strong>, (which contains Kafka), on <em>WSL</em> there are some pre-reqs:</p>

<ul>
<li><em>WSL</em> installed (fairly obvious).</li>
<li>Java 1.7 or later.</li>
</ul>

<h2 id="windows-subsystem-for-linux">Windows Subsystem for Linux</h2>

<p><em>WSL</em> is primarily aimed at developers, and it allows you to run Linux environments directly on Windows in a native format and without the overhead of a virtual machine. Let us retake a look at that statement: <em>run Linux environments directly on Windows in a native format</em>. Yes native format, <em>WSL</em> is not a UNIX-like environment like Cygwin, which wraps non-Windows functionality in Win32 system calls but it serves Linux programs as special, isolated minimal processes (<em>pico-processes</em>) attached to kernel-mode <em>pico-providers</em>. If you want to read all the &ldquo;gory&rdquo; details about <em>WSL</em>: <a href="https://blogs.msdn.microsoft.com/wsl/2016/04/22/windows-subsystem-for-linux-overview/">Windows Subsystem for Linux Overview</a> gives you an excellent introduction.</p>

<p>Installing <em>WSL</em> is very easy; you first enable <em>WSL</em> either via a Powershell command: <code>Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux</code> or by switching it on from &ldquo;Turn Windows features on or off&rdquo; via &ldquo;Control Panel | Programs and Features&rdquo;:</p>

<p><img src="/images/posts/inst_kafka_wsl_enable_wsl.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Enable WSL</em></p>

<p>You should restart the machine after enabling <em>WSL</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> So I have enabled <em>WSL</em> a few times now, and some of the times I have not had to restart.</p>
</blockquote>

<p>When <em>WSL</em> is enabled you need to download and install a distro from the Windows Store. When <em>WSL</em> first was introduced the only distro available was Ubuntu, since then quite a few have been added, and now the distro list looks like so:</p>

<ul>
<li>Ubuntu</li>
<li>OpenSUSE</li>
<li>SLES</li>
<li>Kali Linux</li>
<li>Debian GNU/Linux</li>
</ul>

<p>I started with <em>WSL</em> when Ubuntu was the only distro available, so I have &ldquo;stuck&rdquo; with it, but I do not think the distros are that different. To continue, you choose a distro and let it install. Finally, you start the command shell for the distro from the Windows &ldquo;Start&rdquo; menu:</p>

<p><img src="/images/posts/inst_kafka_wsl_start_distro.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Start the Distro</em></p>

<p>When you start up the distro for the first time the setup finishes, and you are prompted to enter a root password. Now is probably a good time to run <code>sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y</code> where <code>sudo apt-get update -y</code> updates the list of all current program packages in the repositories to determine which packages are candidates for upgrading. The command <code>sudo apt-get upgrade -y</code> upgrades all current program packages in the operating system.</p>

<blockquote>
<p><strong>NOTE:</strong> The above commands are for Ubuntu, so if you have another distro installed check the commands for that particular distro.</p>
</blockquote>

<h4 id="java">Java</h4>

<p>When your distro is up and running, you can now install Java. When reading the documentation about <a href="https://docs.confluent.io/current/installation/versions-interoperability.html#java">supported Java versions</a>, you see that <strong>Confluent Platform</strong> 4.1 is the last version with support for Java 1.7. The <strong>Confluent Platform</strong> version I use is the latest preview (version 5.x), so I install 1.8. Oh, and do not try with 1.9 -  it does not work.</p>

<p>The <a href="https://docs.confluent.io/current/installation/versions-interoperability.html#java">docs</a> mention the JDK, but I have found that the JRE works as well (since I am not writing any Java code) and I use the open source version of Java - OpenJDK. So to install you run the following from the bash shell:</p>

<pre><code class="language-bash">&gt; sudo apt install openjdk-8-jre-headless 
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install JRE</em></p>

<p>As <em>WSL</em> has no GUI, I choose to install the headless version of the JRE as we see in <em>Code Snippet 1</em>. Finally, to check that it installed correctly I do <code>&gt; java -version</code> and the result is like so:</p>

<pre><code class="language-bash">openjdk version &quot;1.8.0_171&quot;
OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.18.04.1-b11)
OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode)
</code></pre>

<p><strong>Code Snippet 2:</strong> Java Version Output*.</p>

<p>In <em>Code Snippet 2</em> everything looks OK, so we can now go ahead with the main attraction.</p>

<h2 id="confluent-platform">Confluent Platform</h2>

<p>By now you may ask yourself what is this thing <strong>Confluent Platform</strong>? Well, <a href="https://confluent.io">Confluent</a> is a company founded by the guys (and girls) that originally built Kafka back at LinkedIn. The company is now focusing on building a streaming platform to help other companies get easy access to enterprise data as real-time streams.</p>

<p>The <strong>Confluent Platform</strong> improves Apache Kafka by expanding its integration capabilities, adding tools to optimise and manage Kafka clusters, and methods to ensure the streams are secure. <strong>Confluent Platform</strong> makes Kafka easier to build and easier to operate. The <strong>Confluent Platform</strong> comes in two flavours:</p>

<ul>
<li><a href="https://www.confluent.io/product/confluent-open-source/">Confluent Open Source</a> is freely downloadable.</li>
<li><a href="https://www.confluent.io/product/confluent-enterprise/">Confluent Enterprise</a> is available on a subscription basis.</li>
</ul>

<p>Back in April Confluent started releasing preview versions of the <strong>Confluent Platform</strong> with the latest and the greatest and that is what I am using. At the time I write this the June preview has just been released, and that is what I am installing here.</p>

<h4 id="installation">Installation</h4>

<p>Before we can install, we need to download the install media which you do from <a href="https://www.confluent.io/preview-release">here</a>. When clicking the &ldquo;Download &hellip;&rdquo; button, a form &ldquo;pops up&rdquo; where you choose your download format and enter your details. I usually choose <code>tar.gz</code>, and that is what I base the following instructions on. Download the file to your PC and then in the bash shell:</p>

<ul>
<li>Create a directory where to extract the files to.</li>
<li><code>cd</code> to the download directory:</li>
</ul>

<p><img src="/images/posts/inst_kafka_wsl_mkdir.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Make Kafka Directory</em></p>

<p>In <em>Figure 3</em> we see how I create the <code>/opt/kafka</code> directory, and how I <code>cd</code> to the Windows directory where my downloaded files are. One of the cool things with <em>WSL</em> is that the local Windows drives gets automatically mounted under the <code>/mnt</code> folder. I can now extract the files:</p>

<blockquote>
<p><strong>NOTE:</strong> The only reason I chose to create the <code>kafka</code> directory under <code>/opt</code> is that traditionally <code>/opt</code> is for third-party applications.</p>
</blockquote>

<p><img src="/images/posts/inst_kafka_wsl_untar.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Extract Files</em></p>

<p>So I <code>tar</code> the files, and we see in <em>Figure 4</em> how the files are extracted. To extract the files takes a couple of minutes and when done we can drill down into the extracted directories and files:</p>

<p><img src="/images/posts/inst_kafka_wsl_kafka_dirs_files.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Directory and File Structure</em></p>

<p>In <em>Figure 5</em> we see how directories and files ended up under a <code>confluent-version...</code> directory (outlined in white) and when we <code>ls</code> into that directory we see sub-directories (also outlined in white), and amongst them a <code>bin</code> directory.</p>

<p>When we drill down into the <code>bin</code> directory and list the content we see a file named <code>confluent</code>. This is an executable file, and we use this file to start and stop all the Confluent components. The <code>bin</code> directory also contains executable files to start and stop individual components, such as <code>kafka-server-start</code>, <code>kafka-server-stop</code> and <code>zookeeper-server-start</code>, and so forth.</p>

<p>Right, enough of this - let us see if we can get the show on the road and spin up all components:</p>

<p><img src="/images/posts/inst_kafka_wsl_start_kafka.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Start Confluent</em></p>

<p>To start all the Confluent components, we use the command <code>sudo ./confluent start (from the</code>bin` directory) and in <em>Figure 6</em> we see how the various components startup, awesome!</p>

<h4 id="control-center">Control Center</h4>

<p>Part of the <strong>Confluent Platform</strong> installation (Enterprise version) is the <em>Control Center</em>. The <em>Control Center</em> (I copied the text from the <a href="https://www.confluent.io/confluent-control-center/">Control Center</a> site) &ldquo;gives the administrator monitoring and management capabilities, delivering automated, curated dashboards so that Kafka experts can easily understand what is happening without tweaking dashboards&rdquo;. So let us see if we can connect with the <em>Control Center</em>. If we connect from the same machine as we installed <strong>Confluent Platform</strong> on, the address is <code>http://localhost:9021</code>:</p>

<p><img src="/images/posts/inst_kafka_wsl_control_center.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Confluent Control Center</em></p>

<p>Cool, <em>Control Center</em> seems to be up and running, let us now use it to create a <em>Topic</em> so we can do a final test.</p>

<h4 id="topics">Topics</h4>

<p>When you send messages to a Kafka broker, you typically send it to a &ldquo;Topic&rdquo;, which is like a collection point in the broker for &ldquo;like-minded&rdquo; messages. If you are a database dude like me, you can see it as a table in a database where you keep records of the same type.</p>

<p>Typically you create multiple topics in you Kafka cluster to cater for multiple message types, and <em>Control Center</em> can help you with that. In <em>Figure 7</em> we see at the bottom left corner, outlined in red, &ldquo;Topics&rdquo;. Click on that, and you see existing default topics. Click on, in the far right corner, the &ldquo;Create topic&rdquo; button and you see something like so:</p>

<p><img src="/images/posts/inst_kafka_wsl_create_topic.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Topics</em></p>

<p>In the topic name box enter &ldquo;testing&rdquo; and then click &ldquo;Create with defaults&rdquo; and we are back seeing the existing topics as well as the newly created &ldquo;testing&rdquo; topic:</p>

<p><img src="/images/posts/inst_kafka_wsl_topic_created.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>New Topic</em></p>

<p>When we have a topic, we can now see whether we can publish and consume messages.</p>

<h2 id="test-send-receive">Test Send &amp; Receive</h2>

<p><strong>Confluent Platform</strong> is now up and running, and you can now start doing &ldquo;cool&rdquo; stuff. However, to make sure everything works let us use the built-in command line clients to send and receive some test messages.</p>

<p>What we do is that in the open bash shell we <code>cd</code> to the <code>/opt/kafka/confluent-xxx/bin/</code> directory. We use the command line producer <code>kafka-console-producer</code> to send messages:</p>

<pre><code class="language-bash">sudo ./kafka-console-producer --broker-list localhost:9092 --topic testing
&gt;Hello World!
&gt;Life Is Awesome!
&gt;We Have Installed Kafka on Windows!
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Publishing Messages</em></p>

<p>We see in <em>Code Snippet 3</em> how we target the local broker on port 9092, and the topic we send to is the &ldquo;testing&rdquo; topic we created above. After hitting enter, we create one message after each other (hit enter in between).</p>

<p>To consume messages we open a second bash shell and <code>cd</code> into the <code>/bin</code> directory as before, and to receive messages we use the <code>kafka-console-consumer</code> command line client:</p>

<pre><code class="language-bash">sudo ./kafka-console-consumer --bootstrap-server localhost:9092 --topic testing --from-beginning
Hello World!
Life Is Awesome!
We Have Installed Kafka on Windows!
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Consume Messages</em></p>

<p>When running the consumer we see in <em>Code Snippet 4</em> how we receive the messages we just sent. If we were to go back to the publisher and create some more messages we immediately see them in the consumer bash shell. It works! So now we can start creating streaming applications using proper <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">clients</a>. If you write .NET applications, I suggest you look at the <a href="https://github.com/confluentinc/confluent-kafka-dotnet">Confluent client</a> which is very feature rich.</p>

<p>When we are done with the <strong>Confluent Platform</strong>, we stop it from the <code>/bin</code> directory:</p>

<p><img src="/images/posts/inst_kafka_wsl_stop_kafka.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Stopping Kafka</em></p>

<p>We stop Kafka by calling <code>sudo ./confluent stop</code> and then as <em>Figure 10</em> shows, all components shut down in an orderly fashion.</p>

<p>We have installed <strong>Confluent Platform</strong> on <em>WSL</em>, started it, published and consumed some messages and stopped it. All is good! Or is it?</p>

<h2 id="issue">ISSUE</h2>

<p>So what happens when you try to start the platform again:</p>

<p><img src="/images/posts/inst_kafka_wsl_start_error.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Error at Start Up</em></p>

<p>That is not good! We get an error when we try to start the platform after a shutdown. What we see here is a &ldquo;half&rdquo; known issue which is most prevalent on Windows machines, and it has to do with Kafka trying to clean up old log files. If you drill down in the Kafka log files you see an error looking something like this: <code>FATAL Shutdown broker because all log dirs in &lt;path_to_logs&gt; have failed (kafka.log.LogManager)</code>.</p>

<p>At the moment I do not have a solution for it other than that before each startup run something like so: <code>sudo rm -fr /tmp/confl*</code> which removes all Kafka related log directories. This is obviously not a solution in a production environment or a &ldquo;proper&rdquo; test/dev environment but for us just wanting to do some &ldquo;quick and dirty&rdquo; testing on <em>WSL</em> it is sufficient.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed a little bit what <em>WSL</em> is and how we can install <strong>Confluent Platform</strong> on <em>WSL</em>. We looked at we can test the installation by creating a topic and then publish and consume messages using the command line publish and consume clients.</p>

<p>Having <strong>Confluent Platform</strong> installed we can now use a <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">client of choice</a> to start doing &ldquo;cool&rdquo; stuff. Keep an eye on my blog for future <strong>Confluent Platform</strong> and Kafka posts!</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/series/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/series/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/series/spees_and_sql_compute_context) -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 27]]></title>
    <link href="http://nielsberglund.com/2018/07/08/interesting-stuff---week-27/" rel="alternate" type="text/html"/>
    <updated>2018-07-08T12:11:28+02:00</updated>
    <id>http://nielsberglund.com/2018/07/08/interesting-stuff---week-27/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/ip-filtering-for-event-hubs-and-service-bus/">IP filtering for Event Hubs and Service Bus</a>. A frequently asked for feature in Azure Event Hubs is the ability to restrict access to the Event Hubs to certain well-known sites, alternatively rejecting traffic from specific IP addresses. The Azure team has now announced a public preview of IP filtering.</li>
<li><a href="https://databricks.com/blog/2018/07/02/build-a-mobile-gaming-events-data-pipeline-with-databricks-delta.html">Build a Mobile Gaming Events Data Pipeline with Databricks Delta</a>. This blog post shows how to build a data pipeline in AWS using the <a href="https://databricks.com/product/unified-analytics-platform">Databricks Unified Analytics Platform</a>. Even though they in the blog post use AWS it should be possible to do the same on Azure since Databricks is now available there as well.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="http://mattwarren.org/2018/07/05/.NET-JIT-and-CLR-Joined-at-the-Hip/">.NET JIT and CLR - Joined at the Hip</a>. This is another excellent blog post by <a href="https://twitter.com/matthewwarren">Matthew</a>, looking at how the .NET JIT compiler works together with CLR.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2018/07/boner-events-first-microservices">QCon NY: Jonas Bonér on Designing Events-First Microservices</a>. An <a href="https://www.infoq.com/">InfoQ</a> article summarising a QCon talk by <a href="http://jonasboner.com/">Jonas Bonér</a> of <a href="https://akka.io/">Akka</a> fame. The main point Jonas makes is that events-first domain-driven design (DDD) and event streaming are critical in a microservices based architecture. Oh and by th way, if you are interested in event-driven systems and microservices go and download Jonas mini-book <a href="https://www.lightbend.com/blog/reactive-microsystems-the-evolution-of-microservices-at-scale-free-oreilly-report-by-jonas-boner">Reactive Microsystems - The Evolution Of Microservices At Scale</a>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/june-preview-release-confluent-plaform/">June Preview Release: Packing Confluent Platform with the Features You Requested!</a>. A blog post by the Confluent team announcing the latest preview release of Confluent Platform. This release packs quite a few new features, and I am especially interested in the KSQL support for nested data as well as the ability to join two streams together.</li>
<li><a href="https://www.infoq.com/news/2018/07/event-sourcing-kafka-streams">Experiences from Building an Event-Sourced System with Kafka Streams</a>. An article from <a href="https://www.infoq.com/">InfoQ</a> about how engineers at <a href="https://www.wix.com/">Wix</a> built an event sourced system using <a href="https://kafka.apache.org/documentation/streams/">Kafka Streams</a>.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0">A Feature Selection Tool for Machine Learning in Python</a>. This blog post is about a Python tool that helps with feature selection in a dataset.</li>
<li><a href="https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420">A Complete Machine Learning Project Walk-Through in Python: Part One</a>. First post in a series walking through a complete Python machine learning solution.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<ul>
<li><a href="/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/">sp_execute_external_script and SQL Compute Context - II</a>. I finally published part 2 of the <a href="/spees_and_sql_compute_context">sp_execute_external_script and SQL Server Compute Context</a> series. In the post, I tried to figure out why the performance is so much better when executing in a <strong>SQL Server Compute Context</strong> in <strong>SQL Server ML Services</strong> compared to executing in the local context (it is SQL Server after all). Even though I &ldquo;sort of&rdquo; figured it out, a few questions arose and hopefully I can answer those questions in a future post.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> [SQL Server R Services](/sql_server_2k16_r_services) -->

<!-- [series2]: <> [Install R Packages in SQL Server ML Services](/sql_server_ml_services_install_packages) -->

<!-- [series3]: <> [sp_execute_external_script and SQL Server Compute Context](/spees_and_sql_compute_context) -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[sp_execute_external_script and SQL Compute Context - II]]></title>
    <link href="http://nielsberglund.com/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/" rel="alternate" type="text/html"/>
    <updated>2018-07-07T10:54:21+02:00</updated>
    <id>http://nielsberglund.com/2018/07/07/sp_execute_external_script-and-sql-compute-context---ii/</id>
    <content type="html"><![CDATA[<p>I wrote the post <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">sp_execute_external_script and SQL Compute Context - I</a> about how the <strong>SQL Server Compute Context</strong> (SQLCC) works with <code>sp_execute_external_script</code> (SPEES), as I wanted to correct some mistakes I did in the <a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services - sp_execute_external_script - III</a> post. I initially thought one post would be enough, but quite soon I realised I was too optimistic, and at least one more post would be needed, if not more. So this is the first followup post about SPEES and SQLCC.</p>

<p>To see other posts (including this) in the series, go to <a href="/spees_and_sql_compute_context"><strong>sp_execute_external_script and SQL Server Compute Context</strong></a>.</p>

<p>One of the reasons for me realising that one post is not enough is that while I wrote and executed code for the first <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">post</a>, I noticed some fairly significant performance differences using SQLCC compared to not using SQLCC (SQLCC performed better :)). So that is part of what we look at in this post.</p>

<p></p>

<h2 id="recap">Recap</h2>

<p>In quite a few posts about <strong>SQL Server Machine Learning Services</strong> we have discussed how, as part of the functionality in RevoScaleR, you can define where a workload executes. By default, it executes on your local machine, but you can also set it to execute in the context of somewhere else: Hadoop, Spark and also SQL Server. So, in essence, you can run some code on your development machine and have it execute in the environments mentioned above.</p>

<p>In the <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> post we saw that even when we executed from inside SQL Server, the compute context was the local context: <code>RxLocalSeq</code>. If we want to use the SQLCC we used <code>RxInSqlServer</code> and <code>rxSetComputeContext</code>:</p>

<pre><code class="language-r"># set up the connection string
sqlServerConnString &lt;- &quot;Driver=SQL Server;
                        server=.; # localhost
                        database=testParallel;
                        uid=some_uid;pwd=some_pwd&quot;

# set up the context
sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                        numTasks = 1)
# set the compute context to be the sql context
rxSetComputeContext(sqlCtx)    
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Set up SQL Server Compute Context</em></p>

<p>To setup the context we see in <em>Code Snippet 1</em> how we use a connection string pointing to the SQL Server where we want to execute the code. In this case, it is the instance we are on.</p>

<blockquote>
<p><strong>NOTE:</strong> The connection string is for where we want to execute, not necessarily where the data we use resides.</p>
</blockquote>

<p>We also see in <em>Figure 1</em> how <code>RxInSqlServer</code> has the <code>numTasks</code> parameter for you to set the number of tasks (processes) to run for each computation. The parameter defines the maximum number of tasks SQL Server can use. SQL Server can, however, decide to start fewer processes. Finally in <em>Figure 1</em> we call <code>rxSetComputeContext</code> which ensures that any code with functions that support SQLCC, executes under the compute context.</p>

<p>In the <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> post, we saw how when we execute inside of SQL Server via SPEES we by default run in the local context and only by setting the context as in <em>Code Snippet 1</em> we can execute in SQLCC.</p>

<p>An interesting observation when we set the <code>numTasks</code> parameter to a value greater than 1 is that when we run the code, we run it hosted in an <code>mpiexec.exe</code> process:</p>

<p><img src="/images/posts/sql_ml_services_comp_mpi.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Parallel Execution in Compute Context</em></p>

<p>In <em>Figure 1</em> we now see not only the &ldquo;usual&rdquo; RTerm and <code>BxlServer.exe</code> processes but also a new hosting process, outlined in red, <code>mpiexec.exe</code>. Underneath the <code>mpiexec.exe</code> process we see the <code>smpd.exe</code> process (outlined in green) and then four RTerm processes with <code>BxlServer.exe</code> processes which handle the workload. So, <code>mpiexec.exe</code> and <code>smpd.exe</code> are parts of <a href="https://msdn.microsoft.com/en-us/library/bb524831%28v=vs.85%29.aspx?f=255&amp;MSPPError=-2147217396"><strong>Microsoft MPI</strong></a> which is an implementation of MPI which is a communication protocol for programming parallel computers.</p>

<p>All this is somewhat interesting, but the most interesting thing (at least for me) is the performance difference we saw when executing the same code in the local context compared to the SQLCC. When executing with <code>numTasks</code> set to 1 (as it would be under the local context) code that ran in ~40 seconds in the local context took ~30 seconds to run in SQLCC! Once again, we did not run it with multiple tasks in SQLCC, so just be running in SQLCC we received a performance gain of about 30%!</p>

<blockquote>
<p><strong>NOTE:</strong> The performance gain is of course not always 30%, it depends on data volumes.</p>
</blockquote>

<p>So, as I said at the beginning of this post - let us try and figure out why the performance is better using SQLCC.</p>

<h2 id="housekeeping">Housekeeping</h2>

<p>Before we &ldquo;dive&rdquo; into today&rsquo;s topics let us look at the code and the tools we use today. This section is here for those who want to follow along in what we are doing in the post.</p>

<h4 id="helper-tools">Helper Tools</h4>

<p>To help us figure out the things we want, we use <em>Process Monitor</em> to filter TCP traffic.</p>

<h4 id="code">Code</h4>

<p>This is the database objects we use in this post:</p>

<pre><code class="language-sql">USE master;
GO

SET NOCOUNT ON;
GO

DROP DATABASE IF EXISTS TestParallel;
GO

CREATE DATABASE TestParallel;
GO

USE TestParallel;
GO

DROP TABLE IF EXISTS dbo.tb_Rand_50M
GO
CREATE TABLE dbo.tb_Rand_50M
(
  RowID bigint identity PRIMARY KEY, 
  y int NOT NULL, rand1 int NOT NULL, 
  rand2 int NOT NULL, rand3 int NOT NULL, 
  rand4 int NOT NULL, rand5 int NOT NULL,
);
GO

INSERT INTO dbo.tb_Rand_50M(y, rand1, rand2, rand3, rand4, rand5)
SELECT TOP(50000000) CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 25 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 14 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 50 AS INT)
  , CAST(ABS(CHECKSUM(NEWID())) % 100 AS INT)
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO

</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Setup of Database, Table and Data</em></p>

<p>We use more or less the same database and database object as in the <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> post:</p>

<ul>
<li>A database: <code>TestParallel</code>.</li>
<li>A table: <code>dbo.tb_Rand_50M</code>. This table contains the data we want to analyse.</li>
</ul>

<p>In addition to creating the database and the table <em>Code Snippet 2</em> also loads 50 million records into the <code>dbo.tb_Rand_50M</code>. Be aware that when you run the code in <em>Code Snippet 2</em> it may take some time to finish due to the loading of the data. Yes, I know - the data is entirely useless, but it is a lot of it, and it helps to illustrate what we want to do.</p>

<p>The code we use is almost like what we used in <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a>:</p>

<pre><code class="language-sql">DECLARE @isCtx bit = 0;
DECLARE @numTasks int = 1;
DECLARE @start datetime2 = SYSUTCDATETIME();
EXEC sp_execute_external_script
      @language = N'R'
    , @script = N'
      # set up the connection string
      sqlServerConnString &lt;- &quot;Driver=SQL Server;server=.;
                              database=testParallel;
                              uid=&lt;username&gt;;pwd=&lt;userpwd&gt;&quot;
      
      if(useContext == 1) {
        sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                                numTasks = tasks)
        # set the compute context to be the sql context
        rxSetComputeContext(sqlCtx)
      }

      mydata &lt;- RxSqlServerData(sqlQuery = &quot;SELECT y, rand1, rand2, 
                                            rand3, rand4, rand5 
                                            FROM dbo.tb_Rand_50M&quot;,
                                connectionString = sqlServerConnString);
                        
      myModel &lt;- rxLinMod(y ~ rand1 + rand2 + rand3 + rand4 + rand5, 
                      data=mydata)

      OutputDataSet &lt;- data.frame(nRows=myModel$nValidObs);'
    , @params = N'@tasks int, @useContext bit'
    , @tasks = @numTasks
    , @useContext = @isCtx
WITH RESULT SETS ((NumberRows int NOT NULL));
SELECT DATEDIFF(ms, @start, SYSUTCDATETIME())
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Test Code</em></p>

<p>As we see in <em>Code Snippet 3</em> we parameterize the <code>sp_execute_external_script</code> call, and we have parameters for whether to use the SQLCC and also how many tasks to run when executing in the context. The default is to execute in the local context, and when executing in SQLCC <code>numTasks</code> is 1.</p>

<h2 id="performance-differences">Performance Differences</h2>

<p>To start with, let us repeat - more or less - what we did in <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a> and compare execution times when running in the local context (<code>@isCtx = 0</code>) and when in SQLCC  (<code>@isCtx = 1</code>). In both cases, we execute with the default number of tasks (<code>numTasks = 1</code>).</p>

<blockquote>
<p><strong>NOTE:</strong> Do a couple of executions in the local context as well as in the SQLCC to ensure you get representative numbers for both.</p>
</blockquote>

<p>When I run the code on my SQL Server instance I get the following results:</p>

<ul>
<li>Local context: ~40 seconds</li>
<li>SQLCC: ~24 seconds</li>
</ul>

<p>So, the same workload shows an approximately 40% performance improvement when running in the SQLCC compared to the local context and this is in line with what we saw in <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a>. Why is this, we do the same things:</p>

<ul>
<li>We load data</li>
<li>We apply the <code>rxLinMod</code> function.</li>
<li>We run with the same number of tasks (single threaded).</li>
</ul>

<p>A question I have now is at what stage in the script, the script receives the 50 million rows? Comment out in the code, (<em>Code Snippet 3</em>), the <code>myModel</code> and <code>OutputDataSet</code> lines of code. When you now execute in the local context, you see the execution time is ~ 1 second. When you do the same in the SQLCC the time is about the same. It seems like the actual loading of the data happens not in the <code>RxSqlServerData</code> call, but in the call - in this case - to <code>rxLinMod</code>. Hmm, I wonder what happens if we instead of pulling the data, pushed the data to the <code>rxLinMod</code> call by using <code>@input_data_1</code>:</p>

<pre><code class="language-sql">DECLARE @isCtx bit = 0;
DECLARE @numTasks int = 1;
DECLARE @start datetime2 = SYSUTCDATETIME();
EXEC sp_execute_external_script
      @language = N'R'
    , @script = N'
      # set up the connection string
      sqlServerConnString &lt;- &quot;Driver=SQL Server;server=.;
                              database=testParallel;
                              uid=&lt;username&gt;;pwd=&lt;userpwd&gt;&quot;
      
      if(useContext == 1) {
        sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                                numTasks = tasks)
        # set the compute context to be the sql context
        rxSetComputeContext(sqlCtx)
      }
                      
      myModel &lt;- rxLinMod(y ~ rand1 + rand2 + rand3 + rand4 + rand5, 
                         data=InputDataSet)

      OutputDataSet &lt;- data.frame(nRows=myModel$nValidObs);'
    , @input_data_1 = N'SELECT y, rand1, rand2, rand3, rand4, rand5 
                        FROM dbo.tb_Rand_50M'
    , @params = N'@tasks int, @useContext bit'
    , @tasks = @numTasks
    , @useContext = @isCtx    
WITH RESULT SETS ((NumberRows int NOT NULL));
SELECT DATEDIFF(ms, @start, SYSUTCDATETIME())
GO
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Pushing the Data</em></p>

<p>In <em>Code Snippet 4</em> we see how we push the data through the <code>@input_data_1</code> straight to the <code>rxLinMod</code> call via <code>InputDataSet</code>. The code here does not look any different than from most of the other code used in many of my blog posts. When I execute it in the local context (<code>@isCtx bit = 0)</code> however:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_push_error.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Error Pushing Data</em></p>

<p>Oh, it looks like we try to push too much data as we see, highlighted in <em>Figure 2</em>, a memory issue. Ok, but this is what the SQLCC is all about - efficiently handling large volumes of data, so let us execute the same code but in the SQLCC (<code>@isCtx bit = 1</code>):</p>

<p><img src="/images/posts/sql_ml_services_compII_input_data_ctx_error.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Push and SQLCC Error</em></p>

<p>Ouch, it seems that to use SQLCC we need to pull data through <code>RxSqlServerData</code>. Never mind, I still want to push large volumes of data, so I change <code>@input_data_1</code> to do a <code>SELECT TOP(30000000) ...</code> (30 million) from the table instead. When I push my 30 million rows in the local context the execution time is around  17 seconds. What are the timings if we execute the code in <em>Code Snippet 3</em> with a <code>TOP (30000000)</code> both in the local context as well as SQLCC and compare execution times:</p>

<ul>
<li>Local context push (<em>Code Snippet 4</em> and <code>@isCtx = 0</code>): ~ 17 seconds.</li>
<li>Local context pull (<em>Code Snippet 3</em> and <code>@isCtx = 0</code>): ~ 23 seconds.</li>
<li>SQLCC pull (<em>Code Snippet 3</em> and <code>@isCtx = 1</code>): ~ 15 seconds.</li>
</ul>

<p>That was interesting, the timings between pushing the data in the local context are almost the same as pulling the data in SQLCC, and the push in the local context is much faster than the pull in the same context. What gives?</p>

<p>All we have done so far points to that the difference in performance comes from loading the data, so the question is what the difference is when loading it from the local context compared to the SQLCC, and is SQLCC always faster. Let us start with the last question first; is SQLCC always faster?</p>

<p>To test this change the <code>TOP</code> clause to <code>TOP(50)</code> and execute <em>Code Snippet 4</em> (pushing the data) and <em>Code Snippet 3</em> pulling the data both in the local context as well as SQLCC and take note of the timings:</p>

<ul>
<li>Local context push (<em>Code Snippet 4</em> and <code>@isCtx = 0</code>): ~ 200 ms.</li>
<li>Local context pull (<em>Code Snippet 3</em> and <code>@isCtx = 0</code>): ~ 260 ms.</li>
<li>SQLCC pull (<em>Code Snippet 3</em> and <code>@isCtx = 1</code>): ~ 1.6 seconds.</li>
</ul>

<p>That was quite a difference and now, all of a sudden, SQLCC is slowest! Why is that? Let us use <em>Process Monitor</em> to try to figure out why this is the case. However, before we do that let us recap a little bit about the internal workings when we execute <em>SPEES</em>.</p>

<h4 id="internals">Internals</h4>

<ul>
<li>The host for an external engine is <code>BxlServer.exe</code>.</li>
<li>When we execute <em>SPEES</em> the SqlSatellite (loaded by the BxlServer) connects to SQL Server over a TCP connection.</li>
<li>Data is sent over the TCP connection from and to SQL Server.</li>
<li>The data sent among other things authentication data, script data (the actual external script) as well as the dataset.</li>
</ul>

<p>The figure below illustrates connections and so forth in a &ldquo;simple&rdquo; case where we push data to the SqlSatellite in the local context:</p>

<p><img src="/images/posts/sql_r_services_ext_scriptIII_single_process.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Process Flow</em></p>

<p>In <em>Figure 4</em> we see what happens when we execute <code>sp_execute_external_script</code> and the numbers in the figure stands for:</p>

<ol>
<li>We call <code>sp_execute_external_script</code> and SQL Server calls into the launchpad service.</li>
<li>The launchpad service creates RTerm processes which in turn creates BxlServer processes. One process becomes the executing process.</li>
<li>A TCP connection from the SqlSatellite in the executing process gets established.</li>
<li>SQL server sends input data to the SqlSatellite.</li>
<li>The <code>BxlServer.exe</code> does the processing.</li>
<li>SQL Server receives data back from the SqlSatellite.</li>
</ol>

<p>The <a href="/sql_server_2k16_r_services">SQL Server R Services</a> series covered in &ldquo;excruciating&rdquo; details what data SQL Server sends to the BxlServer. If you want to read up on it I suggest <strong>Internals</strong> <a href="/2017/08/29/microsoft-sql-server-r-services---internals-x/">X</a>, <a href="/2017/10/20/microsoft-sql-server-r-services---internals-xi/">XI</a>, <a href="/2017/10/31/microsoft-sql-server-r-services---internals-xii/">XII</a>, <a href="/2017/11/25/microsoft-sql-server-r-services---internals-xiv/">XIV</a> and <a href="/2017/12/02/microsoft-sql-server-r-services---internals-xv/">XV</a>.</p>

<h4 id="investigation-using-performance-monitor">Investigation using Performance Monitor</h4>

<p>To see what happens when we execute our three scenarios (local push, local pull, SQLCC pull) we set up some <em>Process Monitor</em> event filters to capture TCP traffic from SQL Server to the SqlSatellite, where <code>BxlServer.exe</code> is &ldquo;hosting&rdquo; the SqlSatellite. The filters we set up are for &ldquo;Process Name&rdquo; and &ldquo;Operation&rdquo;. We want the process to be <code>BxlServer.exe</code> and the operation &ldquo;TCP Receive&rdquo;.</p>

<p>So, run <em>Process Monitor</em> as admin. To set the filter; under the <em>Filter</em> menu click the Filter menu item, and you see the &ldquo;Process Monitor Filter&rdquo; dialog. To create the filter we enter the conditions we want to match:</p>

<ul>
<li>The <em>Process Name</em> (from the first drop down) should be  <em>is</em> (from the second drop-down): <code>bxlserver.exe</code>.</li>
<li><em>Operation</em> (first drop-down) <em>is</em> (second dropdown): &ldquo;TCP Receive&rdquo;</li>
</ul>

<p>You add and include the conditions included and added, and at this stage, the filter dialog looks something like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_procmon_filter.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Filters BxlServer</em></p>

<p>What the filter says is that any &ldquo;TCP Receive&rdquo; events for <code>bxlserver.exe</code> should be monitored and displayed. When you have clicked &ldquo;OK&rdquo; out of the dialog box, we are ready to test this by executing the code for local context push (<em>Code Snippet 4</em>), local context pull (<em>Code Snippet 3</em> and <code>@isCtx = 0</code>) and SQLCC pull (<em>Code Snippet 3</em> and <code>@isCtx = 1</code>). When executing we look at the <em>Process Monitor</em> output, and the output for the local push is like so:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_procmon_push2.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>TCP Receive Local Context Push</em></p>

<p>We see in <em>Figure 6</em> that the output looks quite &ldquo;tidy&rdquo; and by looking at the <code>Path</code> column see a connection between SQL Server and the SqlSatellite on port 13273 (<code>win10-dev:13273</code>). Furthermore, we see:</p>

<ul>
<li>There is one <code>BxlServer.exe</code> process with a process id of 17260.</li>
<li>The data the BxlServer receives are what we covered in the <a href="/sql_server_2k16_r_services">SQL Server R Services</a> series.</li>
<li>The 50 rows we pushed to the BxlServer is the outlined (in blue) row with a length of 1392.</li>
</ul>

<p>Ok, so onto the local context pull:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_procmon_pull_loc_ctx2.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>TCP Receive Local Context Pull</em></p>

<p>Looking at <em>Figure 7</em> we see that there is quite a difference between when we push the data to the SqlSatellite. First, we see (highlighted in red) the usual connection between SQL Server and the SqlSatellite and how SQL Server sends data (authentication and script) to the SqlSatellite. Then, however, we see data going from SQL Server from a &ldquo;strange&rdquo; port: <code>ms-sql-s</code>. That &ldquo;name&rdquo; is <a href="http://www.t1shopper.com/tools/port-number/1433/"><em>IANA</em>&rsquo;s</a> (Internet Assigned Numbers Authority) definition for SQL Server&rsquo;s port 1433. As we know, port 1433 is the default port SQL uses for connections and retrieval of data. So it looks like that when we use pull, we connect to SQL Server over the default port and retrieves the data that way. Thinking about it, it makes sense as the connection is an ODBC connection. All the packets received by the SqlSatellite are the regular ODBC data packets. The actual 50 rows of data are in the packet outlined in blue with a length of 1358. As we use ODBC the protocol used to send the data is TDS.</p>

<p>Oh, TDS - that is probably a reason why the local pull is slower than local push, as the local push uses the <a href="/2017/11/25/microsoft-sql-server-r-services---internals-xiv/"><strong>Binary eXchange Language</strong></a> protocol (BXL) which is very efficient for transferring data. Another reason why the local pull is slower than the local push, even with small datasets, is that for local pull there is much more happening, as we see in <em>Figure 7</em>.</p>

<p>Right, then what about SQLCC pull:</p>

<p><img src="/images/posts/sql_ml_services_compctx_II_procmon_pull_sqlcc2.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>TCP Receive SQLCC Pull</em></p>

<p>Oh my, that is a lot! As in <em>Figure 7</em> the sections outlined in red is the connection between SQL Server and the SqlSatellite, and in blue it is the &ldquo;ODBC&rdquo; connection. What is noticeable is that there are multiple sections interleaved, as well that there are multiple <code>BxlServer.exe</code> processes involved (process id&rsquo;s 2108, 13360 and 15340). Well, maybe that is not such a surprise as we spoke about it in <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">Context - I</a>.</p>

<p>What is more interesting though is that we receive the dataset both via the ODBC connection outlined in blue (length 1358), as well as the way we do it in the local push context, outlined in purple (length 1392)! That means that SQL sends data using both the TDS protocol as well as the BXL protocol.</p>

<p>By seeing the amount of &ldquo;stuff&rdquo; happening in <em>Figure 8</em> we do realise why the SQLCC pull is not as efficient as local push and local pull (1.6 s vs ~200 ms). Having seen all this, we probably ask ourselves why the SQLCC pull was a lot faster (~15 s) than local pull (~23 s) for a big dataset and somewhat faster than the local push (~17 s)?</p>

<p>Let us execute the code in <em>Code Snippet 3</em> and <em>Code Snippet 4</em> with <code>TOP (30000000)</code> (30 million) and see what <em>Process Monitor</em> tells us. For local push, we see many packets with a size of 65495 which is the maximum size for BXL data package. When we execute the local pull, we see many TDS packets with a size of 4096 followed by many packages with sizes ranging from ~70,000 up to 2.5 Mb. For me, it looks like the local pull is nowhere as efficient as the local push. Finally, the SQLCC pull shows the same behaviour as local pull with many TDS 4096 packages. However, after the TDS packages follows BXL packages where most have the maximum size of 65495.</p>

<blockquote>
<p><strong>NOTE:</strong> I do not know why, in the case of SQLCC, data is first loaded via TDS and then BXL. I also do not know why in the case of local pull we see multiple 4096 packages followed by packages with an arbitrary big size. I see if I can find answers to this, in which case update this post (or write a new).</p>
</blockquote>

<h2 id="summary">Summary</h2>

<p>This post set out to try to find out why SQLCC performs better than local context. I believe we found why this is the reason but not necessarily how it works.</p>

<p>What did we see:</p>

<ul>
<li>Local push performs really, really well up until it does not :). It performs well up until you hit memory restrictions.</li>
<li>Some of the memory issues can be alleviated by using the <code>@r_rowsPerRead</code> parameter (not shown in this post).</li>
<li>When pushing the data (<code>@input_data_1</code>) we cannot use SQLCC.</li>
<li>Both local pull as well as SQLCC uses ODBC connections, and the data transfer protocol is TDS.</li>
<li>When using SQLCC the BXL protocol is also used.</li>
<li>By the use of BXL we get very efficient processing of data, and that is the reasons we see good performance.</li>
</ul>

<p>After writing this post, I have quite a few questions which I will try to answer in a future post.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 26]]></title>
    <link href="http://nielsberglund.com/2018/07/01/interesting-stuff---week-26/" rel="alternate" type="text/html"/>
    <updated>2018-07-01T05:51:55+02:00</updated>
    <id>http://nielsberglund.com/2018/07/01/interesting-stuff---week-26/</id>
    <content type="html"><![CDATA[<p>Geez, does time fly or what? We are already past the halfway mark of the year! Anyway, throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://www.allthingsdistributed.com/2018/06/purpose-built-databases-in-aws.html">A one size fits all database doesn&rsquo;t fit anyone</a>. A very interesting post by <a href="https://en.wikipedia.org/wiki/Werner_Vogels">Werner Vogels</a>, CTO at Amazon, where he argues that (from the article): <em>The days of the one-size-fits-all monolithic database are behind us, and developers are now building highly distributed applications using a multitude of purpose-built databases.</em>. As I said, a very interesting read!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-syslog-processing-apache-kafka-ksql-enriching-events-with-external-data/">We ❤ syslogs: Real-time syslog processing with Apache Kafka and KSQL—Part 3: Enriching events with external data</a>. This article is the third in the series about syslog processing and Apache Kafka. In this episode <a href="https://twitter.com/rmoff">Robin Moffat</a> discusses how the inbound streams of syslog data can be enriched. Awesome article!</li>
</ul>

<h2 id="big-data-cloud">Big Data / Cloud</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/structured-streaming-with-databricks-into-power-bi-cosmos-db/">Structured streaming with Azure Databricks into Power BI &amp; Cosmos DB</a>. A post, discussing the concept of Structured Streaming and how a data ingestion path can be built using <a href="https://docs.microsoft.com/en-gb/azure/azure-databricks/what-is-azure-databricks">Azure Databricks</a> to enable the streaming of data in near-real-time. The post also talks about how Databricks can be connected directly into Power BI for reporting etc., and to Cosmos DB for persistence.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/the-emerging-big-data-architectural-pattern/">The emerging big data architectural pattern</a>. A very interesting blog post, discussing the popularity and success of the <a href="https://en.wikipedia.org/wiki/Lambda_architecture">Lambda</a> architecture as well as some of the shortcomings. The post then goes on to talk about how some of the shortcomings of Lambda can be solved by the use of Azure and <a href="https://azure.microsoft.com/en-gb/services/cosmos-db/">Azure Cosmos DB</a>. In essence, the post discusses how we can implement the <a href="http://dataottam.com/2016/06/02/understand-kappa-architecture-in-2-minutes/">Kappa</a> architecture in Azure.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/a-closer-look-at-azure-data-lake-storage-gen2/">A closer look at Azure Data Lake Storage Gen2</a>. Microsoft recently announced <a href="https://azure.microsoft.com/en-us/services/storage/data-lake-storage/">Azure Data Lake Storage Gen2</a> and this post discusses some of the new features and capabilities.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://databricks.com/blog/2018/06/27/rstudio-integration.html">Announcing RStudio and Databricks Integration</a>. This post announces the integration of RStudio with the <a href="https://databricks.com/glossary/what-is-unified-analytics">Databricks Unified Analytics Platform</a>. Databricks &ldquo;pops up&rdquo; all over the place lately. I really need to look into it!</li>
<li><a href="https://blogs.msdn.microsoft.com/buckwoody/2018/06/28/the-data-analysis-maturity-model-level-three-distributed-consistent-reporting-systems/">The Data Analysis Maturity Model – Level Three: Distributed, consistent reporting systems</a>. The third &ldquo;episode&rdquo; in <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody&rsquo;s</a> series about data analysis maturity levels. In this post, Buck talks about distributed and consistent reporting systems.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<ul>
<li><a href="/2018/06/30/installing-r-packages-in-sql-server-machine-learning-services---ii/">Installing R Packages in SQL Server Machine Learning Services - II</a>. I published part 2 of the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services Series</a>. In this post, we discussed how to use functionality in RevoScaleR to install packages on a remote <strong>SQL Server ML Services</strong> instance.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> (/sql_server_2k16_r_services) -->

<!-- [series2]: <> (/sql_server_ml_services_install_packages) -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing R Packages in SQL Server Machine Learning Services - II]]></title>
    <link href="http://nielsberglund.com/2018/06/30/installing-r-packages-in-sql-server-machine-learning-services---ii/" rel="alternate" type="text/html"/>
    <updated>2018-06-30T16:57:01+02:00</updated>
    <id>http://nielsberglund.com/2018/06/30/installing-r-packages-in-sql-server-machine-learning-services---ii/</id>
    <content type="html"><![CDATA[<p>This post is the second post in a series about installing R packages in SQL Server Machine Learning Services.</p>

<p>To see all posts in the series go to <a href="/sql_server_ml_services_install_packages"><strong>Install R Packages in SQL Server ML Services Series</strong></a>.</p>

<p>Why this series came about is a colleague of mine <a href="https://www.linkedin.com/in/dane-bax/">Dane</a> pinged me and asked if I had any advice as he had issues installing an R package into one of their SQL Server instances. I tried to help him and then thought it would make a good topic for a blog post. Of course, at that time I didn&rsquo;t think it would be more posts than one, but here we are.</p>

<p></p>

<h2 id="recap">Recap</h2>

<p>In <a href="/2018/06/23/installing-r-packages-in-sql-server-machine-learning-services---i/">Installing R Packages in SQL Server Machine Learning Services - I</a> we introduced <a href="https://www.linkedin.com/in/dane-bax/">Dane</a> and how he wanted to install some R packages into SQL Server Machine Learning Service instance. I said that before you start installing packages into SQL Server Machine Learning Services, you should make sure you have <strong>Rtools</strong> installed on the machine SQL Server ML Services runs on.</p>

<p>You need <em>RTools</em> if you ever need to compile R packages from source, as <em>Rtools</em> contains compilers and utilities to do this. You do not install <em>Rtools</em> for each SQL Server ML Services instance you have on the server, but just once, and you need to install it from an account with elevated permissions.</p>

<p>In the first post I said that there were three ways to install packages into SQL Server ML Services:</p>

<ul>
<li>R packet managers</li>
<li>T-SQL</li>
<li>RevoScaleR</li>
</ul>

<p>So in the first post, I covered R packet managers, where an R packet manager is an R command line tool or GUI installed on the SQL Server Machine Learning Services machine. The packet manager should be run with elevated permissions and target the R engine for the instance on which you want to install the package. The easiest is to use either of the R tools that come as part as part of SQL Server&rsquo;s R service:</p>

<ul>
<li>The command line tool: <code>Rterm.exe</code>.</li>
<li>The GUI: <code>Rgui.exe</code>.</li>
</ul>

<p>These two packet managers lives in the <code>\\&lt;path_to_SQL_Server_instance&gt;\R_SERVICES\bin\x64</code> directory. When you install packages via an R packet manager, they can only be installed to the default packet library for that instance. The file system folder for this library has restricted access and to write to this folder you need elevated permissions. Typical code for installing packages from a packet manager can look like so:</p>

<pre><code class="language-r"># get the library path
libPath &lt;- .libPaths()[1]
install.packages(&quot;pkg_name&quot;, lib = libPath, 
                  repos = &quot;url_for_the_repo&quot;, 
                  dependencies = TRUE)
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install Packages Command</em></p>

<p>In <em>Code Snippet 1</em> we see how I first retrieve the library path. The library path is where I install the package to, and once again - I can only install to the default instance directory. Then in the <code>install.packages</code> call I use these parameters:</p>

<ul>
<li>First parameter is always the name of the package(s) to install.</li>
<li><code>lib</code>: is for the library folder to install to.</li>
<li><code>repos</code>: the base URL(s) of the repositories to use. If left out, the repo used is the Microsoft MRAN repo, which may not be what you want.</li>
<li><code>dependencies</code>: indicating whether to also install missing packages which these packages depend on/link to/import/suggest (and so on recursively).</li>
</ul>

<p>The big drawback with using an R packet manager is that the user who does it need to be able to run the chosen packet manager from an elevated prompt and from the box SQL Server is installed on!</p>

<p>In this post, we look at how we can install without being admin, and without having to be logged onto the box as admin.</p>

<h2 id="housekeeping">Housekeeping</h2>

<p>Before we &ldquo;dive&rdquo; into today&rsquo;s topics let us look at the code we use today. This section is here for those of who want to follow along in what we are doing in the post.</p>

<pre><code class="language-sql">IF NOT EXISTS(SELECT 1 FROM sys.server_principals 
              WHERE name = 'dane')
BEGIN
  CREATE LOGIN dane
  WITH PASSWORD = 'password1234$';
END

IF NOT EXISTS(SELECT 1 FROM sys.server_principals 
              WHERE name = 'nielsb')
BEGIN
  CREATE LOGIN nielsb
  WITH PASSWORD = 'password1234$';
END

IF NOT EXISTS(SELECT 1 FROM sys.server_principals 
              WHERE name = 'user1')
BEGIN
  CREATE LOGIN user1
  WITH PASSWORD = 'password1234$';
END

IF NOT EXISTS(SELECT 1 FROM sys.server_principals 
              WHERE name = 'user2')
BEGIN
  CREATE LOGIN user2
  WITH PASSWORD = 'password1234$';
END

IF NOT EXISTS(SELECT 1 FROM sys.server_principals 
              WHERE name = 'user3')
BEGIN
  CREATE LOGIN user3
  WITH PASSWORD = 'password1234$';
END

DROP DATABASE IF EXISTS DataScienceDB;
GO

CREATE DATABASE DataScienceDB;
GO

USE DataScienceDB;
GO

CREATE USER dane
FROM LOGIN dane;

CREATE USER nielsb
FROM LOGIN nielsb;

CREATE USER user1
FROM LOGIN user1;

CREATE USER user2
FROM LOGIN user2;

CREATE USER user3
FROM LOGIN user3;
GO

ALTER ROLE db_owner
  ADD MEMBER nielsb;
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Logins, Database and Users</em></p>

<p>In <em>Code Snippet 2</em> we create some logins as well as a database and in that database users for the logins. As you see, we do continue with the &ldquo;theme&rdquo; of Dane the data scientist wanting to do &ldquo;stuff&rdquo; in the database. As <code>nielsb</code> is seen to be &ldquo;trustworthy&rdquo; (take that Dane), we add him to the <code>db_owner</code> role.</p>

<h2 id="permissions-and-sp-execute-external-script">Permissions and sp_execute_external_script</h2>

<p>This topic is not directly related to installing R packages via RevoScaleR, but unless you get the permissions right for users that want to call <code>sp_execute_external_script</code> (SPEES), the rest of the post will not make any sense anyhow. Fortunately I wrote a <a href="/2018/06/24/sp_execute_external_script-and-permissions/">blog post</a> about <em>SPEES</em> and the permissions required, so let us follow what is in that post and set the permissions for the users we created in <em>Code Snippet 2</em>. The code below must be run by <code>sysadmin</code>:</p>

<pre><code class="language-sql">-- the grant to execute SPEES to public only 
-- needs to be run once on the instance
USE master;
GO

GRANT EXECUTE ON sp_execute_external_script TO public;
GO

USE DataScienceDB;
GO

GRANT EXECUTE ANY EXTERNAL SCRIPT TO dane;
GRANT EXECUTE ANY EXTERNAL SCRIPT TO user1;
GRANT EXECUTE ANY EXTERNAL SCRIPT TO user2;
GRANT EXECUTE ANY EXTERNAL SCRIPT TO user3;
GO
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Grant Execute Permissions to Users</em></p>

<p>We see how the code in <em>Code Snippet 3</em> first, from within the <code>master</code> database, grants execution rights on <em>SPEES</em> to <code>public</code>. This only needs to be run once, and arguably this should be part of enabling <em>SPEES</em>.</p>

<p>We then switch over to the database where the users exist, and we grant the <code>EXECUTE ANY EXTERNAL SCRIPT</code> rights to the individual users that should be able to use <em>SPEES</em>. However, what about <code>nielsb</code>, should he not be granted the permission as well? No, as he is part of <code>db_owner</code> he automatically has the necessary permissions.</p>

<h2 id="revoscaler-package-management">RevoScaleR Package Management</h2>

<p>In the recap above I mentioned about being able to install from a remote box and not being an admin on the SQL Server box, and that is what RevoScaleR gives you. RevoScaleR 9.0.1 and later includes functions for R package management where these functions can be used by remote, non-administrators to install packages on SQL Server without direct access to the server. To install you can use both remote R clients as well as <code>sp_execute_external_script</code> from <em>SSMS</em>. To install using RevoScaleR functions you need to execute inside a <strong>SQL Server Compute Context</strong> (SQLCC).</p>

<blockquote>
<p><strong>NOTE:</strong> To read more about <em>SQLCC</em> look at the <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">sp_execute_external_script and SQL Compute Context - I</a> post.</p>
</blockquote>

<p>Remember from my <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/"><em>SQLCC</em> post</a> how we set up an <em>SQLCC</em>:</p>

<pre><code class="language-r"># set up the connection string
sqlServerConnString &lt;- &quot;Driver=SQL Server;server=&lt;instance&gt;;
                        database=&lt;the_db&gt;;
                        uid=&lt;the user id&gt;;pwd=&lt;the password&gt;&quot;

# set up a compute context
sqlCtx &lt;- RxInSqlServer(connectionString = sqlServerConnString, 
                        numTasks = 1)
# set the compute context to be the sql context
rxSetComputeContext(sqlCtx)
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>R Code - SQL Server Compute Context</em></p>

<p>In <em>Code Snippet 4</em> we see how we set up an <em>SQLCC</em> by defining a connection string which then implies that any R package installation is specific to a SQL Server instance as well as a database.</p>

<h4 id="configuration">Configuration</h4>

<p>Above I wrote that even non-admins could install R packages but that requires configuration and to use RevoScaleR you need to enable remote management of R packages both on the SQL Server instance as well as for a particular database. To enable, an admin on the SQL Server box uses a RevoScaleR command line utility: <code>RegisterRext.exe</code>. The same utility can also be used to disable package management on a database and/or instance level.</p>

<p>The location for <code>RegisterRext.exe</code> is at: <code>&lt;path_to_sql_instance&gt;\R_SERVICES\library\RevoScaleR\rxLibs\x64\RegisterRext.exe</code> and when you run it you need to run it as admin. These are the available commands when executing:</p>

<ul>
<li><code>/install</code> - copies the launcher binaries to the right location for a SQL server instance. Not applicable in this post.</li>
<li><code>/uninstall</code> - removes the launcher binaries from the SQL server instance. Not applicable in this post.</li>
<li><code>/installpkgmgmt</code> - installs package management support for given SQL server instance and database also if specified.</li>
<li><code>/uninstallpkgmgmt</code> - uninstalls package management support from given SQL server instance and database also if specified.</li>
<li><code>/installrts</code> - installs real time scoring support for given SQL server instance and database. Topic for a future post.</li>
<li><code>/uninstallrts</code> - uninstalls real time scoring support for given SQL server instance and database. Topic for a future post.</li>
</ul>

<p>We send in parameters to tell RegisterRext where to execute:</p>

<ul>
<li><code>/instance:value</code> is an optional parameter, we use <code>&lt;value&gt;</code> as the instance name, else the commands are performed on the default instance. Please note that the name is just that; not <code>/instance:server_name\instance_name</code> but <code>/instance:instance_name</code>. It took me a while to figure that out.</li>
<li><code>/database:value</code> is an optional parameter where <code>&lt;value&gt;</code> indicates what database to run on.<br /></li>
<li><code>/user:username</code> is an optional parameter, and we use the <code>&lt;username</code>&gt; specified to connect via the SQL authentication mode. Note that once this parameter is specified, <code>/password:&lt;value&gt;</code> is a required parameter.</li>
<li><code>/password: * | &lt;password&gt;</code> is a required parameter once the <code>/user</code> parameter is provided. You can use <code>/password:*</code> in order to be prompted for the password when running the tool instead of providing it in cleartext as a parameter. Note that providing the password without the user is ignored.</li>
</ul>

<p>If no username and password is supplied, then Windows authentication is used (in the context of the user which is logged on to the SQL Server box and runs <code>RegisterRext.exe</code>).</p>

<p>Let us go back to <a href="https://www.linkedin.com/in/dane-bax/">Dane</a> and his situation and see what we can do. Admin of the SQL Server instance Dane <del>plays on</del> works hard on has created a database for Dane with a lot of data where Dane now want to do data science &ldquo;things&rdquo;. Part of the things Dane want to do requires packages that are not part of a SQL Server ML Services installation.</p>

<p>So the admin decides to enable Dane&rsquo;s database for remote package management. As this is the first time a database on the instance is enabled, the admin needs to enable the instance itself before the database.</p>

<p>To enable the instance a user with admin rights on the box, logs on and runs <code>RegisterRext.exe</code> from an elevated command prompt:</p>

<pre><code class="language-bash">&gt; RegisterRext.exe /installpkgmgmt
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Enable Default Instance</em></p>

<p>In <em>Code Snippet 5</em> we enable packet management on the default instance (as that is where Dane&rsquo;s database is), and we enable it using Windows Authentication. Regardless of type of authentication, the user has to have <code>VIEW SERVER STATE</code> permissions on the SQL Server instance. The output from <em>Code Snippet 5</em> is like so (edited for readability):</p>

<pre><code class="language-bash">&gt;RegisterRExt /installpkgmgmt

Source directory to pick the RExtension binaries determined 
to be &quot;C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\
R_SERVICES\library\RevoScaleR\rxLibs\x64\&quot;.

Connecting to SQL server...

Sql server binn directory is 
&quot;C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\MSSQL\Binn&quot;.

Stopping service MSSQLLaunchpad...
Copied RLauncher.dll from C:\Program Files\Microsoft SQL Server\
MSSQL14.MSSQLSERVER\R_SERVICES\library\RevoScaleR\rxLibs\x64\ 
to C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\MSSQL\Binn.

Installing SQL R services package management for instance ''

Installing version 1.0 artifacts

Creating package management account MSSQLSERVERPKG...

Saving package management user account configuration...

Working directory long path:'C:\Program Files\Microsoft SQL Server\
MSSQL14.MSSQLSERVER\MSSQL\ExtensibilityData' and short path:
'C:\PROGRA~1\MICROS~1\MSSQL1~1.MSS\MSSQL\EXTENS~1'.

Working directory long path:'C:\Program Files\Microsoft SQL Server\
MSSQL14.MSSQLSERVER\MSSQL\ExtensibilityData' and short path:
'C:\PROGRA~1\MICROS~1\MSSQL1~1.MSS\MSSQL\EXTENS~1'.

Working directory long path:'C:\Program Files\Microsoft SQL Server\
MSSQL14.MSSQLSERVER\MSSQL\ExtensibilityData' and short path:
'C:\PROGRA~1\MICROS~1\MSSQL1~1.MSS\MSSQL\EXTENS~1'.

Configuring security for folder C:\Program Files\Microsoft SQL Server\
MSSQL14.MSSQLSERVER\MSSQL\ExtensibilityData...

SQL R services package management installed successfully.

Starting service MSSQLLaunchpad...
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Output Enable Instance</em></p>

<p>A couple of interesting points from what we see in <em>Code Snippet 6</em>:</p>

<ul>
<li>The launchpad service is stopped when the install process starts and at the end restarted.</li>
<li>The install process creates a new packet management account. This account appears together with the worker accounts in the <em>user accounts</em> pool that you find in the <code>ExtensibilityData</code> folder.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> As the instance only is enabled once, this could potentially be part of when we enable an instance for external scripts.</p>
</blockquote>

<p>Having enabled the instance, admin can now go ahead and enable the database:</p>

<pre><code class="language-bash">&gt; RegisterRext.exe /installpkgmgmt /database:DataScienceDB
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Enable Database</em></p>

<p>In <em>Code Snippet 7</em> we see how we enable packet management on the <code>DataScienceDB</code> database, once again using Windows Authentication. Whereas enabling an instance requires <code>VIEW SERVER STATE</code> permissions for the authenticated user, when enabling a database the user needs both <code>VIEW SERVER STATE</code> as well as being part of the <code>db_owner</code> database role.</p>

<p>The output from <em>Code Snippet 7</em> is like so:</p>

<pre><code class="language-bash">&gt;RegisterRExt /installpkgmgmt /database:DataScienceDB

Source directory to pick the RExtension binaries determined 
to be &quot;C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\
R_SERVICES\library\RevoScaleR\rxLibs\x64\&quot;.

Connecting to SQL server...

Sql server binn directory is &quot;C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\MSSQL\Binn&quot;.

Installing SQL R services package management for database 'DataScienceDB'

Installing version 1.0 artifacts

Working directory long path:'C:\Program Files\Microsoft SQL Server\
MSSQL14.MSSQLSERVER\MSSQL\ExtensibilityData' and short path:
'C:\PROGRA~1\MICROS~1\MSSQL1~1.MSS\MSSQL\EXTENS~1'.

Creating package management folder C:\Program Files\Microsoft SQL Server\
MSSQL14.MSSQLSERVER\MSSQL\ExtensibilityData\
rpkgs\4ecd5724-d180-47da-a8e0-01d6c1b9bd0f...

Creating package management table rpackages...

Creating package management role rpkgs-users...

Creating package management role rpkgs-private...

Creating package management role rpkgs-shared...

Creating package management stored procedures ...

Installing version 1.1 artifacts

Altering package management table rpackages to include attributes flags...

Creating package management stored procedures ...

SQL R services package management installed successfully for database 'DataScienceDB'
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Output Enable Database</em></p>

<p>As with the output from enabling an instance (<em>Code Snippet 6</em>) we see some interesting things in <em>Code Snippet 8</em> for when enabling a database, and we see how the process:</p>

<ul>
<li>Creates a table: <code>rpackages</code>.</li>
<li>Creates roles.</li>
<li>Creates stored procedures:</li>
</ul>

<p><img src="/images/posts/sql_ml_install_r_pckgs_procedures.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Stored Procedures Created</em></p>

<p>In <em>Figure 1</em> we see the procedures that the enabling process created. In a future post, we talk more about these procedures</p>

<h2 id="revoscaler-functions-scopes-roles">RevoScaleR Functions, Scopes &amp; Roles</h2>

<p>RevoScaleR exposes functions similar to CRAN R functions for package management:</p>

<ul>
<li><code>rxSqlLibPaths</code>: Retrieves the path of the instance library on the remote server.</li>
<li><code>rxFindPackage</code>: retrieves the path for one or more packages on the remote SQL Server.</li>
<li><code>rxInstallPackages</code>: Install packages and their dependencies in an SQLCC from a remote client. You can specify which repo to install from or also install from locally saved zipped packages.</li>
<li><code>rxInstalledPackages</code>:  Retrieves a list of packages installed in the specified compute context.</li>
<li><code>rxSyncPackages</code>:  Copy information about a package library between the file system and database.</li>
<li><code>rxRemovePackages</code>:  Removes packages from a specified compute context. It also removes dependencies if other packages on SQL Server no longer use them.</li>
</ul>

<p>Back to our &ldquo;hero&rdquo; Dane. Admin has just told him that the SQL Server instance together with his database are now package management enabled, so he can begin to do his awesome &ldquo;stuff&rdquo;. So what Dane first want to do is to see what packages are available to him in his database, and he does not really want to use <em>SSMS</em>, but instead use his R client of choice; Visual Studio together with <strong>Microsoft R Client</strong>:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs_VS1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Visual Studio with Microsoft R Client</em></p>

<p>In <em>Figure 2</em> we see how Dane&rsquo;s R project in Visual Studio targets  Microsoft R Client. Dane now wants to use the following code to see packages available on his database:</p>

<pre><code class="language-r">&gt; sqlCCConnString &lt;- &quot;Driver=SQL Server; server=&lt;server_name&gt;; 
                     database=daneb; uid=daneb; pwd=&lt;danes_pwd&gt;&quot;
&gt; sqlCC &lt;- RxInSqlServer(connectionString = sqlCCConnString, numTasks = 1)
&gt; rxSetComputeContext(sqlCC)
&gt; sqlPackages &lt;- rxInstalledPackages(computeContext = sqlCC)
&gt; sqlPackages
</code></pre>

<p><strong>Code Snippet 9:</strong></p>

<p>In <em>Code Snippet 9</em> we see how Dane:</p>

<ul>
<li>Sets up a connection string pointing to his database for the SQLCC.</li>
<li>Creates the SQLCC and sets it.</li>
<li>Calls <code>rxInstalledPackages</code> to retrieve packages.</li>
</ul>

<p>When Dane executes the code in <em>Code Snippet 9</em> it does not go as he wants - he gets an error at the <code>rxInstalledPackages</code> call:</p>

<pre><code class="language-r">Error in rxCheckPackageManagementVersion
     (connectionString = computeContext@connectionString) : 
  The package management feature is not enabled for the current 
  user or not supported on SQL Server version 14.0.1000.169
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Execution Error</em></p>

<p>Hmm, the error we see in <em>Code Snippet 10</em> says something about &ldquo;The package management feature is not enabled&hellip;&rdquo;  What is this all about?</p>

<h4 id="scopes-roles">Scopes &amp; Roles</h4>

<p>The error above is related to what happened when we enabled the database. Remember how the process created roles in the database and the user that tries to execute some of the package management functions need to be in some of the roles unless the user is part of <code>db_owner</code>. In <em>Code Snippet 8</em> we see the roles, and there is a role named <code>rpkgs-users</code>. When I see that role, it seems like a good candidate for Dane to be part of, so I add Dane to the role:</p>

<pre><code class="language-sql">ALTER ROLE [rpkgs-users] 
  ADD MEMBER dane;
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Add Member to Role</em></p>

<p>Notice in <em>Code Snippet 11</em> how I have to enclose the role name in brackets([]) as the name has a hyphen in it. When Dane now tries to execute the code:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs_scope_error.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Error</em></p>

<p>Ok, we seem to be a little bit further than before, and it seems we now can execute, but there is still an error, and it looks related to &ldquo;scope&rdquo;, whatever that is.</p>

<p>Scopes has to do with installation of packages via the <em>SQLCC</em> and the usage of those packages. There are two scopes:</p>

<ul>
<li>Shared: shares packages with other users in the database.</li>
<li>Private: accessible only to the user installing the package.</li>
</ul>

<p>Scope and the roles we see above are related in that the roles define what the user can do and in what scope:</p>

<ul>
<li><code>rpkgs-users</code>: users in this role can use packages installed by users belonging to the <code>rpkgs-shared</code> role. Cannot read via the <code>rx...</code> functions.</li>
<li><code>rpkgs-private</code>: this role has all permissions as the <code>rpkgs-users</code> role has. A user in this role can furthermore, install, remove and use private packages installed by the user in question.</li>
<li><code>rpkgs-shared</code>: The same permissions as the <code>rpkgs-private</code> role. Users in this role can install, remove, and use shared packages.</li>
</ul>

<p>So, let us now see what we can do with these roles:</p>

<pre><code class="language-sql">ALTER ROLE [rpkgs-users] 
  ADD MEMBER user1;

ALTER ROLE [rpkgs-shared] 
  ADD MEMBER dane;

ALTER ROLE [rpkgs-shared] 
  ADD MEMBER user2;

ALTER ROLE [rpkgs-private] 
  ADD MEMBER user3;

</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Add Users to Roles</em></p>

<p>After we have added users to various roles, let us see what happens when different users want to install packages.</p>

<h2 id="revoscaler-package-installation">RevoScaleR Package Installation</h2>

<p>In the following examples, I have chosen a package <code>abc</code> mostly due to it not having too many dependencies, which makes it quicker to install. The <code>abc</code> package contains tools for &ldquo;Approximate Bayesian Computation&rdquo; (ABC).</p>

<p>Something to think about are the versions of <em>RevoScaleR</em> on your development box vs the version on the remote SQL Server. My, admittedly, limited investigations lead me to believe that the version on the local machine cannot be an earlier version than on the remote SQL Server. If the version is earlier on the local machine, you get permission errors. From what I can tell, the local version can be the same, or newer.</p>

<p>Ok, so to install packages we use <code>rxInstallPackagaes()</code>:</p>

<pre><code class="language-r">rxInstallPackages(pkgs, skipMissing = FALSE, 
                  repos = getOption(&quot;repos&quot;), 
                  verbose = getOption(&quot;verbose&quot;), 
                  scope = &quot;private&quot;, owner = '', 
                  computeContext = rxGetOption(&quot;computeContext&quot;))
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>Signature rxInstallPackages</em></p>

<p>Let us look at the parameters we see in <em>Code Snippet 13</em>:</p>

<p><strong><code>pkgs</code>:</strong></p>

<p>String array of the packages you want to download and install. By default, it downloads the latest version of the package(s). When installing through an <strong>SQLCC</strong> the arrays can be .zip files as well as URL&rsquo;s, <code>http://</code> and <code>file://</code>. If the URL is a file, the location must be available to SQL Server.</p>

<p><strong><code>skipMissing</code>:</strong></p>

<p>Boolean (<code>TRUE</code>, <code>FALSE</code>) indicating whether not to download dependent packages. The argument defaults to false.</p>

<p><strong><code>repos</code>:</strong></p>

<p>String array of base URL&rsquo;s of repos to use. It defaults to the MRAN repo. If you want to install from local files <code>repos</code> has to be explicitly set to <code>NULL</code>.</p>

<p><strong><code>verbose</code>:</strong></p>

<p>Boolean (<code>TRUE</code>, <code>FALSE</code>) indicating whether to output progress during installation.</p>

<p><strong><code>scope</code>:</strong></p>

<p>Related to what I said above about <code>scope</code>. The <code>scope</code> argument indicates whether to install packages, so they are available for other users on the database: <code>shared</code>, or only accessible to the user calling <code>rxInstallPackages</code>: <code>private</code>. The default value is <code>private</code>. To install to <code>shared</code> scope the user has to be in the <code>rpkgs-shared</code> or <code>db_owner</code> role.</p>

<p><strong><code>owner</code>:</strong></p>

<p>A user in the <code>db_owner</code> role can install packages on behalf of other users, and set the <code>owner</code> parameter to a valid database user account.</p>

<p><strong><code>computeContext</code>:</strong></p>

<p>The compute context to install under.</p>

<h4 id="install-already">Install Already</h4>

<p>Finally, we can talk about actually installing a package. Dane now wants to install the <code>abc</code> package:</p>

<pre><code class="language-r">sqlCCConnString &lt;- &quot;Driver=SQL Server; 
                    server=192.168.57.3;
                    database=DataScienceDB; 
                    uid=dane; pwd=password1234$&quot;
sqlCC &lt;- RxInSqlServer(connectionString = sqlCCConnString, numTasks = 1)
rxSetComputeContext(sqlCC)
pkgs &lt;- c(&quot;abc&quot;)
rxInstallPackages(pkgs = pkgs, verbose = TRUE, computeContext = sqlCC)
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>Install R Package</em></p>

<p>In <em>Code Snippet 14</em> we see how Dane:</p>

<ul>
<li>Sets up the connection string for the <em>SQLCC</em>.</li>
<li>Created the <em>SQLCC</em> and sets it.</li>
<li>Defines the package(s): <code>pkgs &lt;- c(&quot;abc&quot;)</code>.</li>
</ul>

<p>When he calls <code>rxInstallPackages</code>, he follows the progress in the Visual Studio R interactive window:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs2_install_progress.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Installation Progress</em></p>

<p>Dane sees in <em>Figure 4</em> how the various packages needed for <code>abc</code> downloads and subsequently installed. After a couple of minutes the installation is completed:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs2_install_fiished.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Installation Finished</em></p>

<p>After the installation has finished, let us see how it works.</p>

<h4 id="usage">Usage</h4>

<p>So in <em>Figure 5</em> we see that the installation finished successfully. Dane wants now to start using the new packages he installed in SQL Server, and for testing purposes, he uses the <code>abc.data</code> package (installed as a dependency of the <code>abc</code> package):</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
         @language = N'R',
         @script = N'
                library(&quot;abc.data&quot;)
                data(human)
                OutputDataSet &lt;-  data.frame(stat.voight)'
WITH RESULT SETS(([pi] float, [TajD.m] float, [TajD.v] float ))
</code></pre>

<p><strong>Code Snippet 15:</strong> <em>Using R Package</em></p>

<p>In <em>Code Snippet 15</em> it is assumed that Dane has logged in as himself and we see how Dane uses <a href="https://www.rdocumentation.org/packages/abc.data/versions/1.0/topics/human"><code>human</code></a> which is a set of R objects containing observed data from three human populations. The call <code>data(human)</code>, loads in four R objects and Dane is interested in <code>stat.voight</code>, which is a data frame with 3 rows and 3 columns. The data frame contains the observed summary statistics for three human populations. In his code, Dane outputs the <code>stat.voight</code> data frame. However, when Dane executes the code, this happens:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs2_error_exec1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Package does not Exist</em></p>

<p>Hmm, what happened here? We know that Dane installed the package, so why does he get an exception that the package does not exist? The reason is that the package he installed is not installed in the R engine, but installed on the database and is not visible. To be able to use the package Dane needs to execute under the <em>SQLCC</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> In future posts we talk more about where the packages are installed and how they get loaded etc.</p>
</blockquote>

<p>So, how does Dane make the <code>abc.data</code> package execute in the <em>SQLCC</em>. Well, the <code>abc.data</code> package does not know about <em>SQLCC</em>, so Dane needs to use a RevoScaleR wrapper function: <code>rxExec</code>. The <code>rxExec</code> function allows distributed execution of a function, and the signature looks like so:</p>

<pre><code class="language-r">rxExec(FUN,   ...  , elemArgs, elemType = &quot;nodes&quot;, 
       oncePerElem = FALSE, timesToRun = -1L,
       packagesToLoad = NULL, execObjects = NULL, 
       taskChunkSize = NULL, quote = FALSE, 
       consoleOutput = NULL, autoCleanup = NULL, 
       continueOnFailure = TRUE, RNGseed = NULL, 
       RNGkind = NULL, foreachOpts = NULL)

</code></pre>

<p><strong>Code Snippet 16:</strong> <em>Signature of rxExec</em></p>

<p>In <em>Code Snippet 16</em> we see <code>rxExec</code>&rsquo;s signature. In a future post we look at <code>rxExec</code> in more detail, but for now what interests us are the two first parameters:</p>

<ul>
<li><code>FUN</code>: The function to execute.</li>
<li><code>...</code>: Arguments passed to the function (<code>FUN</code>).</li>
</ul>

<p>The first parameter is the function we want <code>rxExec</code> to execute and the second parameter passes in the argument(s) the function requires. So, if we go back to what Dane wants to do, he re-writes his code like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
 @language = N'R',
 @script = N'
   # define a function to be passed to rxExec
   usePackageInRxFunction &lt;- function()
   {
     library(&quot;abc.data&quot;)
     data(human)
     return(stat.voight)
   }
   # the &quot;normal&quot; SQLCC stuff
   sqlCCConnString &lt;- &quot;Driver=SQL Server; server=.; 
                       database=DataScienceDB; 
                       uid=dane; pwd=password1234$&quot;
   sqlCC &lt;- RxInSqlServer(connectionString = sqlCCConnString, 
                          numTasks = 1)
   rxSetComputeContext(sqlCC)

   ret &lt;- rxExec(usePackageInRxFunction)
   OutputDataSet &lt;- data.frame(ret)'
WITH RESULT SETS(([pi] float, [TajD.m] float, [TajD.v] float ))
</code></pre>

<p><strong>Code Snippet 17:</strong> <em>Executing in SQLCC</em></p>

<p>We see how Dane in <em>Code Snippet 17</em>:</p>

<ul>
<li>Creates a function <code>usePackageInRxFunction</code> where he does the <code>abc.data</code> related work.</li>
<li>Creates and sets up the <em>SQLCC</em>.</li>
<li>Calls <code>rxExec</code> with <code>usePackageInRxFunction</code> as argument.</li>
<li>Returns a data set.</li>
</ul>

<p>When Dane executes the code the result is:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs2_exec_success1.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Success</em></p>

<p>So, everything works fine, and Dane gets a result back. What happens if someone else wants to execute the same code, for example, <code>nielsb</code> who is in the <code>db_owner</code> role? The only thing <code>nielsb</code> changes is the <code>sqlCCConnString</code> parameter:</p>

<pre><code class="language-sql"> sqlCCConnString &lt;- &quot;Driver=SQL Server; server=.; 
                       database=DataScienceDB; 
                       uid=nielsb; pwd=password1234$&quot;
</code></pre>

<p><strong>Code Snippet 18:</strong> <em>SQLCC Connectionstring for nielsb</em></p>

<p>When <code>nielsb</code> executes the code in <em>Code Snippet 17</em> but with the <code>sqlCCConnString</code> parameter looking like in <em>Code Snippet 18</em> the outcome is:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs2_error_exec2.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>No Package Exception</em></p>

<p>Hmm, that is strange - we know Dane installed the package (and he just used it), so it has to exist. Dane is part of the <code>rpkgs-shared</code> role so he has permissions to create shared packages? The reason here is even though Dane was in <code>rpkgs-shared</code> role when he ran <code>rxInstallPackages</code> (<em>Code Snippet 14</em>) he did not define the scope for the package. Remember when we looked at the signature and the arguments for <code>rxInstallPackages</code> in <em>Code Snippet 13</em> we said that default value for the <code>scope</code> argument is <code>private</code> and that is how the package is installed regardless of what role Dane is in.</p>

<p>So to finish off, let <code>user2</code> who is in <code>rpkgs-shared</code> install the same packages in <code>shared</code> scope. Does that work, can you have the same packages installed multiple times? Yes, you can in that you can have the same packages installed in both <code>private</code> as well as <code>shared</code> scope. So <code>user2</code> executes this code:</p>

<pre><code class="language-r">sqlCCConnString &lt;- &quot;Driver=SQL Server; 
                    server=192.168.57.3;
                    database=DataScienceDB; 
                    uid=user2; pwd=password1234$&quot;
sqlCC &lt;- RxInSqlServer(connectionString = sqlCCConnString, numTasks = 1)
rxSetComputeContext(sqlCC)
pkgs &lt;- c(&quot;abc&quot;)
rxInstallPackages(pkgs = pkgs, verbose = TRUE, scope = &quot;shared&quot;, computeContext = sqlCC)
</code></pre>

<p><strong>Code Snippet 19:</strong> <em>Install R Package</em></p>

<p>The only changes here in <em>Code Snippet 19</em> compared to the code in <em>Code Snippet 14</em> are the user id (<code>uid</code> -obviously), and <code>scope = &quot;shared&quot;</code> in the <code>rxInstallPackages</code> call. The <code>abc</code> package is now installed in <code>shared</code> scope, and anyone in any of the <code>rpkgs-...</code> roles can use the package.</p>

<h2 id="summary">Summary</h2>

<p>This turned out to be a much longer post than I thought it would be, and there are still topics related to RevoScaleR installation functionality that I want to cover, but that comes in future posts.</p>

<p>So, here is what we covered:</p>

<ul>
<li>To use RevoScaleR for package installation both the SQL Server instance as well as the database need to be enabled for package management. You enable package management via <code>RegisterRExt.exe</code> tool and the <code>/installpkgmgmt</code> option. There are additional flags for database enabling, authentication and so forth.</li>
<li>When enabling the database the process creates a table, stored procedures and roles.</li>
<li>For a user to be able to install packages he needs to have necessary permissions on <code>sp_execute_external_script1 as well as the</code>EXECUTE ANY EXTERNAL SCRIPT` permission. He also needs to be in a role which allows him to install packages.</li>
<li>The roles that the enabling process creates are: <code>rpkgs-users</code>, <code>rpkgs-private</code> and <code>rpkgs-shared</code>.</li>
<li>The roles which allows the user to install packages are <code>rpkgs-private</code> and <code>rpkgs-shared</code> (and <code>db_owner</code>).</li>
<li>The roles define the scope of the installed packages: <code>private</code> and <code>shared</code>.</li>
<li>When a user installs a package with <code>private</code> scope, only he can see and use the package.</li>
<li>If the user installs a package with <code>shared</code> scope, all users in any of the roles, including <code>rpkgs-users</code> can use that package. The user needs to be in the <code>rpkgs-shared</code> (or <code>db_owner</code>) to install a <code>shared</code> package.</li>
<li>You use the function <code>rxInstallPackages</code> to install a package, and the function needs to run in an <em>SQLCC</em>.</li>
<li>When the user calls <code>rxInstallPackages</code> he needs to define which scope the package has through the <code>scope</code> argument. If the <code>scope</code> is not defined, it defaults to <code>private</code>.</li>
<li>To use a package, either in <code>private</code> or <code>shared</code> scope, the code needs to run in <em>SQLCC</em>.</li>
<li>For a package that has no knowledge about <em>SQLCC</em>, the functions in the package can be run via <code>rxExec</code>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 25]]></title>
    <link href="http://nielsberglund.com/2018/06/24/interesting-stuff---week-25/" rel="alternate" type="text/html"/>
    <updated>2018-06-24T20:36:32+02:00</updated>
    <id>http://nielsberglund.com/2018/06/24/interesting-stuff---week-25/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="http://joeduffyblog.com/2018/06/18/hello-pulumi/">Hello, Pulumi!</a>. Back in 2017, I wrote in <a href="/2017/06/04/interesting-stuff---week-22/">Weekly Roundup 22</a> about Joe Duffy and his startup Pulumi. At that time, no one knew what Pulumi was all about. Well, that changed with today&rsquo;s linked post. Joe explains about Pulumi, and it does sound extremely exciting. Go forth and read all about it!</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/future-distributed-database-relational">The Future of Distributed Databases Is Relational</a>. This is an <a href="https://www.infoq.com/presentations/future-distributed-database-relational">InfoQ</a> presentation about creating a more modern relational database. It is about Postgres and the journey to scale out and make it truly distributed. Fascinating!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/sql-streaming-apache-flink">Streaming SQL to Unify Batch &amp; Stream Processing w/ Apache Flink @Uber</a>. Another <a href="https://www.infoq.com/">InfoQ</a> presentation. This presentation is about exploring SQL’s role in the world of streaming data and its implementation in Apache Flink and covering streaming semantics, event time, and incremental results. Interesting!</li>
<li><a href="https://data-artisans.com/blog/getting-started-with-da-platform-on-google-kubernetes-engine">Getting Started with dA Platform on Google Kubernetes Engine</a>. This article describes the setup of the <a href="https://data-artisans.com/platform">dA Platform</a> using Google Cloud&rsquo;s Kubernetes engine.</li>
<li><a href="https://www.confluent.io/blog/introducing-confluent-hub/">Introducing Confluent Hub</a>.  From the post: &ldquo;Confluent Hub is a place for the Apache Kafka and Confluent Platform community to come together and share the components the community needs to build better streaming data pipelines and event-driven applications.&rdquo;. Sounds intriguing!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blogs.msdn.microsoft.com/buckwoody/2018/06/20/the-data-analysis-maturity-model-level-two-reliable-data-storage-and-query-systems/">The Data Analysis Maturity Model – Level Two: Reliable Data Storage and Query Systems</a>. The second post in a series by <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> about defining a <em>Data Analysis Maturity Model</em>. In this post, Buck discusses data storage and querying.</li>
<li><a href="http://blog.revolutionanalytics.com/2018/06/pypl-programming-language-trends.html">PYPL Language Rankings: Python ranks #1, R at #7 in popularity</a>. A post by <a href="https://twitter.com/revodavid">David</a> at <a href="http://blog.revolutionanalytics.com">Revolution Analytics</a> about a popularity index of programming languages.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<p>Boy, have I been a busy bee this week. I have managed to get two posts published, which must be a record. But none of the posts is the follow-up post to my [sp_execute_external_script and SQL Compute Context - I][ctx1] post from five weeks ago. This is getting stupid!</p>

<ul>
<li><a href="/2018/06/23/installing-r-packages-in-sql-server-machine-learning-services---i/">Installing R Packages in SQL Server Machine Learning Services - I</a>. This post, which is the first post in a <a href="/sql_server_ml_services_install_packages">series</a>, came about due to a mate of mine asking how he could install R packages into <strong>SQL Server Machine Learning Services</strong>. In this particular post, we look at using R packet managers to install packages.</li>
<li><a href="/2018/06/24/sp_execute_external_script-and-permissions/">sp_execute_external_script and Permissions</a>. While researching the <a href="/sql_server_ml_services_install_packages">Installing R Packages</a> series I came about some issues related to permissions and <code>sp_execute_external_script</code>. This post tries to clarify and explain how it works.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> (/sql_server_2k16_r_services) -->

<!-- [series2]: <> (/sql_server_ml_services_install_packages) -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[sp_execute_external_script and Permissions]]></title>
    <link href="http://nielsberglund.com/2018/06/24/sp_execute_external_script-and-permissions/" rel="alternate" type="text/html"/>
    <updated>2018-06-24T14:36:36+02:00</updated>
    <id>http://nielsberglund.com/2018/06/24/sp_execute_external_script-and-permissions/</id>
    <content type="html"><![CDATA[<p>This post will (hopefully) be short and sweet. It came about as I was testing out &ldquo;stuff&rdquo; for the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series of posts and I could not get it to work as I had expected.</p>

<p></p>

<h2 id="background">Background</h2>

<p>Usually, when I work with <strong>SQL Server Machine Learning Services</strong>, I execute code in the context of admin (yeah I know, do not do that :)). In the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series I used non-admin accounts, and all of a sudden nothing worked.</p>

<p>I tried to research (read Google) the issue, but I could not find a definitive answer, just tidbits here and there. So when I finally realised what the issues were, I decided to write a blog post about it.</p>

<h2 id="housekeeping">Housekeeping</h2>

<p>As in quite a few of my other blog posts, here follows some code to set things up if you want to follow along.</p>

<h4 id="code">Code</h4>

<p>This is the code to &ldquo;set the scene&rdquo;:</p>

<pre><code class="language-sql">IF NOT EXISTS(SELECT 1 FROM sys.server_principals 
              WHERE name = 'user1')
BEGIN
  CREATE LOGIN user1
  WITH PASSWORD = 'password1234$';
END

DROP DATABASE IF EXISTS PermissionDB;
GO

CREATE DATABASE PermissionDB;
GO

USE PermissionDB;
GO

CREATE USER user1
FROM LOGIN user1;
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Login, Database and User</em></p>

<p>In <em>Code Snippet 1</em> we create a login, a database, and then we create a user for the login in the database.</p>

<blockquote>
<p><strong>NOTE:</strong> Below you see in quite a few places the abbreviation SPEES. That is short for <code>sp_excute_external_script</code>.</p>
</blockquote>

<h2 id="permissions">Permissions</h2>

<p>What we see in <em>Code Snippet 1</em> is that we have not assigned <code>user1</code> to any particular roles on either the server or the database, so <code>user1</code> has whatever default permissions he gets during creation. Let us look in <em>SSMS</em> UI and see what server level roles <code>user1</code> belongs to:</p>

<p><img src="/images/posts/sql_ml_speess_permissions1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Roles User1</em></p>

<p>In <em>Figure 1</em> we see how <code>user1</code> belongs to the server role <code>public</code>, and that is the only role he belongs to. So what if we have code like so:</p>

<pre><code class="language-sql">--uncomment the following and execute
--to execute as user1
--EXECUTE AS USER = 'user1';
--GO

EXECUTE sp_execute_external_script
             @language = N'R'
           , @script = N'
               d&lt;-42
               OutputDataSet &lt;- as.data.frame(d)'

--to switch back from user1 uncomment and execute
--the following
--REVERT
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Simple Test Code</em></p>

<p>The code in <em>Code Snippet 2</em> allows you to switch between admin/sa and <code>user1</code> without having to log in as <code>user1</code>.</p>

<p>If someone with sufficient permissions ran the code in <em>Code Snippet 2</em>, the result looks like so:</p>

<p><img src="/images/posts/sql_ml_spees_sa_exec.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>sa Executes SPEES</em></p>

<p>As we see in <em>Figure 2</em> we get back <a href="https://www.independent.co.uk/life-style/history/42-the-answer-to-life-the-universe-and-everything-2205734.html"><em>The Answer to the Ultimate Question of Life, the Universe and Everything.</em></a>, but if <code>user1</code> runs the same code, the result is:</p>

<p><img src="/images/posts/sql_ml_spees_sa_exec_error1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>SPEES Execution Error</em></p>

<p>Oh, dear me, <code>user1</code> receives a permission denied exception! Well, from what we know about SQL Server and permissions it probably was not that unexpected. No problem, we know about SQL Server permissions, so we realise we probably have to <code>GRANT EXECUTE</code> permissions on <em>SPEES</em> to <code>user1</code> (or <code>public</code>):</p>

<pre><code class="language-sql">GRANT EXECUTE ON sp_execute_external_script to user1;
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Grant Execute Permission</em></p>

<p>Being in the database where <code>user1</code> exists and executing the code in <em>Code Snippet 3</em> as admin/sa - what could possibly go wrong:</p>

<p><img src="/images/posts/sql_ml_spees_perm_grant_error1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Grant Permission Error</em></p>

<p>Oops, something did go wrong, as it turns out that if you try to grant permissions on extended stored procedures, which <em>SPEES</em> is, you need to do it from the <code>master</code> database. Cool, let us switch to master and do it there. Well, if you try to do that - then you get another error: the user does not exist in <code>master</code>, sigh!</p>

<p>At this stage you have a couple of options:</p>

<ul>
<li><del>Add the login for the user to the <code>sysadmin</code> role, or the user to the <code>db_owner</code> role in the actual database.</del> No do not do that, I am only kidding! Do.Not.Do.That!</li>
<li>Create the user in <code>master</code> and grant the permission. That would work.</li>
<li>Grant the permission to <code>public</code>.</li>
</ul>

<p>Both options above (I do not count <code>sysadmin</code>, <code>db_owner</code>) have drawbacks:</p>

<ul>
<li>Create the user in <code>master</code>: you now have a user in master, and the question is what &ldquo;shenanigans&rdquo; the user can do.</li>
<li>Grant permission to <code>public</code>: anyone can potentially execute <em>SPEES</em>, not ideal.</li>
</ul>

<p>For reasons that become clear later I go with granting permission to <code>public</code>:</p>

<pre><code class="language-sql">USE master
GRANT EXECUTE ON sp_execute_external_script to public;
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Granting Permission to Public</em></p>

<p>After admin/sa runs the code in <em>Code Snippet 4</em>, <code>user1</code> can now execute the code in <em>Code Snippet 2</em> and we should see <em>The Answer &hellip;</em>:</p>

<p><img src="/images/posts/sql_ml_spees_sa_exec_error2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>SPEES Execution Error</em></p>

<p><a href="https://en.oxforddictionaries.com/definition/eish">Eish</a>, what goes on here? We did grant the permission in <em>Code Snippet 4</em>, so what now? Hmm, if we compare the errors, we see that they are slightly different. The error before granting the permission is something like: &ldquo;The EXECUTE permission was denied &hellip;&rdquo;, whereas the error after granting the permission is like: &ldquo;The user does not have permission &hellip;&rdquo;. It seems that the code in <em>Code Snippet 4</em> did something, but we still miss a piece (or multiple) of the puzzle, and it is permissions related. What permission(s) is the question?</p>

<p>So I decided to try a &ldquo;brute force attack&rdquo;; find all built in permissions in SQL Server, browse through them and see if I see something promising. For this, I used a SQL Server function: <code>sys.fn_builtin_permissions</code>, which - when executed - returns a description of the built-in permissions hierarchy of the server:</p>

<pre><code class="language-sql">SELECT * FROM fn_builtin_permissions('database')
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Retrieve All Functions</em></p>

<p>The parameter (<code>database</code>) in <em>Code Snippet 5</em> indicates what permissions I want back. In this case, I want all permissions on a database level. When I ran the code in <em>Code Snippet 5</em> the function call returned 78 rows, and towards the end of the result I saw something promising:</p>

<p><img src="/images/posts/sql_ml_spees_permissions.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>SPEES Execution Error</em></p>

<p>The highligthed part in <em>Figure 6</em> looks very interesting. I wonder what happens if I do something like this as admin/sa in the database the user is in:</p>

<pre><code class="language-sql">GRANT EXECUTE ANY EXTERNAL SCRIPT TO user1
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Grant External Script</em></p>

<p>The code in <em>Code Snippet 5</em> ran without any issues, and <code>user1</code> can then try following code:</p>

<pre><code class="language-sql">SELECT SUSER_NAME()
EXECUTE sp_execute_external_script
             @language = N'R'
           , @script = N'
               d&lt;-42
               OutputDataSet &lt;- as.data.frame(d)'
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Another Try by user1</em></p>

<p>The <code>SELECT SUSER_NAME()</code> in <em>Code Snippet 6</em> is there to verify that it is the correct user executing. The result when <code>user1</code> executes looks like so:</p>

<p><img src="/images/posts/sql_ml_spees_user1_exec.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Successful Execution</em></p>

<p>So that was the missing link: <code>GRANT EXECUTE ANY EXTERNAL SCRIPT TO ...</code>, and to tell the truth; afterwards, I have seen a few posts on the net mentioning <code>EXECUTE ANY EXTERNAL SCRIPT</code>.</p>

<p>One more thing: above I mentioned that I favour granting the <code>EXECUTE</code> on <em>SPEES</em> to <code>public</code> instead of adding the user to <code>master</code>. The reason for this is what we just have seen: yes you do a &ldquo;blanket&rdquo; <code>GRANT</code> by granting <code>public</code>, but a user still needs to be granted <code>EXECUTE ANY EXTERNAL SCRIPT</code> before he can &ldquo;go wild&rdquo;. That gives admins/dba&rsquo;s some control over who can execute <em>SPEES</em>.</p>

<h2 id="summary">Summary</h2>

<p>To allow a non-admin database user to execute <code>sp_execute_external_script</code> you need to:</p>

<ul>
<li>Grant <code>public</code> execute permissions on <code>sp_execute_external_script</code>, and you do it in <code>master</code>. Obviously, you only need to do it once.</li>
<li>Grant <code>EXECUTE ANY EXTERNAL SCRIPT</code> to the user in the different databases he needs to execute <em>SPEES</em> in.</li>
</ul>

<p>That is it!</p>

<p>One final thing: if you want to read more about <code>sp_execute_external_script</code>, <a href="/sql_server_2k16_r_services">SQL Server R Services</a> series has some posts.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<!-- [series1]: <> (/sql_server_2k16_r_services) -->

<!-- [series2]: <> (/sql_server_ml_services_install_packages) -->]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing R Packages in SQL Server Machine Learning Services - I]]></title>
    <link href="http://nielsberglund.com/2018/06/23/installing-r-packages-in-sql-server-machine-learning-services---i/" rel="alternate" type="text/html"/>
    <updated>2018-06-23T13:13:58+02:00</updated>
    <id>http://nielsberglund.com/2018/06/23/installing-r-packages-in-sql-server-machine-learning-services---i/</id>
    <content type="html"><![CDATA[<p>This post was supposed to be a single post about how to install R packages in SQL Server Machine Learning Services, but once again I completely misjudged the scope of the topic. So this one post turned into a series of posts about how to install R packages in <strong>SQL Server Machine Learning Services</strong> and this is the first post in the series.</p>

<p>To see other posts in the series go to <a href="/sql_server_ml_services_install_packages"><strong>Install R Packages in SQL Server ML Services Series</strong></a>.</p>

<p>As you may know, I am in the process of writing the follow-up post to <a href="/2018/05/20/sp_execute_external_script-and-sql-compute-context---i/">sp_execute_external_script and SQL Compute Context - I</a>. However, I have a hard time getting into the flow of things, so I use any excuse I can, to not have to write. So when <a href="https://www.linkedin.com/in/dane-bax/">Dane Bax</a>, a colleague of mine, contacted me a couple of days ago with a <strong>SQL Server Machine Learning Services</strong> problem, I jumped at the chance to help him, and also write a post about it.</p>

<p><a href="https://www.linkedin.com/in/dane-bax/">Dane</a> works for a sister company of <a href="/Derivco">Derivco</a> as a data scientist and at both <a href="/Derivco">Derivco</a> and the sister company - <a href="https://www.microgaming.co.uk/">Microgaming</a> we are now looking at using SQL Server Machine Learning Services.</p>

<p>His problem was that he wanted to use a CRAN package - <code>bsts</code> - which is not part of a standard SQL Server R installation. He tried a couple of things to get it installed but got errors, so he decided to get in touch with me.</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> The package name <code>bsts</code> stands for <em>Bayesian Structural Time Series</em> and it performs time series regression using dynamic linear models fit using MCMC.</p>
</blockquote>

<h2 id="background">Background</h2>

<p>First of all; why would we need to install R packages if I already have R, either on my local machine or via SQL Server ML Services? Well, the answer to that is that there are a multitude of packages &ldquo;in the wild&rdquo; who do not necessarily come with your R engine of choice, and <code>bsts</code> is an excellent example of this.</p>

<p>If you are an R developer you are probably accustomed to installing packages on your R development environment at will, and - more or less - at whatever location you choose. When using SQL Server ML Services however, it does not work like that as SQL Server cannot load packages from external libraries, even if that library is on the same computer. So when using SQL Server ML Services, you can only install packages to a default library associated with the instance.</p>

<p>The installation of packages can be done in different ways which is what this post is about - but before that, let us look at something somewhat different: <strong>Rtools</strong>.</p>

<h2 id="rtools">Rtools</h2>

<p>The cool thing with R is that it is open source and you can run it on multiple platforms (Windows, Mac, Linux). So in essence, whatever package you want to use you can run on your platform of choice. If you install a package on Mac or Windows, R downloads and installs a pre-compiled (for your OS) packet. On Linux, R downloads the source of the package and builds it on your machine. For the build, R requires some external tools: <code>make</code>, <code>tar</code>, <code>gzip</code>, C/C++ compiler and so forth.</p>

<p>Why I mention this here is that certain packages do not have a binary built for Windows, and if you want to install such a package, you need to build the package from source on your environment. The problem with this is that most of the tools needed to build the package may not exist in the Windows environment.</p>

<p>To be able to compile from source on Windows, the people behind R have made available an installer which installs the required tools for compilation of packages: <strong>Rtools</strong>. So if you think that you ever need to compile an R package from source, then ensure that you have <em>Rtools</em> installed. Why I bring this up is that the <code>bsts</code> package has a dependency on a package that needs to be compiled.</p>

<h4 id="rtools-installation">Rtools Installation</h4>

<p>While you can install R packages to a remote machine, keep in mind that <em>Rtools</em> is not an R package as such, and it is not installed into the engine - but onto the machine where R is. So, for us here, we need to be on the machine that hosts SQL Server ML Services and run the installation on that machine.</p>

<p>We need first to download the installer for <em>Rtools</em>, and when you browse to the <a href="https://cran.r-project.org/bin/windows/Rtools/index.html">download page</a>, you see there are multiple versions dependent on what version of R you have. To find the R version of your SQL Server ML Services installation you can run following code:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
                  @language = N'R' ,
                  @script = N'print(R.Version()$version)'
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Retrieve R Version</em></p>

<p>When I run the code in <em>Code Snippet 1</em> on my SQL Server ML Services instance, I see that the R version is <code>3.3.3</code>, so I download <code>Rtools35.exe</code> to my SQL Server machine and run the installer. By default <em>Rtools</em> installs to <code>C:\RTools</code> and R looks for compilers in the default installation path. If you install anywhere else, you have to point R to the path of <code>gcc</code>, <code>g++</code> and <code>ld</code>, by setting a variable called BINPREF. <em>Rtools</em> installation instructions discuss this in detail. During the install ensure you check the checkbox for &ldquo;Add rtools to system PATH&rdquo;:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs_rtools_path.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Adding Rtools to PATH</em></p>

<p>After having checked the box for editing the PATH as in <em>Figure 1</em>, click through and let the install finish. After installation, it is a good practice to check that the PATH is set. You can do this by running <code>RTerm.exe</code> (on the SQL Server box) and execute <code>Sys.getenv('PATH')</code> from RTerm&rsquo;s command prompt. You find <code>RTerm.exe</code> at the <code>R_SERVICES\bin\x64</code> directory under the path to the SQL Server instance. For example: <code>C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\R_SERVICES\bin\x64</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> Just a word of caution here. When I installed <code>Rtools35.exe</code> I had to manually add to PATH the path to the compilers: <code>C:\Rtools\mingw_64\bin</code>. So look out for that .</p>
</blockquote>

<p>You should also check that you can call the C++ compiler: <code>system('g++ -v')</code> (this is how I realised the path was not correct). That should result in something like:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs_rtools_g++.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Checking for C++ Compiler</em></p>

<p>If everything looks OK, <em>Rtools</em> have now been installed, and the various instances of R (from SQL Server ML instances) can share the <em>Rtools</em> toolchain.</p>

<h2 id="r-packages-installation">R Packages Installation</h2>

<p>When we install R packages for SQL Server ML Services we install them on a per SQL Server instance, and we can install these packages different in ways:</p>

<ul>
<li>R packet managers.</li>
<li>T-SQL.</li>
<li>RevoScaleR.</li>
</ul>

<p>Regardless how we install the packages, they can only be installed to the default packet library for that instance. The file system folder for this library has restricted access and to write to this folder you need admin rights. Well, that is not entirely correct - with some configuration changes even non-admin can install packages via T-SQL and RevoScaleR. However, as we see later, the installation is against the current database.</p>

<h4 id="r-code-for-installation">R Code for Installation</h4>

<p>Before we look at the ways we can install and the tools for installation; what does the code we use to do the installation look like?</p>

<p>As you probably know, the way to install R packages is through the <code>install.packages</code> command. The command has quite a few parameters as you can see <a href="https://www.rdocumentation.org/packages/utils/versions/3.5.0/topics/install.packages">here</a>, but when I install packages I use only a few of the parameters, regardless of the way I install the package:</p>

<pre><code class="language-r"># get the library path
libPath &lt;- .libPaths()[1]
install.packages(&quot;pkg_name&quot;, lib = libPath, 
                  repos = &quot;url_for_the_repo&quot;, 
                  dependencies = TRUE)
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Install Packages Command</em></p>

<p>In <em>Code Snippet 2</em> we see how I first retrieve the library path. This is where I install the package to, and once again - I can only install to the default instance directory. Then in the <code>install.packages</code> call I use these parameters:</p>

<ul>
<li>First parameter is always the name of the package(s) to install.</li>
<li><code>lib</code>: is for the library folder to install to.</li>
<li><code>repos</code>: the base URL(s) of the repositories to use. If left out, the repo used is the Microsoft MRAN repo, which may not be what you want.</li>
<li><code>dependencies</code>: indicating whether to also install missing packages which these packages depend on/link to/import/suggest (and so on recursively).</li>
</ul>

<p>So that is the code for installation of packages. What if you want to see what packages are installed on a particular instance of SQL Server ML Services? For that you can execute something like so from <strong>SQL Server Management Studio</strong> (SSMS):</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
                    @language = N'R', 
                    @script = N'
                    OutputDataSet &lt;- data.frame(
                    installed.packages()[,c(&quot;Package&quot;, &quot;Version&quot;, 
                                        &quot;Depends&quot;, &quot;License&quot;, 
                                        &quot;Built&quot;, &quot;LibPath&quot;)]);'
WITH RESULT SETS ((Package nvarchar(255), Version nvarchar(100), 
                   Depends nvarchar(4000), License nvarchar(1000), 
                   Built nvarchar(100), LibPath nvarchar(2000)));
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Retrieve Installed R Packages</em></p>

<p>So now when we have seen some code how to install packages (and also see what packages already exists), in this post let us look at using R packet managers for the installation.</p>

<h2 id="r-packet-managers">R Packet Managers</h2>

<p>What is an R packet manager? It is an R command line tool or GUI installed on the SQL Server Machine Learning Services machine that can run with elevated permissions and target the R engine for the instance on which you want to install the package. The easiest is to use either of the R tools that come as part as part of SQL Server&rsquo;s R service:</p>

<ul>
<li>The command line tool: <code>Rterm.exe</code>.</li>
<li>The GUI: <code>Rgui.exe</code>.</li>
</ul>

<p>Once again you need to be able to run them with elevated access, so you need admin rights on the machine, and they can only run locally.</p>

<p>So let us say that <a href="https://www.linkedin.com/in/dane-bax/">Dane</a> (from above) wants to install the <code>bsts</code> package mentioned above and he has admin rights on the machine SQL Server is installed on. The choice he has is to use <code>Rterm.exe</code> or <code>Rgui.exe</code>. Dane is not really into command line, so he uses <code>Rgui.exe</code>:</p>

<ul>
<li>He logs onto the SQL Server machine either directly or via <em>Terminal Services</em>.</li>
<li>He navigates to where <code>Rgui.exe</code> is (the same path as above for <code>Rterm.exe</code>).</li>
<li>He right clicks on <code>Rgui.exe</code>:</li>
</ul>

<p><img src="/images/posts/sql_ml_install_r_pckgs_rgui.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Run Rgui as Admin</em></p>

<p>When he clicks on &ldquo;Run as administrator&rdquo; the Rgui application starts up in the context of the R engine of the instance of SQL Server ML Services:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs_rgui2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Rgui</em></p>

<p>In the R Console in Rgui Dane now enters this code:</p>

<pre><code class="language-r">&gt; libPath &lt;- .libPaths()[1]
&gt; install.packages(&quot;bsts&quot;, repos = &quot;https://cloud.r-project.org/&quot;, 
+                  dependencies=TRUE, lib = libPath);
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Install bsts</em></p>

<p>In <em>Code Snippet 4</em> we see that Dane uses the open source <em>CRAN</em> repo which hosts <code>bsts</code>. When he executes the code, it looks like so:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Execute install.packages</em></p>

<p>At the highlighted question in <em>Figure 4</em> (at the bottom) it is best to answer no. Even though Dane said no to compilations, quite a lot of compilations happens for the <code>Boom</code> package:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs_compile.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>C++ Compilation</em></p>

<p>The installation process runs for quite a while, due to the compilation of the <code>Boom</code> package, but eventually, it finishes:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs_success.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Install Success</em></p>

<p>Dane can now check and see if the <code>bsts</code> package has installed and he executes the code in <em>Code Snippet 3</em> to verify that <code>bsts</code> is indeed installed together with the dependent packages. To further confirm that the package exists and functions he can try to load it from <em>SSMS</em>:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
                  @language = N'R' ,
                  @script = N'library(&quot;bsts&quot;)'
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Loading the bsts Library</em></p>

<p>Executing the code in <em>Code Snippet 8</em> results in:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs_loading_bsts.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Loading bsts</em></p>

<p>From <em>Figure 7</em> it seems that everything has worked, sweet!</p>

<p>That is cool, no?! Well, there is one drawback with this: Dane has to have admin rights on the SQL Server box and, no offense Dane, but who in their right minds would give Dane those rights on a production SQL Server box!</p>

<p>Jokes aside, using an R packet manager may be too inconvenient, e.g. anytime a developer want to install packages, someone with admin rights on the box needs to install said packages. In coming posts we look at other options for installing packages.</p>

<h2 id="summary">Summary</h2>

<p>In this post we covered:</p>

<ul>
<li>When you install packages sometimes they require compilation. For that, <strong>Rtools</strong> should be on the box where SQL Server ML Services lives.</li>
<li>There are multiple ways we can install packages:

<ul>
<li>R packet managers.</li>
<li>T-SQL.</li>
<li>RevoScaleR.</li>
</ul></li>
<li>An R packet manager is an R command line tool or GUI installed on the SQL Server Machine Learning Services machine that can run with elevated permissions and target the R engine for the instance on which you want to install the package.</li>
<li>SQL Server ML Services ships with two R packet manager: <code>Rterm.exe</code> and <code>Rgui.exe</code>.</li>
<li>These two packet managers lives in <code>\\&lt;path_to_SQL_Server_instance&gt;\R_SERVICES\bin\x64</code>.</li>
<li>When using a packet manager to install a package, you run the packet manager from an elevated command prompt.</li>
<li>You can use the R command <code>install.packages</code> to install a package from the package manager.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

